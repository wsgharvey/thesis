\chapter{Flexibly-conditional variational auto-encoders}
\label{ch:cigcvae}

% The line of research we look at in this section has two major differences from earlier sections:
% \begin{itemize}
%     \item We look at VAEs instead of diffusion models. Really useful to be able to do arbitrarily-conditional generative modelling with diverse methodologies. And especially if you want to do e.g. latent diffusion this is SOTA. We believe this is the highest-resolution arbitrarily-conditional VAE out there.
%     \item We will look at how to quickly train these systems. Should mention related work including ControlNet that now does similar things with diffusion models
% \end{itemize}

So far in this dissertation we have enabled flexible conditioning in various settings using diffusion models. While diffusion models have demonstrated very good performance on image and video-based tasks, it is not the only modelling methodology with which flexible conditioning is possible. For example, flexible conditioning has already been demonstrated with variational auto-encoders (VAEs)~\citep{ivanov2018variational}; generative adversarial networks~\citep{goodfellow2014generative}; and transformer-based token-prediction architectures~\citep{chang2022maskgit}. 

We believe that enabling and improving flexible conditioning within a diverse set of modelling methodologies is important, not least because the dominant modelling methodology for, e.g., image generation has changed over time. VAEs showed good performance on image generation benchmarks in 2013 after being proposed by \citet{kingma2013auto}. Generative adversarial networks~\citep{goodfellow2014generative} were proposed shortly after, and within a few years were preferred for many image generation tasks~\citep{karras2018style}. Autoregressive methods~\citep{van2016pixel} were proposed shortly after and obtained state-of-the-art likelihoods. Advocacy of diffusion models as a leading method for image generation has been widespread since roughly 2021~\citep{dhariwal2021diffusion}. Since then, MaskGIT~\citep{chang2022maskgit} has gained popularity. Even more recently, methods like rectified flow~\citep{esser2024scaling} and consistency models~\citep{song2023consistency}, which build on the diffusion framework, have been shown to exhibit better sampling speed than true diffusion models while retaining good sample quality. As we saw in \cref{ch:conditional-diffusion}, even using two diffusion models to construct a simple multi-stage generative model (2SDM) can give better sample quality than a pure diffusion model. All this is to say that, while diffusion models are currently a highly-competitive approach for image and video generation tasks, this may not remain the case forever. Furthermore, even now, many approaches use latent diffusion models~\citep{rombach2022high,betker2023improving,brooks2024video} that are a combination of a diffusion model and a VAE. In this chapter we will demonstrate a flexibly-conditional image VAE. While it is not the first flexibly-conditional image VAE~\citep{ivanov2018variational}, it significantly improved on prior work in terms of image quality and resolution and, to the best of our knowledge, these aspects remain unmatched among flexibly-conditional VAEs.


% In this chapter we will demonstrate a flexible variational auto-encoder; that is, a flexible generative model that is based on the variational auto-encoder framework~\citep{kingma2013auto,rezende2014stochastic}. Its good performance shows that flexible generative modelling as a concept is not tied to the diffusion framework and therefore could prove to be future-proof as modelling methodologies change.

\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{figs/cigcvae/qual/0_0_patches-4.png}
  \includegraphics[width=\textwidth]{figs/cigcvae/qual/59_2_3_modddd.png}
  \caption{\textbf{Left column:} Images with most pixels masked out.
    \textbf{Rest:} Completions from IPA, our proposed method.
  }
  \label{fig:cigcvae-headline}
\end{figure*}

% We will demonstrate flexibly-conditional VAEs primarily on the image inpainting problem. 
In particular, we train a single model which is capable of generating an image conditioned on any subset of that image's pixel values, and so is capable of image inpainting. We demonstrate that such a model can achieve excellent performance; see results in \cref{fig:cigcvae-headline}. Furthermore, to mitigate the slow and expensive training usually associated with VAEs, we demonstrate that a flexibly-conditional VAE can be trained with little computational cost by initialising training with a pretrained unconditional VAE. 

% We belive that this finding is extremely encouraging in terms of the applicability of flexible generative modelling; it suggests that the learned representations that are useful for flexible generation are similar to those that are good for unconditional generation, at least within the VAE framework. This provides evidence that flexible generative modelling and unconditional generative modelling are tightly-related tasks, giving us hope that other model classes that are good at unconditional generation will also be amenable to flexibly conditional generation.


Before introducing flexibly-conditional VAEs, we will review background material on VAEs in \cref{sec:vae}. We will then motivate and explain our proposed method for training flexibly-conditional VAEs in \cref{sec:ipa}; report experimental results in \cref{sec:cigcvae-experiments}; and describe a hypothetical application of an inpainting model with the capabilities ours exhibits in \cref{sec:cigcvae-boed}.


\section{A review of variational auto-encoders}
\label{sec:vae}
As with our review of diffusion models in \cref{ch:diffusion}, we will start by describing VAEs~\citep{kingma2013auto,rezende2014stochastic} as unconditional generative models. We will then explain a conditional variant in \cref{sec:conditional-vae}. Like a diffusion model, a VAE is trained to approximate a data distribution $\pdata(\rvx)$ using samples from that data distribution. Given a VAE with parameters $\theta$, we will denote the distribution over data it parameterises $p_\theta(\rvx)$. The key feature of a VAE is that it breaks the process of generating data into two steps: in the first step a VAE samples a latent variable $\rvz$ from a ``prior'' distribution $p_\theta(\rvz)$. The prior distribution can be fixed, e.g. a unit Gaussian, or can have learnable parameters that are included in $\theta$. In the second step, the VAE parameterises a ``likelihood'' $p_\theta(\rvx|\rvz)$. This can be, e.g., a Gaussian for continuous data~\citep{kingma2013auto} or a categorical for discrete data~\citep{child2020very}, with parameters output in either case by a neural network as a function of $\rvz$. We can therefore write the parameterised distribution $p_\theta(\rvx)$ as the marginal of $p_\theta(\rvx|\rvz)$ over the latent variable $\rvz$,
\begin{equation} \label{eq:vae-marginal}
p_\theta(\rvx) = \int p_\theta(\rvx|\rvz)p_\theta(\rvz) \mathrm{d}\rvz.
\end{equation}
Even if both the prior $p_\theta(\rvz)$ and likelihood $p_\theta(\rvx|\rvz)$ are simple distributions, the marginal $p_\theta(\rvx)$ can be complex since the parameters of the likelihood can be given by arbitrarily complex functions of $\rvz$.

As we discussed for diffusion models in \cref{ch:diffusion}, an ideal training objective would maximise the data likelihood $p_\theta(\rvx)$ averaged over all training examples since this corresponds to a maximum-likelihood estimate of the parameters given the dataset. The required marginalisation in \cref{eq:vae-marginal}, however, means that directly estimating the data likelihood is intractable. Therefore, as for diffusion models, we instead derive a training objective that lower-bounds the data likelihood. Starting by taking an expectation of the log of the data likelihood in \cref{eq:vae-marginal}, we can obtain a lower-bound using a proposal distribution $q_\phi(\rvz|\rvx)$ which can be parameterised by a neural network with parameters $\phi$: % our objective can be converted to the log of an expectation over $q_\phi(\rvz)$:
\begin{align}
    \EX_{\pdata(\rvx)} \left[ \log p_\theta(\rvx) \right] &= \EX_{\pdata(\rvx)} \left[ \log \int p_\theta(\rvx|\rvz)p_\theta(\rvz) \mathrm{d}\rvz \right] \\
    &= \EX_{\pdata(\rvx)} \left[ \log \int p_\theta(\rvx|\rvz) \frac{p_\theta(\rvz)}{q_\phi(\rvz|\rvx)} q_\phi(\rvz|\rvx) \mathrm{d}\rvz \right] \\
    &= \EX_{\pdata(\rvx)} \left[ \log \EX_{q_\phi(\rvz|\rvx)} \left[ p_\theta(\rvx|\rvz) \frac{p_\theta(\rvz)}{q_\phi(\rvz|\rvx)} \right] \right]. \label{eq:vae-marginal-is}
\end{align}
Ideally we would be able to obtain an unbiased estimate of this objective. Due to the logarithm operator outside an intractable expectation this is not feasible but, using Jensen's inequality, we can obtain a training objective that lower bounds it by switching their order: 
\begin{align}
    \gO_\text{VAE}(\theta, \phi) := \EX_{\pdata(\rvx)q_\phi(\rvz|\rvx)} \left[ \log p_\theta(\rvx|\rvz) + \log \frac{p_\theta(\rvz)}{q_\phi(\rvz|\rvx)} \right] \leq \EX_{\pdata(\rvx)} \left[ \log p_\theta(\rvx) \right].
    \label{eq:vae-objective}
\end{align}
Note that we are now describing training as maximising an objective rather than minimising a loss as in earlier chapters. We could, alternatively and equivalently, minimise $\gL_\text{VAE}(\theta, \phi) = -\gO_\text{VAE}(\theta, \phi)$ but we make make this choice to fit a common convention in the VAE literature~\citep{kingma2013auto,sohn2015learning,child2020very,vahdat2020nvae}. The lower-bound in \cref{eq:vae-objective} can be stochastically estimated by sampling data $\rvx$ and then sampling $\rvz$ from $q_\phi(\rvz|\rvx)$ and evaluating the terms inside the expectation with that $\rvz$. Since $q_\phi(\rvz|\rvx)$ ``encodes'' data $\rvx$ into latent variables $\rvz$ we will from now on refer to it as the encoder. The tightness of this lower-bound is strongly dependent on how ``good'' the encoder $q_\phi(\rvz|\rvx)$ is, as we can make clear by rewriting \cref{eq:vae-objective} as
\begin{equation}
    \gO_\text{VAE}(\theta, \phi) := \EX_{\pdata(\rvx)} \left[ \log p_\theta(\rvx) - \kl{q_\phi(\rvz|\rvx)}{p_\theta(\rvz|\rvx)} \right].
\end{equation}
This formulation reveals that the lower-bound is tight if and only if the proposal $q_\phi(\rvz|\rvx)$ is equal to the generally intractable posterior $p_\theta(\rvz|\rvx)$ for all $\rvx$, causing the KL divergence term to be zero. Fortunately, maximising this objective with respect to $\theta$ and $\phi$ will optimise the proposal parameters $\phi$ to better match the intractable posterior as well as optimising $\theta$ to increase the model likelihood, and so it is possible to train both $\theta$ and $\phi$ with a unified objective. The full optimisation problem is therefore to find
\begin{align}
    \argmax_{\theta,\phi} \gO_\text{VAE}(\theta, \phi).
\end{align}

\subsection{Hierarchical variational auto-encoders}
\label{sec:hierarchical-vae}
\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figs/thesis/vae-vs-hierarchical-vae.pdf}
    \caption{A comparison of the dependency structure of a ``standard'' VAE (left) with that of a top-down hierarchical VAE (right). For each, we show the dependencies of each variable in the encoder's distribution though dashed orange lines and the dependencies of each variable in the prior and decoder's distributions with solid black lines.}
    \label{fig:vae-vs-hierarchical-vae}
\end{figure}
A hierarchical VAE~\citep{gregor2015draw,kingma2016improving,sonderby2016ladder,klushyn2019learning} is a special case of a VAE in which the latent variables $\rvz$ are structured. In particular, $\rvz$ is partitioned into $L$ disjoint groups, $\rvz_1,\ldots,\rvz_L$. The prior $p_\theta(\rvz) = p_\theta(\rvz_1,\ldots,\rvz_L)$ and encoder $q_\phi(\rvz|\rvx) = q_\phi(\rvz_1,\ldots,\rvz_L|\rvx)$ are then factorised to enable them to represent structured distributions over $\rvz$. Exactly how these distributions are factorised is a design choice but the hierarchical VAEs that we will consider in this thesis are ``top-down'' meaning that they factorise the prior and the encoder distributions in the same order~\citep{vahdat2020nvae,child2020very}:
\begin{equation} \label{eq:hierarchical-vae-factorisations}
    p_\theta(\rvz) = \prod_{l=1}^L p_\theta(\rvz_l|\rvx,\rvz_{<l}) \quad \text{and} \quad q_\phi(\rvz|\rvx) = \prod_{l=1}^L q_\phi(\rvz_l|\rvx,\rvz_{<l})
\end{equation}
where $\rvz_{<l}$ is the set of all latent variable groups from $\rvz_1$ (inclusive) to $\rvz_{l}$ (exclusive) if $l > 1$ and is null otherwise. We show this dependency structure graphically in \cref{fig:vae-vs-hierarchical-vae}.

\subsection{Relationship to diffusion models}
Hierarchical VAEs are closely related to diffusion models, and in fact diffusion models can be interpreted as a special case of a hierarchical VAE. Given an increasing sequence of noise standard deviations $\Sigma[1],\ldots,\Sigma[L]$, we can present a variance-exploding diffusion model as a bottom-up hierarchical VAE with a simple encoder which progressively adds Gaussian noise to transition between each layer of latent variables:
\begin{align}
    q(\rvz|\rvx) &= q(\rvz_L|\rvx) \prod_{l=2}^L q(\rvz_{l-1} | \rvz_{l}) \\
    % &= \gN( \rvz_L ; \rvx, \Sigma[1]^2 \mI ) \prod_{l=L}^2 \gN( \rvz_{l-1} ; \rvz_{l}, (\Sigma[l]^2-\Sigma[l-1]^2) \mI ) \\
    &= \gN( \rvx_1 ; \rvx, \Sigma[1]^2 \mI ) \prod_{l=2}^L \gN( \rvx_{l} ; \rvx_{l-1}, (\Sigma[l]^2-\Sigma[l-1]^2) \mI )
\end{align}
where, to allow for easier comparisons to \cref{ch:diffusion}, we have defined $\rvx_1 := \rvz_L$, $\rvx_2 := \rvz_{L-1}$, \ldots, $\rvx_L := \rvz_1$. Due to the conjugacy of Gaussians, we can equivalently present a factorisation of $q(\rvz|\rvx)$ as a top-down hierarchical VAE encoder. This is reminiscent of the factorisation of $q$ used to derive a lower-bound on the diffusion model likelihood in \cref{sec:diffusion-likelihood}:
\begin{align}
    q(\rvz|\rvx) &= q(\rvz_1|\rvx) \prod_{l=2}^L q(\rvz_l | \rvx, \rvz_{l-1}) \\
    % &= \gN( \rvz_1 ; \rvx, \Sigma[L]^2 \mI ) \prod_{l=2}^L \gN( \rvz_l ; \mu^l_Q(\rvz_{l-1}, \rvx), {\sigma_Q^l}^2 \mI ) \\
    &= \gN( \rvx_L ; \rvx, \Sigma[L]^2 \mI ) \prod_{l=2}^L \gN( \rvx_{l-1} ; \mu^l_Q(\rvx_{l}, \rvx), {\sigma_Q^l}^2 \mI )
\end{align}
where $\mu^l_Q(\rvz_{l}, \rvx) := \mu_Q(\rvz_{l}, \rvx; \rho, \sigma)$ and $\sigma^l_Q := \sigma_Q(\rho, \sigma)$ with $\mu_Q$ and $\sigma_Q$ as defined in \cref{eq:mu-q,eq:sigma-q} and $\rho := \Sigma[l-1]$, $\sigma := \Sigma[l]$. Note that this encoder has no learnable parameters, i.e. $\phi$ is empty. In the DDPM style of sampling described in \cref{sec:diffusion-likelihood}, a diffusion model approximates each $p_\theta(\rvz_l|\rvz_{<l})$ as a Gaussian.

A diffusion model's implicit use of this fixed encoder with this simple Gaussian form
enables some improvements to diffusion model training that are not possible for most VAEs. In particular, diffusion models allow an efficient training objective that can sample a single layer $l$ (i.e. diffusion timestep $t$) and efficiently compute a loss estimate for a data point using this single value of $l$ (or $t$). In contrast, the loss for a hierarchical VAE must typically be computed by evaluating all layers for each data point. The diffusion framework also enables weighting of the loss to improve perceptual quality, as we described in \cref{sec:diffusion-perceptual-quality}, and the use of ODEs or SDEs for improved performance at test-time versus explicitly sampling from each $p_\theta(\rvz_l|\rvz_{<l})$.

Advantages of the VAE framework, on the other hand, include the learnable encoder which makes it theoretically possible to learn fast and effective procedures for sampling datapoints other than the gradual removal of Gaussian noise. VAEs can also be designed to yield latent variables with particular properties, so even a VAE which has gives poor sample quality when used for unconditional generation may be useful as part of, e.g., a latent diffusion model~\citep{rombach2022high,brooks2024video}.

% Although diffusion models currently outperform state-of-the-art VAEs at image and video generation~\citep{esser2024scaling,brooks2024video}, variants of VAEs are still a tool used in state-of-the-art workflows such as latent diffusion models~\citep{rombach2022high,brooks2024video}. In these settings, VAEs can be used to compress data $\rvx$ into lower-dimensional latent variables $\rvz$ that are then modelled with other techniques like diffusion models~\citep{rombach2022high}. Taking a broader perspective, the two-stage diffusion model from \cref{sec:2sdm-2sdm-method} could also be interpreted as a form of variational auto-encoder in which the prior $p_\theta(\rvz)$ and decoder $p_\theta(\rvx|\rvz)$ are both parameterised by diffusion models and the encoder $q_\phi(\rvz|\rvx)$ is frozen to be a Dirac distribution on the CLIP image embedding of $\rvx$. 


\subsection{Conditional variational auto-encoders}
\label{sec:conditional-vae}
We now transport the VAE framework described above into the conditional generative modelling setting, where we want to train an approximation of the posterior $\pdata(\rvx|\rvy)$ given a dataset of $(\rvx, \rvy)$ pairs sampled from $\pdata(\cdot,\cdot)$.

Recall that the distribution over data $\rvx$ modelled by a VAE is defined through a combination of its prior and its decoder, as described in \cref{eq:vae-marginal}. For this distribution to depend on $\rvy$, as it must in a conditional VAE, at least one of the prior or decoder must take $\rvy$ as input. In the conditional VAEs considered in this thesis, only the prior will depend on $\rvy$. For conditional VAEs we will therefore write the prior as $p_\theta(\rvz|\rvy)$ while continuing to write the decoder as $p_\theta(\rvx|\rvz)$. In the case of a hierarchical VAE, we will allow for all layers to be conditioned on $\rvy$ and so factorise $p_\theta(\rvz|\rvy)$ as
\begin{equation}
    p_\theta(\rvz|\rvy) = \prod_{l=1}^L p_\theta(\rvz_l|\rvy,\rvz_{<l})
\end{equation}
The encoder can optionally also take $\rvy$ as an input~\citep{sohn2015learning}, but it will not in the VAEs presented in this thesis so we will continue writing it as simply $q_\phi(\rvz|\rvx)$. In the hierarchical case we will again factorise it as
\begin{equation} \label{eq:conditional-hierarchical-vae-factorisations}
    q_\phi(\rvz|\rvx) = \prod_{l=1}^L q_\phi(\rvz_l|\rvx,\rvz_{<l})
\end{equation}
as in \cref{eq:hierarchical-vae-factorisations}.

The distribution over data modelled by the VAE now becomes
\begin{equation} \label{eq:conditional-vae-marginal}
p_\theta(\rvx|\rvy) = \int p_\theta(\rvx|\rvz)p_\theta(\rvz|\rvy) \mathrm{d}\rvz.
\end{equation}
and we can derive a training objective similar to \cref{eq:vae-objective} as a lower-bound on $\log p_\theta(\rvx|\rvy)$, averaged over $(\rvx, \rvy)$ pairs from the training distribution:
\begin{align}
    \EX_{\pdata(\rvx, \rvy)} \left[ \log p_\theta(\rvx|\rvy) \right] &= \EX_{\pdata(\rvx, \rvy)} \left[ \log \int p_\theta(\rvx|\rvz)p_\theta(\rvz|\rvy) \mathrm{d}\rvz \right] \\
    &= \EX_{\pdata(\rvx, \rvy)} \left[ \log \int p_\theta(\rvx|\rvz)\frac{p_\theta(\rvz|\rvy)}{q_\phi(\rvz|\rvx)}q_\phi(\rvz|\rvx) \mathrm{d}\rvz \right] \\
    &= \EX_{\pdata(\rvx, \rvy)} \left[ \log \EX_{q_\phi(\rvz|\rvx)} \left[ p_\theta(\rvx|\rvz)\frac{p_\theta(\rvz|\rvy)}{q_\phi(\rvz|\rvx)} \right] \right] \\
    &\geq \EX_{\pdata(\rvx, \rvy)} \left[ \EX_{q_\phi(\rvz|\rvx)} \left[ \log p_\theta(\rvx|\rvz) + \log \frac{p_\theta(\rvz|\rvy)}{q_\phi(\rvz|\rvx)} \right] \right] \\
    &:= \gO_\text{CVAE}(\theta, \phi).
\end{align}




Similarly to the unconditional VAE training objective, we can break this down as
\begin{align} \label{eq:conditional-vae-objective}
    \gO_\text{CVAE}(\theta, \phi) = \EX_{\pdata(\rvx,\rvy)} \left[ \underbrace{\EX_{q_\phi(\rvz|\rvx)}\left[\log p_\theta(\rvx|\rvz) \right]}_\text{reconstruction objective} - \underbrace{\kl{q_\phi(\rvz|\rvx)}{p_\theta(\rvz|\rvy)}}_\text{KL divergence} \right]
\end{align}
showing that training the conditional VAE involves balancing maximising a ``reconstruction'' term with minimising an expected KL divergence between the encoder's distribution $q_\phi(\rvz|\rvx)$ and the conditional prior $p_\theta(\rvz|\rvy)$.

We now consider one further decomposition of $\gO_\text{CVAE}(\theta, \phi)$ which yields additional insights about the properties of conditional VAEs:
\begin{align}
    \gO_\text{CVAE}(\theta, \phi) =& \EX_{\pdata(\rvx,\rvy)q_\phi(\rvz|\rvx)} \left[ \log \frac{\log p_\theta(\rvx|\rvz)p_\theta(\rvz|\rvy)}{q_\phi(\rvz|\rvx)} \right] \\
    =& \EX_{\pdata(\rvx,\rvy)q_\phi(\rvz|\rvx)} \left[ \log \pdata(\rvx|\rvy) + \log \frac{\log p_\theta(\rvx|\rvz)p_\theta(\rvz|\rvy)}{q_\phi(\rvz|\rvx)\pdata(\rvx|\rvy)} \right] \\
    \label{eq:elbo-kl-joints}
    =& \EX_{\pdata(\rvx,\rvy)} \left[ \log \pdata(\rvx|\rvy) \right] \\
    &- \EX_{\pdata(\rvy)} \left[ \kl{q_\phi(\rvz|\rvx)\pdata(\rvx|\rvy)}{p_\theta(\rvx|\rvz)p_\theta(\rvz|\rvy)} \right] \nonumber .
\end{align}
\Cref{eq:elbo-kl-joints} states that the conditional VAE objective is the average likelihood of $\rvx$ given $\rvy$ under the data distribution minus a KL divergence describing the match between the model's distribution $p_\theta(\rvx|\rvz)p_\theta(\rvz)$ and a distribution given by encoding sampled data $q_\phi(\rvz|\rvx)\pdata(\rvx|\rvy)$. Given that asymmetry of the KL divergence~\citep{bishop2006pattern}, it also reveals that the learned $p_\theta(\rvx|\rvz)p_\theta(\rvz)$ will be mass-covering and so can be expected to produce more diverse samples than are realistic if it is unable to perfectly match $q_\phi(\rvz|\rvx)\pdata(\rvx|\rvy)$. Any modes of the true conditional distribution that the model's distribution misses will be heavily penalised by $\gO_\text{CVAE}(\theta,\phi)$. 

We end with a note on how the conditional VAE objective (\cref{eq:conditional-vae-objective}) can be estimated in practice. The reconstruction term in \cref{eq:conditional-vae-objective} is tractable to estimate for each $(\rvx,\rvy)$ with a single sample of $\rvz$ as long as likelihoods can be computed under $p_\theta(\rvx|\rvz)$. The KL divergence in \cref{eq:conditional-vae-objective} will be tractable in  a traditional (non-hierarchical) VAE where the distributions compared are e.g. both Gaussian. In a hierarchical VAE it is generally not tractable but it is possible to obtain the following unbiased estimate of it. In particular, in a top-down hierarchical VAE~\citep{vahdat2020nvae,child2020very} we can decompose it as
\begin{align}
    \kl{q_\phi(\rvz|\rvx)}{p_\theta(\rvz|\rvy)} &= \EX_{q_\phi(\rvz|\rvx)} \left[  \log \frac{q_\phi(\rvz|\rvx)}{p(\rvz|\rvy)} \right] \\
    &= \EX_{q_\phi(\rvz|\rvx)} \left[ \log \frac{\prod_{l=1}^L q_\phi(\rvz_l|\rvx,\rvz_{<l})}{\prod_{l=1}^L p_\theta(\rvz_l|\rvy,\rvz_{<l})} \right] \\
    &= \EX_{q_\phi(\rvz|\rvx)} \left[ \sum_{l=1}^L \log \frac{q_\phi(\rvz_l|\rvx,\rvz_{<l})}{p_\theta(\rvz_l|\rvy,\rvz_{<l})} \right] \\
    &= \EX_{q_\phi(\rvz|\rvx)} \left[ \sum_{l=1}^L \kl{q_\phi(\rvz_l|\rvx,\rvz_{<l})}{p_\theta(\rvz_l|\rvy,\rvz_{<l})} \right]
\end{align}
which is an expectation of the sum of layer-wise KL divergences. Each layer-wise KL divergence is tractable under common parameterisations of $p_\theta(\rvz_l|\rvy,\rvz_{<l})$ and $q_\phi(\rvz_l|\rvx,\rvz_{<l})$ like Gaussian distributions. We can therefore estimate the overall KL divergence by simply drawing one sample of $\rvz$ from the encoder and using it to compute layer-wise KL divergences.


\begin{figure*}[t]
  \centering
  \begin{subfigure}[b]{.32\textwidth}
    \centering
    \includegraphics[height=3.8cm]{figs/cigcvae/arch_small-standard.pdf}
    \caption{Estimating ELBO.}
    \label{fig:cigcvae-hierarchical-vae}
  \end{subfigure}
  \begin{subfigure}[b]{.32\textwidth}
    \centering
    \includegraphics[height=3.8cm]{figs/cigcvae/arch_small-forward.pdf}
    \caption{Estimating $\gO_\text{CVAE}$.}
    \label{fig:cigcvae-forward-arch}
  \end{subfigure}
  \begin{subfigure}[b]{.32\textwidth}
    \centering
    \includegraphics[height=3.8cm]{figs/cigcvae/arch_small-sampling.pdf}
    \caption{Sampling $\rvx\sim p_{\theta,\psi}(\cdot|\rvy)$.}
    \label{fig:cigcvae-reverse-arch}
  \end{subfigure}
  \vspace{-1mm}
  \caption{A hierarchical VAE architecture with $L=2$ layers of latent
    variables. Part (a) illustrates the computation of the ELBO for an
    unconditional VAE; part (b) illustrates the computation of our training
    objective $\gO_\text{CVAE}$; and part (c) illustrates the drawing
    of conditional samples. The encoder is shown in orange; the prior and the
    decoder (which maintains a deterministic hidden state $h_{l}$) are both
    shown in black; and the conditional prior is shown in blue. The computation
    graph needed to sample $\rvz$ in each case is drawn with dashed lines, and
    the remainder of the computation graph is drawn with solid lines.}
  \label{fig:cigcvae-conditional-architectures}
  \vspace{-2mm}
\end{figure*}


\section{Inference in a pretrained auto-encoder}
\label{sec:ipa}

We now describe our approach to training conditional, and flexibly-conditional, VAEs. It is designed to allow initialisation from a pretrained unconditional VAE, enabling fast and cheap training.  This is particularly important in the VAE setting because of the typically slow training times seen with VAEs. Take as an example the VD-VAE~\citep{child2020very}, which we will build on in this chapter. The VD-VAE takes on the order of 1 GPU-year to train on the $256\times256$ FFHQ dataset~\citep{karras2019style} whereas the generative adversarial networks (GANs) that were state-of-the-art at the time of VD-VAE's publication could be trained on the same
dataset in a matter of GPU-weeks~\citep{lin2021anycost,karras2020analyzing}. One hypothesis for the cause of this disparity
is that, whereas the ``mass-covering'' training objective for a VAE forces it to
assign probability mass over the entirety of the data distribution, a GAN can
``cut corners'' by dropping modes~\citep{arora2017gans,arora2017generalization}. Conditional GANs~\citep{zheng2019pluralistic,zhao2021large} and conditional VAEs~\citep{sohn2015learning,ivanov2018variational} suffer from the same disparity in training times as their unconditional counterparts. We will show that re-using publicly available pretrained models to initialise our conditional VAEs can lead to training times and sample quality competitive with GAN-based approaches while avoiding mode dropping.

% We present an approach based on the conditional VAE framework but, to mitigate the associated slow training times, we design the architecture so that we can incorporate pretrained unconditional VAEs. 


While requiring an existing pretrained model is a limitation, we note that:
\textbf{(1)} The unconditional VAE need not have been (pre-)trained on the same dataset as the conditional model; we show unconditional models trained on
ImageNet are suitable for later use with various photo datasets.
\textbf{(2)} A single unconditional VAE can be used for later training of
conditional VAEs on any desired conditional generation tasks (e.g.~the same
image model may be later used for image inpainting or image colourisation).
\textbf{(3)} There is an increasing trend in the machine learning community towards
sharing large, expensively trained models~\citep{wolf2020transformers},
sometimes referred to as foundation models~\citep{bommasani2021opportunities}.
Most of the unconditional VAEs in our experiments use publicly-available
pretrained weights released by \citet{child2020very}. By presenting a use case
for foundation models in image modelling, we hope to encourage even more sharing
of pretrained weights in this domain.

We demonstrate our approach in several settings but our focus is image inpainting, the problem of inferring the posterior distribution over images given the observation of a subset of pixel values. For some applications such as photo-editing the implicit distribution defined by GANs is good enough. We argue that our approach has substantial advantages when
image inpainting is used as part of a larger pipeline, and discuss one possible
instance of this in \cref{sec:cigcvae-boed}: Bayesian optimal experimental design (BOED)
for guiding a sensor or hard attention
mechanism~\citep{ma2018eddi,harvey2019near,rangrej2021achieving}. In this case,
missing modes of the posterior over images is likely to lead to bad decisions.
We show that our objective corresponds to the mass-covering KL divergence and so
covers the posterior well. This is supported empirically by results indicating that, not only is the visual quality of our inpaintings (see \cref{fig:cigcvae-headline}) close to the
state-of-the-art~\citep{zhao2021large}, but our coverage of the ``true''
posterior over inpaintings is superior to that of any of our baselines.


\subsection{Notation}
We have described VAE parameters as a combination of $\phi$, the encoder parameters, and $\theta$, parameters shared between the prior and decoder. In the conditional case, we modify $\theta$ to contain the conditional prior parameters and decoder parameters. In the following sections we will want to simultaneously consider decoder parameters, unconditional prior parameters, and conditional prior parameters. To do so, we will from now on use $\theta$ to mean the unconditional prior and decoder parameters, as for an unconditional VAE. We will then denote the conditional prior parameters $\psi \in \Psi$.
% To convert an unconditional VAE architecture to a conditional architecture, we
% introduce a \textit{conditional prior} with parameters $\psi\in\Psi$.
% Given a conditioning input $\rvy$ (which could be, e.g., an image with some
% pixels masked out in the case of inpainting),
To denote the conditional prior's approximate posterior over the latent variables given conditioning information we therefore use $p_\psi(\rvz|\rvy)$.
%
Our approximation of $\pdata(\rvx|\rvy)$ becomes
\begin{equation}
  \label{eq:marginal-image-posterior}
  p_{\theta,\psi}(\rvx|\rvy) := \int p_\theta(\rvx|\rvz)p_\psi(\rvz|\rvy) \mathrm{d}\rvz
\end{equation}
with learnable parameters $\theta$ and $\psi$. We can sample from
$p_{\theta,\psi}(\rvx|\rvy)$ by sampling $\rvz\sim p_\psi(\cdot|\rvy)$ and then
$\rvx\sim p_\theta(\cdot|\rvz)$ as illustrated in \cref{fig:cigcvae-reverse-arch}. 
%

We introduce some more notation before describing our method further. Let the
distribution of paired data be $\pdata(\rvx, \rvy)$, and recall that
training an unconditional VAE matches two joint distributions: the distribution
of unconditional samples, $p_\theta(\rvz, \rvx)$; and the distribution
resulting from sampling data $\rvx$ and then encoding it, $\pdata(\rvx)q(\rvz|\rvx)$. We define extensions of each to include $\rvy$:
\begin{align}
  p_\theta(\rvz, \rvx, \rvy) &= p_\theta(\rvz)p_\theta(\rvx|\rvz)\pdata(\rvy|\rvx), \label{eq:pmodel} \\
  q_\phi(\rvz, \rvx, \rvy) &= \pdata(\rvx, \rvy)q_\phi(\rvz|\rvx), \label{eq:r}
\end{align}
% alternatively:
% \begin{align}
%   p_\theta(\rvz, \rvx, \rvy) &= p_\theta(\rvz)p_\theta(\rvx|\rvz)\pdata(\rvy|\rvx), \label{eq:pmodel} \\
%   q_\phi(\rvz, \rvx, \rvy) &= \pdata(\rvx, \rvy)q_\phi(\rvz|\rvx), \label{eq:r}
% \end{align}
where $\pdata(\rvy|\rvx)$ is a (potentially intractable) conditional
distribution of $\pdata(\rvx, \rvy)$.
%
Note that $p_\theta(\rvz, \rvx, \rvy)$ and $q_\phi(\rvz, \rvx, \rvy)$ are exactly the two distributions matched by the conditional VAE
objective in \cref{eq:elbo-kl-joints}, $p_\theta(\rvz)p_\theta(\rvx|\rvz)$ and $\pdata(\rvx)q_\phi(\rvz|\rvx)$, with an additional factor of
$\pdata(\rvy|\rvx)$ applied to each.
%
Therefore, if the unconditional VAE represented by $\theta$ and $\phi$ is well
trained, $p_\theta(\rvz, \rvx, \rvy)$ and
$q_\phi(\rvz, \rvx, \rvy)$ will be close. From now on, we will use
$p_\theta$ and $q_\phi$ respectively to refer to any marginals and conditionals of these
joint distributions, with the specific marginal or conditional clear from
context.

So far we have introduced the notation we will use for conditional VAEs. For flexibly-conditional VAEs, we will use exactly the same notation. This means that we will no longer explicitly discuss $\gY$, the list of the observed data indices. Instead, we will define $\rvy$ to implicitly contain $\gY$ in the flexible conditioning setting. In particular, for our flexibly-conditional image models, $\rvy$ will be structured to include a binary mask describing which pixels are observed.

\subsection{Training objective}

Our training objective, previously used for training conditional
VAEs~\citep{sohn2015learning,ivanov2018variational} and neural
processes~\citep{garnelo2018neural}, is
\begin{align}
  \label{eq:forward-elbo}
  \gO_\text{CVAE}(\theta, \phi, \psi) &= \EX_{\pdata(\rvx, \rvy)} \EX_{q_\phi(\rvz|\rvx)} \left[ \log \frac{p_\theta(\tildeI|\rvz)p_\psi(\rvz|\rvy)}{q_\phi(\rvz|\rvx)} \right] \\
  &\leq \EX_{\pdata(\rvx, \rvy)} \left[ \log p_{\theta,\psi}(\tildeI{}|\rvy) \right]
\end{align}
where $\tilde{\rvx}$ is the part of $\rvx$ we wish to predict. In general we can
set $\tildeI{}:=\rvx$ but for inpainting define $\tildeI{}$ to consist of only
the dimensions of $\rvx$ not observed in $\rvy$, abusing notation by
ignoring the implication that $\tildeI{}$ is formally a function of $\rvy$
as well as $\rvx$. Our architectures have pixel-wise independent
likelihoods so $p_\theta(\tildeI{}|\rvz)$ is tractable in either case. \Cref{eq:forward-elbo}
lower-bounds $\log p_{\theta,\psi}(\tildeI{}|\rvy) $ similarly to how the ELBO of
an unconditional VAE lower-bounds $\log p_\theta(\rvx)$. The major difference
is that the prior, $p_\theta(\rvz)$, is replaced by $p_\psi(\rvz|\rvy)$.
This is reflected in \cref{fig:cigcvae-forward-arch} where each $\rvz_l$ is conditioned
on $\rvy$ via the conditional prior.
%
As described in \cref{sec:conditional-vae},
reduced-variance estimates of \cref{eq:forward-elbo} can be obtained by
computing KL divergences between $q(\rvz_l|\rvz_{<l},\rvx)$ and
$p_\psi(\rvz_l|\rvz_{<l},\rvy)$ analytically for each layer $l$..

We are particularly interested in the properties of the learned conditional prior.
Recall the joint distribution $q_\phi(\rvz, \rvx, \rvy) = \pdata(\rvx,
\rvy)q_\phi(\rvz|\rvx)$. Then $q_\phi(\rvz|\rvy)$ is the intractable
posterior given by marginalising out $\rvx$ and conditioning on $\rvy$. We
find that fitting $\psi$ to maximise $\gO_\text{CVAE}(\theta,
\phi, \psi)$ is equivalent to minimising the mass-covering KL divergence
from $q_\phi(\rvz|\rvy)$ to $p_\psi(\rvz|\rvy)$. We formalise
this statement in the following theorem, which is proven in
\cref{proof:forward-kl}.
\begin{theorem} \label{theorem:forward-kl} For any set $\Psi$ of
  permissible values of $\psi$, and for any $\theta\in\Theta$ and
  $\phi\in\Phi$,
  \begin{equation} \label{eq:forward-theorem}
    \argmax_{\psi \in \Psi} \gO_\text{CVAE}(\theta, \phi, \psi) = \argmin_{\psi \in \Psi} \EX_{\pdata(\rvy)} \left[ \kl[\big]{ q_\phi(\rvz|\rvy) }{ p_\psi(\rvz|\rvy) } \right].
  \end{equation}
\end{theorem}
Due to the mass-covering properties of this ``forward'' KL
divergence~\citep{bishop2006pattern}, this theorem indicates that 
the learned $p_\psi(\rvz|\rvy)$ should have good coverage of all
modes of $q_\phi(\rvz|\rvy)$. Intuitively, the resulting diverse samples of
latent variables $\rvz\sim p_\psi(\cdot|\rvy)$ should lead to
diverse samples of $\tildeI{} \sim p_\theta(\cdot|\rvz)$ which cover the
``true'' posterior $\pdata(\tildeI{}|\rvy)$. We formalise this in
\cref{proof:forward-kl} by showing that maximising $\gO_\text{CVAE}$
also minimises an upper-bound on a KL divergence in $\tildeI{}$-space.

\subsection{Faster training with a pretrained VAE}
To justify using weights trained as part of an unconditional VAE we present
\cref{theorem:joint-training}.
%
\begin{theorem} \label{theorem:joint-training} Assume that we have a
  sufficiently expressive encoder and decoder that there exist parameters
  $\optimal{\theta}\in\Theta$ and $\optimal{\phi}\in\Phi$ which make the
  unconditional VAE objective (\cref{eq:vae-objective}) equal to its upper bound of
  $-\mathcal{H}\left[ \pdata(\rvx) \right]$. Assume also that $\tildeI{}$ is
  defined such that there is a mapping from $(\tildeI{},\rvy)$ to $\rvx$ and
  that the mutual information ${\mutinf{} := \EX_{p_{\theta^*}(\tildeI{}, \rvy,
      \rvz)} \left[ \log \frac{p_{\theta^*}(\tildeI{},\rvy|\rvz)
      }{ p_{\theta^*}(\tildeI{}|\rvz)p_{\theta^*}(\rvy|\rvz) }
    \right]}$ is zero (see discussion below). Then, given a sufficiently expressive
  conditional prior,
  \begin{equation} \label{eq:joint-training}
    \max_{\psi} \gO_\text{CVAE}(\optimal{\theta}, \optimal{\phi}, \psi) = \max_{\theta, \phi, \psi} \gO_\text{CVAE}(\theta, \phi, \psi).
  \end{equation}
\end{theorem}
This is proven in \cref{proof:joint-training} and implies that we can use values
of $\theta$ and $\phi$ learned using the unconditional VAE objective. Then to
train a conditional generative model we need only optimise $\psi$. This
leads to faster convergence, as well as faster training iterations since we only
need to compute gradients for, and perform update steps on, the conditional prior's parameters $\psi$. For all of our experiments in
\cref{sec:cigcvae-experiments} we use pretrained models released by
\citet{child2020very}, leveraging between 2 GPU-weeks and 1 GPU-year of
unconditional VAE training for each dataset. We name our method IPA (Inference
in a Pretrained Auto-Encoder).

\textcolor{black}{\Cref{theorem:joint-training} relies on the assumption that
  the mutual information $\mutinf$ is zero; as we argue in
  \cref{proof:joint-training}, this is true for inpainting and also
  ``approximately'' holds if $\rvy$ consists of high-level features. When
  lower level features are conditioned on, e.g.~for super-resolution, there may
  be a significant gap between the left- and right- hand sides of
  \cref{eq:joint-training}. } \Cref{theorem:joint-training} also applies only if
the unconditional VAE parameters are learned on the same dataset as the
conditional VAE is trained on; otherwise there will be a mismatch between the
form of $\pdata$ used in \cref{eq:vae-objective} to fit $\theta^*$ and $\phi^*$, and
the form of $\pdata$ implicit in the $\gO_\text{CVAE}$ objective.
However we find empirically that we can use unconditional VAE parameters trained
on ImageNet~\citep{deng2009imagenet} with IPA on several other photo datasets.

\subsection{Related work}
\paragraph{Inference in pretrained VAEs}
Several prior studies perform conditional generation using a previously trained
unconditional VAE.
%
Like us, \citet{rezende2014stochastic,nguyen2016plug,wu2018conditional} do so
through inference in the VAE's latent space. However, they use non-amortized
inference (Gibbs sampling, variational inference, and MCMC respectively),
leading to slow sampling times for any new $\rvy$.
%
\citet{duan2019pre} learn variational distributions over $\rvz$ for every possible
value of $\rvy$, but this is not possible when $\rvy$ is
high-dimensional or continuous-valued.
%
\citet{yeh2017semantic} fit the latent variables of a GAN given observations,
but this is neither amortized nor probabilistic.
%

\paragraph{Conditional VAEs}
Past research on conditional
VAEs~\citep{sohn2015learning,zheng2019pluralistic,ivanov2018variational,wan2021high}
has generally been unable to take advantage of pretrained weights as we have due
to a difference in architectures: unlike almost all prior work, the IPA decoder
does not receive $\rvy$ as input. The dependence between $\rvy$ and the
decoder's output must therefore be expressed solely through the conditional
distribution over the latent variables, $p_\psi(\rvz|\rvy)$. This is a crucial
difference because it means that the decoder can have exactly the same
architecture as that of an unconditional VAE. This is key to letting us copy the
pretrained weights of an unconditional VAE to speed up training. The exception
to the above is \citet{ma2018eddi} who, like us, use a conditional VAE decoder
with no dependence on $\rvy$. Their training objective and use case are
different, however, and they do not consider using pretrained models or use an
architecture which can scale to photorealistic images. Leveraging unconditional
VAEs lets us drastically reduce the computational budget required to train a
conditional VAE. We believe that the method introduced in this chapter was the first to demonstrate photorealistic image inpainting with conditional VAEs at resolutions as high as
$256\times256$.
% 
Another benefit of the decoder having no dependence on $\rvy$ is that it
makes impossible the ``posterior collapse'' phenomenon discussed by
\citet{zheng2019pluralistic}, in which a conditional VAE's decoder learns to
ignore $\rvz$ and produce outputs conditioned solely on $\mathbf{y}$.


\paragraph{Image inpainting/flexibly-conditional generative modelling}
As we have defined it, inpainting is essentially the same as flexibly-conditional generative modelling in the image domain. 
This correspondence holds poorly, though, when considering early work on image inpainting~\citep{bertalmio2000image,bertalmio2001navier,ballester2001filling,levin2003learning,criminisi2003object,kohler2014mask,ren2015shepard} which aimed to deterministically fill in missing pixels in images rather than learning a generative model.
Even many methods incorporating generative adversarial networks (GANs), which
were introduced by \citet{goodfellow2014generative} as a tool to learn
distributions, have been found to result in little or no diversity in the
completions produced for a given
input~\citep{song2018spg,yu2018generative,yu2019free,pathak2016context,iizuka2017globally}.

More recent approaches which we would consider ``flexibly-conditional generative models'' include some which have managed to obtain diverse completions using
the GAN framework~\citep{zhao2020uctgan,zhao2021large,liu2021pd}.
%
Other approaches include sampling low-resolution images using
VAEs~\citep{zheng2019pluralistic,peng2021generating} or transformer-based
sequence models~\citep{zheng2021tfill,wan2021high}, and then use a GAN for
upsampling. In contrast, we use a pure VAE which models image inpaintings at the full
resolution. As well as ensuring diverse coverage of the posterior, using such a
likelihood-based model enables applications such as out-of-distribution
detection for inputs $\rvy$, which we demonstrate in \cref{supp:cigcvae-ood}. We are aware of one prior flexibly-conditional VAE~\citep{ivanov2018variational} but it was demonstrated on lower resolution and less diverse image datasets. More recent approaches to inpainting include those of \citet{song2020score} and that of \citet{tian2023control} who use diffusion models.



\begin{table*}
  \tiny
  \caption{Comparison of our flexibly-conditional VAE and baselines on the image inpainting task with a variety of metrics. Best performance is shown in \textbf{bold},
    and second best is \underline{underlined}. In the last row, $t$ denotes the
    ``temperature'' parameter \citep{child2020very}.}
  \label{tab:cigcvae-results-completion}
  \centering
  \begin{tabular}{lccccccccc}
    \toprule
    \multicolumn{1}{r}{} & \multicolumn{3}{c}{CIFAR-10}  & \multicolumn{3}{c}{ImageNet-64}  & \multicolumn{3}{c}{FFHQ-256} \\
    \cmidrule(r){2-4} \cmidrule(r){5-7} \cmidrule(r){8-10} % \cmidrule(r){6-7} \cmidrule(r){8-9}
    Method        & \quad FID$\downarrow$ \quad     & P-IDS$\uparrow$  & LPIPS-GT$\downarrow$ & \quad FID$\downarrow$ \quad    & P-IDS$\uparrow$  & LPIPS-GT$\downarrow$& \quad FID$\downarrow$ \quad    & P-IDS$\uparrow$  & LPIPS-GT$\downarrow$ \\
    \midrule
    ANP                        & 30.03               & ~~5.86              & .0447                         & -                 & -                 & -                & 39.95              & ~~0.93              & .256 \\
    CE                         & 21.92               & ~~4.77              & .0628                         & -                 & -                 & -                & 39.02              & ~~0.66              & .267 \\
    RFR                        & 44.35               & ~~2.76              & .0883                         & -                 & -                   & -              & 72.50              & ~~0.46              & .271 \\
    PIC                        & 14.73               & ~~5.95              & .0332                         & 40.0              & 0.24                & .170           & 11.60              & ~~2.76              & .169 \\
    CoModGAN                   & ~~\underline{9.65}  & 11.59               & .0326                         & 20.2              & ~~7.09              & .160           & ~~\textbf{2.33}    & \textbf{13.57}      & .143 \\
    IPA-R                      & 19.21               & ~~8.56              & .0330                         & 28.8              & 6.46                & .166           & ~~8.82             & ~~4.56              & .142 \\
    IPA (ours)                 & 10.50               & \underline{13.24}   & \textbf{.0262}                & \underline{18.9}  & ~~\underline{9.20}  & \underline{.138}  & ~~3.93             & ~~7.79              & \underline{.123} \\
    IPA$_{t=0.85}$ (ours)      & ~~\textbf{8.61}     & \textbf{14.19}      & \underline{.0263}             & \textbf{15.1}     & \textbf{11.26}    & \textbf{.133}     & ~~\underline{3.29} & ~~\underline{8.50}  & \textbf{.117} \\
    \bottomrule
  \end{tabular}
  \vspace{-1em}
\end{table*}


\section{Experiments} \label{sec:cigcvae-experiments}

\paragraph{Comparison to image inpainting baselines}

We create an IPA inpainting model based on the VD-VAE unconditional
architecture~\citep{child2020very}, and evaluate it for inpainting on
three datasets: CIFAR-10~\citep{krizhevsky2009learning},
ImageNet-64~\citep{deng2009imagenet}, and FFHQ-256~\citep{karras2019style}. We
compare against four baselines from the prior literature: Co-Modulated Generative Adversarial Networks
(CoModGAN)~\citep{zhao2021large}; Pluralistic Image Completion
(PIC)~\citep{zheng2019pluralistic}; Context Encoders
(CE)~\citep{pathak2016context}; and Attentive Neural Processes
(ANP)~\citep{kim2019attentive}. We also considered two further methods: we show
samples from VQ-VAE~\citep{peng2021generating} (but not quantitative results
which were too slow to compute because it takes roughly one minute per test
image); and we report results for Recurrent Feature Reasoning for Image
Inpainting (RFR)~\citep{li2020recurrent} with the caveat that it is slow to run
on images with many missing pixels and so, although it used a similar
computational budget to the other models, its training did not converge.

Given pretrained unconditional VAE parameters, IPA is faster to train than the
best-performing baseline, CoModGAN. IPA takes 115 GPU-hours to train on
CIFAR-10, and under 7 GPU-weeks on FFHQ-256. The CoModGAN models are trained for
270 GPU-hours and 8 GPU-weeks respectively. We provide more training details in
\cref{supp:cigcvae-exp-details}.

We report the FID~\citep{heusel2017gans} and P-IDS~\citep{zhao2021large} metrics
between a set of sampled completions from each method and a reference set.
Broadly speaking, these measure the sample quality.
%
To investigate how well the samples capture all modes of
$\pdata(\rvx|\rvy)$, we also report the LPIPS-GT. We compute this using
LPIPS~\citep{zhang2018unreasonable}, a measure of distance between two images.
Specifically, we compute the average over test pairs $(\rvx, \rvy)$ of
$\min_{k=1}^K(\text{LPIPS}(\rvx^{(k)} , \rvx))$, with each $\rvx^{(k)} \sim
p_{\theta,\psi}(\cdot | \rvy)$. As $K \rightarrow \infty$, the LPIPS-GT should tend
to zero if the ground truth is always within the support of
$\pcomp(\rvx|\rvy)$. If not, the LPIPS-GT will remain high, penalising
methods which miss modes of the posterior. We use $K=100$. We confirm in
\cref{supp:cigcvae-additional-results} that the LPIPS-GT correlates with diversity
metrics used by related work~\citep{zhu2017toward,li2020multimodal}.


For the inpainting tasks, we sample from $\pdata(\rvx, \rvy)$ by
first sampling an image $\rvx$ from the dataset, and then sampling an
image-sized binary mask $m$ from the freeform mask distribution used by
\citet{zhao2021large}, which is itself based on \citet{yu2018generative}. We
then set $\rvy = \texttt{concatenate}(\rvx \odot m, m)$. Here, $\odot$ is a
pixel-wise multiplication operation which removes information from the missing
pixels. The concatenation is performed along the channel dimension and makes it
possible to distinguish between unobserved pixels and zero-valued pixels.

For evaluation, since the number of observed pixels in freeform masks varies
considerably, we follow \citet{zhao2021large} and partition the mask
distribution by conditioning the procedure to return a mask with the proportion
of pixels observed within some range (0-20\%, 20-40\%, and so on) and report
metrics for each range separately in \cref{fig:cigcvae-metrics} (or
\cref{supp:cigcvae-additional-results} for ImageNet-64). To summarise the
overall performance in \cref{tab:cigcvae-results-completion}, we sample masks from a
uniformly-weighted mixture distribution over these five partitions. In terms of
the LPIPS-GT scores in \cref{tab:cigcvae-results-completion}, IPA outperforms the best
baselines by roughly 20\%. \Cref{fig:cigcvae-metrics} shows that there is an improvement
for any proportion of observed pixels. This suggests that IPA produces reliably
diverse samples with good coverage of $\pdata(\rvx|\rvy)$. In contrast, we
believe that the GAN-based approaches occasionally miss modes of
$\pdata(\rvx|\rvy)$ and can therefore fail to capture the ground-truth.
This hypothesis is supported by samples from CoModGAN we display in
\cref{supp:cigcvae-comodgan-failure}. In terms of sample fidelity, as measured by both
FID and P-IDS, IPA outperforms all baselines on CIFAR-10 when over $40\%$ of the
image is observed, and on ImageNet-64 when over $20\%$ is observed. IPA comes
second to CoModGAN when less is observed and on FFHQ-256.

\begin{figure*}[t]
  %\vspace{-.1cm}
  \centering
  \includegraphics[width=\textwidth]{figs/cigcvae/metrics}
  %\vspace{-.6cm}
  \caption{Image-completion evaluation metrics on CIFAR-10 and FFHQ-256, plotted as a function of the proportion of pixels masked out. Error bars on LPIPS-GT show the standard error of our
    estimate for a single trained network.}
  \label{fig:cigcvae-metrics}
  \vspace{-.5cm}
\end{figure*}


\paragraph{Edges-to-photos}
We provide an additional demonstration of IPA on the Edges2Shoes and
Edges2Handbags datasets~\citep{isola2016image}, where the task is to generate an
image conditioned on the output of an edge detector applied to that image. We
downsample the datasets to $64\times64$ so that we can use unconditional VAEs
pretrained on ImageNet~\citep{deng2009imagenet} at this resolution by
\citet{child2020very}. We show in \cref{fig:cigcvae-training} that IPA is useful for
these tasks, and provide further discussion below. The generated images are
diverse and photorealistic, as shown in \cref{supp:cigcvae-image-samples}.



\begin{figure*}[t]
  \centering
  \includegraphics[scale=1]{figs/cigcvae/training-both}
  \caption{Comparison of training from scratch; using IPA starting from an ImageNet-pretrained model; and using IPA starting from a model pretrained on the same dataset. For each, we plot ELBO (\cref{eq:forward-elbo} computed with $\tildeI{} := \rvx$) and FID during training. Error bars show standard deviations computed with 3 runs. Using IPA makes training faster and lower-variance.}
  \label{fig:cigcvae-training}
\end{figure*}

\paragraph{Effectiveness of pretrained weights}
We now seek to determine how important the pretrained unconditional VAE weights
are to IPA. \Cref{fig:cigcvae-training} compares IPA with conditional VAEs which use IPA's architecture
but are trained from scratch, which we will refer to as ``from-scratch''
baselines. In these baselines, $\theta$ and $\phi$ are randomly initialised and trained to
maximise the conditional VAE objective (\cref{eq:forward-elbo}, with $\tildeI{} := \rvx$) along with
$\psi$.
%
With an infinite training budget, the end-to-end training of the from-scratch
baselines is likely to lead them to outperform any IPA models. Nevertheless it
is apparent from \cref{fig:cigcvae-training} that, in the more realistic situation of a
finite training budget, using IPA can be beneficial. This is the case even for
training budgets of up to a few GPU-weeks on the relatively small CIFAR-10
dataset. In fact, even with only a couple of days of training, IPA on CIFAR-10
(with CIFAR-10 pretraining) achieves better FID and ELBO scores than the
from-scratch baseline trained for several weeks. For ImageNet-64, IPA performs
better after 4 hours than the from-scratch baseline does after a week.
%
For Edges2Handbags and Edges2Shoes, training with IPA for 2 days yields
performance similar to or better than training with the from-scratch baseline
for 1 week, as measured by the ELBO. This is despite IPA on these datasets using
a trained ImageNet-64 model rather than a model pretrained on those specific
datasets, supporting our suggestion that the dataset used for pretraining need
not exactly match what IPA is then trained on.
%
Measured by the FID score, IPA's performance is even more appealing: wherever
ELBOs are similar between IPA and the from-scratch baselines, IPA achieves a
significantly better FID score.
%
We see that IPA pretrained on ImageNet is less effective for CIFAR-10
than it is for the edges-to-photos datasets, but it improves on the from-scratch
baseline in terms of ELBO for the first 30 hours of training, and in terms of
FID until the from-scratch baseline is trained for several weeks.

\paragraph{An alternative training objective}
In \cref{tab:cigcvae-results-completion} and \cref{fig:cigcvae-metrics}, we report results for
IPA-R, a variation of IPA with a different training objective corresponding to a
mode-seeking KL divergence. IPA almost always outperforms IPA-R, but we
nonetheless provide a full description of IPA-R in \cref{supp:cigcvae-ipa-r}.




\section{Inpainting for Bayesian experimental design} \label{sec:cigcvae-boed}

In this section, we explore a potential application for stochastic image inpainting that
requires a faithful representation of the posterior $\pdata(\rvx|\rvy,\gY)$.
%
In particular, we consider whether it is possible to automatically target a
chest x-ray at areas most likely to reveal abnormalities. This could avoid the
need to scan the entire chest and so bring benefits including reducing the
patient's radiation exposure.
%
While doing so is not possible with standard x-ray machines (which do not take
multiple scans consecutively), and would need to be extensively validated before
use in a clinical setting, we believe this is a worthwhile avenue to explore.
%
Specifically, our imagined system performs a series of x-ray scans, each
targeted at only a small portion of the area of interest. We can select the
coordinates $\coord_t = (x_t, y_t)$ of the location to scan at each step $t$, and
this selection can be informed by what was observed in the previous scans. The
task we consider is how to select $\coord{}_t$ to be maximally informative. In
particular, assume we wish to infer a variable $v$ representing, e.g., whether
the patient has a particular illness. Bayesian optimal experimental design
(BOED)~\citep{chaloner1995bayesian} provides a framework to select a value of
$\coord{}_t$ that is maximally informative about $v$.
%
It involves taking a Bayesian perspective on the problem of estimating $v$. We
have one posterior distribution over $v$ after taking scans at
$\coord{}_1,\ldots,\coord{}_{t-1}$ and another (typically lower entropy) distribution after
conditioning on a scan at $\coord{}_t$ as well.
%
The \textit{expected information gain}, or EIG, quantifies the utility of the
choice of $\coord{}_t$ as the expected difference in entropy between these two
distributions. Using BOED involves estimating the EIG and selecting the scan
location, $\coord{}_t$, to minimise it.

\begin{figure}[t]
  \centering
  \hspace{-.75cm}
  \begin{minipage}{.4\textwidth}
    \centering
    \includegraphics[scale=.77]{figs/cigcvae/boed-auroc-curve-rearranged}
  \end{minipage}
  ~
  \begin{minipage}{0.54\textwidth}
    \centering
    \includegraphics[scale=.77]{figs/cigcvae/shrunk-boed-vis}
  \end{minipage}
  \caption{Evaluation of our IPA-based system for selecting scan locations. \textbf{Left:} Classification AUROC scores after $1,\ldots,5$ scans
    chosen with each method. Scores for the ``EIG-'' methods more quickly approach the
    upper bound achieved by processing the full image. \textbf{Right:}
    Visualisation of IPA used within BOED used to select three scan locations for diagnosing `Effusion'. The left column shows the observations made prior to each time step. We then show samples from IPA (or the dataset when $t=1$). The
    rightmost column shows the EIG overlaid on the pixel-space average of
    sampled images, with the optimal $\coord{}_t$ marked by a red cross.}
  \label{fig:cigcvae-boed}
  \vspace{-.4cm}
\end{figure}



We use an estimator for the EIG similar to that of \citet{harvey2019near}. It
requires two components:
\textbf{(\rom{1})} A neural network trained to classify $v$ given a series of
scans at locations $\coord{}_1,\ldots,\coord{}_t$. This outputs a classification distribution
which we denote $g(v|f_{\coord{}_1,\ldots,\coord{}_t}(\rvx))$, where $f_{\coord{}_1,\ldots,\coord{}_t}$ is a
function mapping from an image to the values of the pixels observed by scans at
$\coord{}_1,\ldots,\coord{}_t$. We use this classification distribution as an approximation of
the posterior over $v$, whose entropy we attempt to minimise by performing BOED.
\textbf{(\rom{2})} A method for sampling image completions conditioned on some
observed pixel values $f_{\coord{}_1,\ldots,\coord{}_{t-1}}(\rvx)$. \citet{harvey2019near}
used a “stochastic image completion” module which contributed significant
complexity to their method. We entirely replace this with IPA.

Let the pixel values observed so far be $\rvy_{\coord{}_1,\ldots,\coord{}_{t-1}} =
f_{\coord{}_1,\ldots,\coord{}_{t-1}}(\rvx)$ for a latent image $\rvx$. Given these, we
estimate the EIG of location $\coord{}_t$ as
\begin{align}
  \label{eq:new-eig}
  \text{EIG}(\coord{}_t;& \rvy_{\coord{}_1,\ldots,\coord{}_{t-1}}) \approx \overbrace{\mathcal{H} \left[ \frac{1}{N} \sum_{n=1}^N g(\cdot|f_{\coord{}_1,\ldots,\coord{}_t}(\rvx^{(n)})) \right]}^{\text{entropy after $t-1$ scans}} - \overbrace{\frac{1}{N} \sum_{n=1}^N  \mathcal{H} \left[ g(\cdot|f_{\coord{}_1,\ldots,\coord{}_t}(\rvx^{(n)})) \right]}^{\text{expected entropy after $t$ scans}},
\end{align}
where $\rvx^{(1)},\ldots,\rvx^{(N)}$ are sampled image inpaintings from IPA
given $\rvy_{\coord{}_1,\ldots,\coord{}_{t-1}}$. In \cref{sec:cigcvae-supp-boed} we report
hyperparameters, provide further details of our EIG estimator, and compare it to
the estimators used in related work. To select $\coord{}_t$, we simply estimate
$\text{EIG}(\coord{}_t; \rvy_{\coord{}_1,\ldots,\coord{}_{t-1}})$ for many different values of
$\coord{}_t$ and select the value which maximises it. This process of selecting $\coord{}_t$
and then taking a scan is repeated for each $t=1,\ldots,T$.

We experiment on the NIH Chest X-ray 14 dataset~\citep{wang2017chestx} at
$256\times256$ resolution.
%
%
% , in which each image is labelled with the presence or
% absence of 14 illnesses. We set $v$ to be a binary label indicating the
% presence/absence of a particular illness. A simple extension, with appropriate
% training data, could be to define $v$ to be the severity of an illness, and
% therefore also infer the severity.
%
We simulate a scanner which returns a $64 \times 64$ pixel patch from this
image, and the task is to diagnose the binary presence or absence of an illness.
We run separate experiments diagnosing each of edema, effusion, infiltration and
``no finding'' (an additional label meaning there are no diagnosed illnesses).
With appropriate data, this framework could be extended to also infer the
severity of a given illness.
%
We envisage BOED being used to select scan locations for an x-ray without
necessarily performing an automated diagnosis. However, to quantify the
informativeness of the chosen locations, \cref{fig:cigcvae-boed} shows the results of
using $g$ to perform a diagnosis, or classification, based on the chosen scan
locations.
%
Since the conditional distribution $g$ (used to estimate the EIG) depends on
which illness we are classifying, the choice of scan locations is different in
each case.
%
We compare against a baseline where the image completion is performed by
CoModGAN (our best-performing image completion baseline) rather than IPA, as
well as numerous baselines which choose scan locations without image
completion; see \cref{sec:cigcvae-supp-boed} for details.

Our method (denoted EIG-IPA) narrowly but consistently outperforms EIG-CoModGAN.
%
% We hypothesise that this is due to CoModGAN's tendency to sometimes produce
% almost no diversity in its completions, even when the observed pixels are
% uninformative.
%
We hypothesise that this is due to the aforementioned tendency of CoModGAN to
sometimes collapse to a single mode of the posterior, and exhibit an example of
this behaviour on the x-ray dataset in \cref{supp:cigcvae-image-samples}. In the BOED
context, such ``overconfident'' image completion could lead to salient scan
locations being ignored. Nonetheless, both EIG-IPA and EIG-CoModGAN
significantly outperform the other baselines, giving performance much closer to
the upper bound of a CNN with access to the entire image. Another benefit of the
``EIG-'' approaches is that the choice of scan locations is highly
interpretable; we can see why a particular location was chosen with
visualisations similar to the right of \cref{fig:cigcvae-boed}. This shows the sampled
images $\rvx^{(n)}$ and the estimated EIG for each $\coord{}_t$. In \cref{sec:cigcvae-supp-boed}, we
show that we can further quantify the contribution of each $\rvx^{(n)}$ to the
estimated EIG for each $\coord{}_t$.

% \section{Discussion and Conclusion}

% We have presented IPA, a method to adapt an unconditional VAE into a conditional
% model. Image completions generated with IPA are close to the state-of-the-art in
% terms of visual fidelity, and improve on all baselines in terms of their
% coverage of the posterior as measured by LPIPS-GT. This high-fidelity coverage
% of the posterior makes IPA ideal for use in Bayesian optimal experimental
% design, as demonstrated.
% %
% % In addition, IPA has all the benefits of a
% % likelihood-based method, such as the potential to perform out-of-distribution
% % detection.
% % 
% \textcolor{black}{ Our theoretical results suggest that, for the applications
%   presented, using the weights of an unconditional VAE is approximately as good
%   as training a conditional VAE from scratch. We note however, that there are
%   applications for which these results will not hold (e.g.~super-resolution).
%   They also provide no guarantees when pretraining is performed on a different
%   dataset, although we show empirically that IPA can still be effective in this
%   case. }
% % 
% Future work may look more rigorously at these settings or further improve the
% image samples by, e.g., using a conditional prior with more expressive
% distributions.
% %
% Preliminary experiments using normalizing flows helped improve the ELBO, but
% with little impact on the FID scores.

\section{Discussion}

We have demonstrated an application of VAEs as flexibly-conditional generative models in the image domain, enabling better handling of uncertainty in image inpainting than prior methods. Doing so involved scaling flexibly-conditional VAEs to higher resolution and more diverse data than prior work~\citep{ivanov2018variational}. The fact that this relatively simple method enabled flexibly conditioning with VAEs also provides a promising signal for the potential to use flexible conditioning with other generative modelling frameworks that may emerge in the future.
%This also serves to show that flexible generative models can be built on the VAE framework as well as on the  diffusion model framework, proving that we are not limited to a single generative modelling framework and providing a positive signal about the potential of combining flexible generative modelling with other frameworks that may show promise in the future.