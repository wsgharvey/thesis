\chapter{Introduction}
\label{ch:introduction}

Progress in deep generative modeling is providing increasingly faithful models of real-world data. Deep generative mocels are now frequently used to generate images~\citep{rombach2022high,ho2022imagen} that are indistinguishable from photographs or the creations of visual artists. Generative models of language~\cite{wolf2020transformers} can generate long passages that pass as human-written. Generative models of molecules like proteins have been shown to generate novel yet feasible configurations that could revolutionise the process of drug discovery~\citep{hoogeboom2022equivariant,watson2022broadly}. 

Most of the real-world impact of generative models has come from \text{conditional} generative models. 
They have resulted in tools for illustrators, who can now write a descriptive text caption and receive an image that fits this caption~\citep{rombach2022high,ho2022imagen}. This is clearly more useful to them than receiving a random sample from the data distribution and the same is true if we consider text-conditional versus unconditional video generation~\citep{ho2022video}. In an image-editing use-case, a generative model will, at the very least, need to be conditioned on the image we wish to edit~\citep{rombach2022high,sheynin2023emu}. Large language models have achieved popularity when packaged as chat bots~\cite{wolf2020transformers} that generated responses conditioned on a user's prompts. If we want to generate plausible new drugs, sampling molecules unconditionally would be very inefficient compared to sampling molecules conditioned on them having certain properties or interacting with target molecules~\citep{watson2022broadly}. Conditional generative models can also be used as ``digital twins'' simulating a piece of machinery, e.g. a jet engine, in silico~\citep{munk2022probabilistic,fuller2020digital}. Adjusting a digital twin's conditioning inputs allows the user to, e.g., predict the effect of tuning the settings of the jet engine. Weather forecasts with state-of-the-art accuracy can be obtained by conditioning models of atmospheric dynamics on up-to-date measurements~\citep{lam2022graphcast}. Conditioning generative models of text on input audio yields a state-of-the-art speech-to-text, or transcription, system~\citep{radford2023robust} and, similarly, conditioning generative models of audio on text yields state-of-the-art text-to-speech systems~\citep{tan2024naturalspeech}.


Many of the conditional generative models in these examples were trained from scratch, which takes significant time and expense with multiple recent generative model training runs estimated to have cost in the region of 100 million US dollars in compute and hardware~\citep{knight2023openai,stanford2024artificial}. One alternative to training from scratch is to derive a conditional generative model by finetuning a previously-trained unconditional model~\citep{tian2023control,sheynin2023emu}. Doing so reduces the training cost and data requirements relative to training from scratch and is usually an acceptable solution if the number of conditioning tasks of interest, and therefore conditional models which must be finetuned, is limited. In this thesis we will explore a third approach: creating ``flexible'' generative models which are trained once in a way that enables them to be re-used for a myriad of different conditioning tasks at test-time without further training.

The specific set of conditioning tasks that we may care about is domain dependent, but a reasonable goal in domains with fixed-dimensionality data is as follows. Denoting the data $\rvx \in \mathbb{R}^d$, a flexible generative model for this domain could ideally, at test-time, generate $\rvx$ conditioned on $\rvy := \mM \odot \rvx$ for any binary mask $\mM \in \{0,1\}^d$, where $\odot$ represents element-wise multiplication. This type of flexible model could enable many new use-cases in the domains described previously. Flexible models are already widely used in image editing, in the form of image inpainting models that can be conditioned on the values of any set of image pixels and then generate plausible values for the remainder~\citep{rombach2022high,zhao2021large,harvey2021conditional}. In the video domain, such a flexible model would be capable of tasks like generating a video conditioned on: a given image being the first frame; a given image being the last frame; the first few frames; every $n$th frame (i.e. temporal super-resolution); or any of many other imaginable tasks. These tasks take on even more significance if we consider the case where the video is the first-person view of an agent. Conditioning on the first few frames may then mean conditioning on an agent's current state and conditioning on certain values of later frames could mean conditioning on the agent fulfilling some goal like navigating to a given location. The flexible video model then has the potential to provides an alternative to traditional GPS-based navigation systems. Finally, returning to the example of a digital twin, consider a digital twin for an engine in which measurements from the real engine deviate from the measurements predicted by its digital twin. This leads the user to suspect a fault in the engine. With a flexible model, the user can narrow down what the fault may be as they take more and more relevant measurements by simply conditioning the model on each measurement as they take it. This would not be possible with a standard conditional model unless the set of measurements that could be taken was limited.

The work in this thesis focuses on enabling flexible generation within the diffusion modelling framework, which has become dominant for models of images and video~\citep{sohl2015deep,ho2020denoising,dhariwal2021diffusion,rombach2022high,ho2022imagen,peebles2022scalable,brooks2024video}. We will begin this thesis by reviewing the diffusion modelling framework. We will show that diffusion models are naturally amenable to being used for conditional generation and even flexibly conditional generation of the type that we target. Our central contribution will be introducing innovations that enable flexible conditioning in challenging settings. First we consider the long video domain where the complexity and dimensionality of typical datapoints is such that it is infeasible to take the standard approach of training a single model of all frames. Next we will target flexible generative modelling in domains where the dimensionality of different data points varies and so must be sampled along with the data values given any conditioning information; this is challenging for traditional conditional generative models. Finally, we recognise that the diffusion modelling may not remain the dominant paradigm for generative modelling of images and video forever. We therefore end by demonstrating that a different generative model class, variational auto-encoders, are also amenable to being used to build flexible generative models.

\section*{Thesis outline}
In \cref{ch:diffusion} we describe diffusion models as unconditional generative models which can be fit to a data distribution $\pdata(\rvx)$. In \cref{ch:conditional-diffusion} we will build on this by reviewing conditional diffusion models which parameterise an approximation of the conditional distribution $\pdata(\rvx|\rvy)$ given conditioning information $\rvy$.

In \cref{ch:flexible-diffusion} we will define and explain the flexible diffusion framework, including our proposed training objective for flexible diffusion models and examples of flexible diffusion models.

In \cref{ch:fdm} we review the problem of learning generative models for long videos and then propose modifications to our flexible diffusion framework to adapt it to this domain. The resulting models are both 



% Deep generative models, popularly described as ``generative AI'', are increasingly finding use-cases in the real-world. Generative models of language, or ``large language models'' have become popular when packaged as chat bots~\cite{wolf2020transformers}. 
% Image generative models are deployed as ``text-to-image'' systems~\citep{rombach2022high,ho2022imagen} that are useful for quickly generating illustrations, and are used inside a new generation of image editing tools~\citep{rombach2022high,sheynin2023emu}.

% narrative is: here are a bunch of unconditional models

% imagine what we could do if they were arbitrarily conditional...

% digital twin example
% image editing
% video editing
% world modeling and planning

% video editors~\cite{}, real-time transcription systems~\cite{}, 3D modeling, protein design, weather forecasting~\citep{lam2022graphcast}, and more~\cite{}. In the context of creating human-like or animal-like intelligence, deep generative models can also be viewed as ``world models'', which describe how an agent's state and environment will change over time, and in response to its actions, and can allow for planning~\citep{ha2018world}. 

% Signature use-cases of generative AI rely on large models and expensive training runs, with the largest training runs costing over \$100 million~\cite{}. Given the cost of training these models, it is fortunate that a growing trend has been to re-use them by adapting them for different tasks~\citep{bommasani2021opportunities}. Techniques have been proposed to adapt open-source text-to-image models for image inpainting, video generation, and 3D model generators. In this way, the cost of training the base, or ``foundation'', model is shared across multiple applications. In this dissertation, we strive to go further down this route by designing models in which the final adaptation stage is unnecessary. That is, models which can be trained once and applied out-of-the-box to many different tasks. To do so we must not only train models over a broad range of tasks, but design them so that their performance is not significantly degraded versus being trained on a single task.

% Mention diffusion models being really good for this, and the dawn of them.
