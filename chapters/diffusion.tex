\chapter{Review: Diffusion Models}
\label{sec:diffusion}

Diffusion models, or DMs~\citep{sohl2015deep,ho2020denoising,nichol2021improved,song2020score}, can be understood as hierarchical variational auto-encoders with a very simple encoder: rather than parameterising a distribution with a neural network, the encoder simply adds Gaussian noise to the data. This ``encoder'' is commonly then referred to as the ``forward'' (or ``noising'') process. The equivalent of the VAE's prior and decoder are then the ``reverse'' (or ``generative'') process, which uses a learned ``denoising'' function in the form of a neural network. Sampling from a DM begins with drawing a sample from a Gaussian distribution that approximates a heavily noised sample from the data distribution. Running the reverse/generative process on this sample then gradually removes noise from it, moving the sample closer to the data manifold. By the end of the generative process, if all is trained well, the samples from this process should match the data distribution. As we will show later, fitting the DM simply involves training a neural network to predict clean data give noisy data using a mean-squared error loss. This leads to more stable training than for a VAE, in which the prior, decoder, and encoder are all learned jointly, leading to a ``moving target'' problem as each component must keep compensating for changes to the others.


\section{Sampling from a diffusion process}

\begin{figure}
    \centering
    \includegraphics[scale=1]{figs/thesis/diffusion_process.pdf}
    \caption{An example diffusion process for the data distribution $q(\rvx_0)$ shown at the top left. We use $g(t) := \sqrt{2t}$ so that $\sigma_t = t$. \textbf{Top row:} The data distribution $q(\rvx_0)$ and marginals $q(\rvx_t)$ for $t=0.3$ and $t=2$. These marginals are the convolution of the data distribution with a Gaussian distribution of standard deviation $\sigma = t$. For $t=2$, the distribution looks near-Gaussian. \textbf{Middle row:} The distributions from the top row shown in log-space. The green tangent arrows represent $\nabla_{\rvx_t} \log q(\rvx_t)$, the score functions that a neural network would be trained to estimate. \textbf{Bottom row:} The continuous evolution of the forward process for all $t$ between $0$ and $2$. The marginals plotted in the top rows correspond to the ``slices'' made by the dashed green lines. Each faded white line marks a percentile of the $q(\rvx_t)$. The red lines are stochastic trajectories sampled with the SDE described in \cref{sec:diffusion-forward-sde}. The blue line is sampled with the ODE described in \cref{sec:diffusion-forward-ode} and parameterized with the score estimates from the second row.}
    \label{fig:diffusion-overview}
\end{figure}


\begin{table*}
  \caption{Definitions for symbols used.}
  \label{tab:notation}
  \centering
  \begin{tabular}{rp{9cm}}
    \toprule
    Symbol    & Definition   \\
    \midrule
    $\rvx$                                  & Data which we wish to learn a generative model of. \\
    $\rvy$                                  & Data on which the generative model should be conditioned. \\
    $\pdata(\rvx, \rvy)$                    & Data distribution. \\
    \midrule
    $t$                                     & Time in relation to the diffusion SDE and ODE.  \\
    $\rvx_t$                                & ``Noisy'' data at time $t \geq 0$ in the diffusion process.  \\
    $q(\rvx,\rvy,\rvx_{t_1},\ldots,\rvx_{t_n})$   & Joint distribution defined by the data distribution and forward SDE for any $t_1,\ldots,t_n \geq 0$. We will also use $q$ to denote any conditional or marginal of such a distribution.  \\
    $g(t)$                                  & Scaling factor for Wiener process noise added in the forward SDE at time $t$. Defined to be positive for all $t > 0$. \\
    $\sigma(t)$                             & Given by $\sigma(t)^2 = \int g(t)^2 \mathrm{d}t$. Standard deviation of noise added by the forward SDE between time $0$ and $t$. \\
    \midrule
    $\sigma$                                & Equivalent to $\sigma(t)$. When it simplifies results, we will define the process explicitly in terms of $\sigma$ instead of using $t$. \\
    $\rvx_\sigma$                           & ``Noisy'' data described in terms of the standard deviation of added noise $\sigma$ instead of the time $t$. Equivalent to $\rvx_t$ for $t$ satisfying $\sigma = \sigma_t$. \\
    $q(\rvx,\rvy,\rvx_{\sigma_1},\ldots,\rvx_{\sigma_n})$   & Equivalent to $q(\rvx,\rvy,\rvx_{t_1},\ldots,\rvx_{t_n})$ if each $\sigma_i = \sigma(t_i)$. This distribution is agnostic to $g(t)$. \\
    SNR                                     & The signal-to-noise ratio, given by $\frac{1}{\sigma^2}$. \\
    \bottomrule
  \end{tabular}
\end{table*}

\subsection{The forward process as an SDE} \label{sec:diffusion-forward-sde}
This forward process is defined through a combination of the initial condition $\rvx_0 \sim \pdata(\cdot|\rvy)$ and the stochastic differential equation (SDE):\footnote{We describe here the ``variance-exploding'' diffusion process. We generalise to a broader family of forward processes in ..., which require more complex notation.}
\begin{equation} \label{eq:forward-diffusion}
    \mathrm{d}\rvx = g(t) \mathrm{d}\rvw
\end{equation}
defined for all $t \geq 0$, with $\rvw$ being the standard Wiener process, also known as Brownian motion. The rate at which noise is added, $g(t)$, is a hyperparameter. We will keep this exposition general by not assuming any value for it, but note that \citet{karras2022elucidating} show that setting $g(t) := \sqrt{2t}$ yields good performance.

For any finite set of times $t_1,\ldots,t_n$, the forward process defines a joint distribution over all of $\rvx_{t_1},\ldots,\rvx_{t_n}$, $\rvx$, and $\rvy$. We will denote such joint distributions as $q(\rvx,\rvy,\rvx_{t_1},\ldots,\rvx_{t_n})$, and similarly denote marginal or conditional distributions derived from them with $q$, with the specific distribution of interest being clear from variable names. Note that $q(\rvx_0,\rvy) = \pdata(\rvx_0,\rvy)$ is simply the data distribution. Many conditional distributions that follow from this are tractable, including the distribution over $\rvx_t$ conditioned on $\rvx$:
\begin{equation}
    q(\rvx_t|\rvx,\rvy) := \gN \left( \rvx_t; \rvx, \sigma(t)^2 \mI \right) \quad \text{where} \quad \sigma(t)^2 := \int_0^t g(t')^2 \mathrm{d}t'.
\end{equation}
If we view the forward process as adding noise to clean data then $\sigma(t)$ can be viewed as the ``noise-to-signal ratio'' of $\rvx_t$ and, if we follow \citet{karras2022elucidating} by setting $g(t) := \sqrt{2t}$ then we have the simple relationship that $\sigma(t) = t$ for all $t\geq0$.

If $t$ is large and correspondingly $\sigma(t)$ is large, $q(\rvx_t|\rvx,\rvy)$ will be very well approximated by the Gaussian $\gN(0; \sigma(t)^2\mI)$. In particular, we can observe that the KL divergence
\begin{equation}
    \kl{q(\rvx_t|\rvx,\rvy)}{\gN(0; \sigma(t)^2\mI)} :=\frac{1}{2\sigma(t)^2} \rvx^\top \rvx
\end{equation}
diminishes with $\sigma(t)^2$. This forward process can therefore be used to map from clean data $\rvx$ to data that is arbitrarily well-approximated by a simple distribution. In the next section we will discuss how to do the more challenging counterpart: transforming samples from a simple distribution into samples from the data distribution.

\subsection{The reverse SDE} \label{sec:diffusion-forward-sde}
Our aim in this section is to ``invert'' the forward process from the previous section, yielding a way to create clean data from noise. To do so we refer to a result  derived by \citet{anderson1982reverse}, and recalled in the diffusion modelling context by \citet{song2020score}, showing that the reverse of a diffusion process is also a diffusion process. A straightforward application of this result to the diffusion SDE in \cref{eq:forward-diffusion} reveals that the corresponding reverse-time diffusion SDE is
\begin{equation} \label{eq:reverse-diffusion}
    \mathrm{d}\rvx = -g(t)^2 \nabla_\rvx \log q(\rvx_t|\rvy) \mathrm{d}t + g(t) \mathrm{d}\bar{\rvw}
\end{equation}
with $\bar{\rvw}$ is a standard Wiener process when time flows backward from $\infty$ to $0$ and $\nabla_\rvx \log q(\rvx_t)$ is the generally intractable score of $q(\rvx_t)$ as defined through the forward process. The task of the neural network in a diffusion model is to estimate $\nabla_{\rvx_t} \log q(\rvx_t)$, and we will see in \cref{sec:diffusion-training} how this is done. Assuming that we have such an estimator, which we call $s_\theta(\rvx_t, \rvy, t) \approx \nabla_{\rvx_t} \log q(\rvx_t)$, we can define a generative model as follows. Given large $T \in \mathbb{R}^{>0}$, we sample $\rvx_T \sim \gN(\vzero, \sigma(t)^2 \mI)$; then we simulate the SDE
\begin{equation} \label{eq:reverse-diffusion-with-nn}
    \mathrm{d}\rvx = -g(t)^2 s_\theta(\rvx_t, \rvy, t) \mathrm{d}t + g(t) \mathrm{d}\bar{\rvw}
\end{equation}
from time $t=T$ to $t=0$ with any SDE solver.\footnote{Possible choices include Euler integration~\citep{ho2020denoising}, more complex Runge-Kutta~\citep{grathwohl2018ffjord} solvers, or a Heun solver~\citep{karras2022elucidating}.} We will call the distribution over $\rvx$ parameterized by this process $p_\theta^\text{SDE}(\rvx)$. It approximates the data distribution $\pdata(\rvx)$, with any approximation error being due to (1) imperfect fitting of the score function; and (2) the approximation of $q(\rvx_T)$ with a Gaussian. If $T$ is chosen to be large enough, source (1) will dominate. In practice there will also be some approximation error from numerically simulating the SDE, but we consider $p_\theta^\text{SDE}(\rvx)$ to be the distribution achieved with perfect SDE integration.

% Assuming that we have such an estimator, \cref{eq:reverse-diffusion} provides a way to morph from $\rvx_T$ sampled from a Gaussian distribution for some large $T$, to approximate samples from the data distribution $\rvx_0 \sim \pdata(\cdot|\rvy)$. To do so, we only need a tool for integrating SDEs. This can be simple like Euler integration, or may be a more complex Runge-Kutta~\citep{grathwohl2018ffjord} or Heun~\citep{karras2022elucidating} integration method.

\subsection{A reverse ODE} \label{sec:diffusion-ode}
In this section we will describe how to replace the SDE that parameterizes our generative model with an ODE. This can make the integration simpler and we will describe how it enables exact likelihood evaluation for any given $\rvx$.  \citet{song2020score} show that, for any diffusion process, there exists a deterministic process whose trajectories share the same marginal distributions $\{q(\rvx_t)\}_t$ as the stochastic diffusion process. This deterministic process is described by the ODE
\begin{align}
    \mathrm{d}\rvx &= - \frac{1}{2} g(t)^2 \nabla_\rvx \log q(\rvx_t|\rvy) \mathrm{d}t \\
    &\approx - \frac{1}{2} g(t)^2 \nabla_\rvx s_\theta(\rvx_t, \rvy, t) \mathrm{d}t. \label{eq:diffusion-ode-with-nn}
\end{align}
We can now use the same generative model described in \cref{sec:diffusion-forward-sde} except that we integrate an ODE instead of an SDE. We will call the resulting distribution $p_\theta^\text{ODE}(\rvx)$. Note that this distribution will not necessarily be identical to $p_\theta^\text{SDE}(\rvx)$ if the approximation of the score function is imperfect, but both will approximate the data distribution.

Sampling from our model now involves simply sampling $\rvx_T \sim \gN(\vzero, \sigma(t)^2\mI)$ and then computing $\rvx_0 = f^{T:0}_\theta(\rvx_T)$, where $f^{T:0}_\theta$ is a function that simulates \cref{eq:diffusion-ode-with-nn} from time $T$ to $0$. Importantly, $f^{T:0}_\theta$ is deterministic and, since it comes from the integration of an ODE, invertible. If we assume that $f^{T:0}_\theta$ is also continuously differentiable then we can apply the change of variables rule to find
\begin{equation}
    p_\theta^\text{ODE}(\rvx_0) = p_\theta(\rvx_T) \left| \det \mJ_{f^{T:0}_\theta}(\rvx_T) \right|^{-1}
\end{equation}
where $\det \mJ_{f^{T:0}_\theta}(\rvx_T)$ is the determinant of the Jacobian of $f^{T:0}_\theta$ evaluated at $\rvx_T$. Following \citet{chen2018neural}, this can be efficiently estimated by integrating the trace
\begin{equation}
    \text{Tr}\left( \frac{\partial}{\partial \rvx_t} \left( - \frac{1}{2} g(t)^2 \nabla_\rvx s_\theta(\rvx_t, \rvy, t) \right)
    \right)
\end{equation}
of the Jacobian of the ``drift'' term in \cref{eq:diffusion-ode-with-nn} along the path of $\rvx_t$ while simulating the ODE. \citet{song2020score} showed that diffusion models can yield statae-of-the-art data log likelihoods when evaluated in this way.

% A downside is that it has been observed empirically that not adding noise during integration can lead to more error build up if the approximation of $\nabla_\rvx \log q(\rvx_t|\rvy)$ is imperfect~\citep{karras2022elucidating}. We will describe in \cref{TODO} more advanced techniques which can combine the advantages of adding noise during sampling with those of this deterministic approach.

% The other major advantage of having an ODE is that it allows for exact evaluation of the likelihood of any given data point under the distribution defined by the model.


\section{Learning the score function} \label{sec:diffusion-training}

\subsection{Training is agnostic to the specific mapping between $t$ and $\sigma$}
The objective, when training a diffusion model, is to obtain a good estimate of the score function, $s_\theta(\rvx_t, \rvy, t) \approx \nabla_{\rvx_t} \log q(\rvx_t)$ for all $t$ of interest so that we can use it to parameterise our SDE or ODE. Given that $q(\rvx_t) = \int q(\rvx) \gN(\rvx_t; \rvx, \sigma_t^2\mI) \mathrm{d}\rvx$, we can equivalently state our objective as obtaining a good estimate of 
\begin{equation}
\nabla_{\rvx_t} \log q(\rvx_t) = \nabla_{\rvx_t} \log \int q(\rvx) \gN(\rvx_t; \rvx, \sigma_t^2\mI) \mathrm{d}\rvx
\end{equation}
for all $\sigma(t)$ of interest.

This does not depend on $t$ except through $\sigma(t)$ and so for the rest of this chapter we will consider the problem of learning a score function for given values of $\sigma$ without considering how they relate to $t$. This means that we can re-use the same learned score function estimator even if we decide to change the relationship between $\sigma$ and $t$ by changing the value of $g(t)$ in the forward SDE (\cref{eq:forward-diffusion}). We can therefore disentangle the learning of a diffusion model from the definition of the ODEs and SDEs used to sample from it. To make this clear, in the remainder of this section when we have in mind a particular noise-to-signal ratio $\sigma=\sigma_t$ and wish to refer to the corresponding variable $\rvx_t$, we will do so without reference to $t$ by simply writing $\rvx_\sigma$. We will also now write the score estimate as $s_\theta(\rvx_\sigma, \rvy, \sigma)$ so that it takes $\sigma$ as an argument instead of $t$. In the following, we derive an objective for matching $s_\theta(\rvx_\sigma, \rvy, \sigma)$ to $\nabla_{\rvx_\sigma} \log q(\rvx_\sigma |\rvy)$~\citep{vincent2011connection,song2019generative}. These scores are the gradients depicted by arrows in \cref{fig:diffusion-overview}.

\subsection{An objective for learning to estimate the score function}

We first point out that in \cref{sec:diffusion-ode} we described how to compute the likelihoods of data with respect to our diffusion model. One solution to training the diffusion model would be to explicitly optimise this objective. This has been done in the past, and the resulting models are called continuous normalising flows~\cite{chen2018neural}. We will instead derive an alternative based on minimising the mean squared error of an estimate of a score function. We will call this the implicit score-matching objective $\gL_\text{ISM}$. It has two benefits over the maximum likelihood objective. First, $\gL_\text{ISM}$ is cheaper to train with since its possible to obtain an unbiased estimate of it while evaluating the neural network only once on each data point, instead of simulating a full ODE trajectory as is required for the maximum likelihood objective. Second, $\gL_\text{ISM}$ is a more stable objective; we will see that it can be framed as a simple mean-squared error objective. We will then in \cref{sec:diffusion-likelihood} see the relationship between $\gL_\text{ISM}$ and maximum likelihood.

% One may ask why, then, we derive the $\gL_\text{ISM}$ objective in \cref{eq:diffusion-loss-all-sigma} instead of directly optimising the likelihood of the training data. One reason is that the exact likelihood is expensive to compute since it requires an entire simulation of the ODE trajectory. In this section we show, however, that an appropriate choice of $\lambda(\sigma)$ turns the objective proposed in \cref{eq:diffusion-loss-all-sigma} into a lower bound on the data likelihood. This enables likelihood-based training with a cheap-to-evaluate training objective.

To learn to estimate the score function, an ideal objective would be the mean-squared error between the neural network's outputs and the desired score function. We call this the score-matching objective $\gL_\text{SM}$,
\begin{align}
    \mathcal{L}_\text{SM}(\theta, \sigma) &= \EX_{q(\rvx_\sigma, \rvy)} \left[ || \nabla_{\rvx_\sigma} \log q(\rvx_\sigma|\rvy) - \rvs_\theta(\rvx_\sigma, \rvy, \sigma) ||_2^2 \right],
\end{align}
but in general $\gL_\text{SM}$ is intractable because we do not have access to $\nabla_{\rvx_\sigma} \log q(\rvx_\sigma|\rvy)$. We can, however, obtain an unbiased estimate of $\gL_\text{SM}$. To do so we begin by breaking it down as
\begin{align}
    \EX_{q(\rvx_\sigma, \rvy)} \big[ 
    || \rvs_\theta(\rvx_\sigma, \rvy, \sigma) ||_2^2
    % \nonumber\\ &\qquad\qquad\quad
    - 2 \left\langle \nabla_{\rvx_\sigma} \log q(\rvx_\sigma|\rvy),
    \rvs_\theta(\rvx_\sigma, \rvy, \sigma) \right\rangle \big]
    % \nonumber\\ &\quad
    + C_1
\end{align}
where $C_1$ does not depend on $\theta$. We show in \cref{sec:proof-that-diffusion-does-score-matching} that this second term can be rewritten in terms of the tractable score of the Gaussian $q_{\sigma}(\rvx_\sigma|\rvx,\rvy)$ instead of the intractable score of $q_{\sigma}(\rvx_\sigma|\rvy)$:
\begin{align}
\allowdisplaybreaks
    &\EX_{q(\rvx_\sigma, \rvy)} \left[ - 2 \left\langle \nabla_{\rvx_\sigma} \log q(\rvx_\sigma|\rvy), \rvs_\theta(\rvx_\sigma, \rvy, \sigma) \right\rangle \right] \\
    =& \EX_{q(\rvx, \rvx_\sigma, \rvy)} \left[ -2 \left\langle \nabla_{\rvx_\sigma} \log q(\rvx_\sigma|\rvx), \rvs_\theta(\rvx_\sigma, \rvy, \sigma) \right\rangle  \right].
\end{align}
Substituting this identity into $\mathcal{L}_\text{SM}(\theta, \sigma)$, and letting $C_2$ be a scalar that do not depend on $\theta$, we get
\begin{align}
    \mathcal{L}_\text{SM}(\theta, \sigma) &= \EX_{q(\rvx, \rvx_\sigma, \rvy)} \big[ 
    || \rvs_\theta(\rvx_\sigma, \rvy, \sigma) ||_2^2
    \nonumber\\ &\qquad\qquad\qquad
    - 2 \left\langle \nabla_{\rvx_\sigma} \log q(\rvx_\sigma|\rvx), \rvs_\theta(\rvx_\sigma, \rvy, \sigma) \right\rangle \big] + C_1 \\
    &= \EX_{q(\rvx, \rvx_\sigma, \rvy)} \big[ 
    || \rvs_\theta(\rvx_\sigma, \rvy, \sigma) ||_2^2
    \nonumber\\ &\qquad\qquad\qquad
    - 2 \left\langle \nabla_{\rvx_\sigma} \log q(\rvx_\sigma|\rvx), \rvs_\theta(\rvx_\sigma, \rvy, \sigma) \right\rangle
    \nonumber\\ &\qquad\qquad\qquad
    + || \nabla_{\rvx_\sigma} \log q(\rvx_\sigma|\rvx) ||_2^2 \big] + C_2 \\
    &= \EX_{q(\rvx, \rvx_\sigma, \rvy)} \big[ 
    || \rvs_\theta(\rvx_\sigma, \rvy, \sigma) - \nabla_{\rvx_\sigma} \log q(\rvx_\sigma|\rvx) ||_2^2 \big] + C_2.
\end{align}
Introducing the analytic score of the Gaussian $\rvx$, and defining our implicit score-matching loss as $\mathcal{L}_\text{ISM}(\theta, \sigma) := \mathcal{L}_\text{SM}(\theta, \sigma) - C_2$, we get
\begin{align} \label{eq:ism-loss}
    \mathcal{L}_\text{ISM}(\theta, \sigma) &= \EX_{q(\rvx, \rvx_\sigma, \rvy)} \left[ 
    || \rvs_\theta(\rvx_\sigma, \rvy, \sigma) - \frac{\rvx-\rvx_\sigma}{\sigma^2} ||_2^2 \right]
\end{align}
We can therefore train a diffusion model with the simple objective of a mean-squared error loss between the neural network's output and $\frac{\rvx-\rvx_\sigma}{\sigma^2}$.

Recall that, for sampling, we need the score function estimate to be accurate for a range of $\sigma$. In particular, we will consider a range $\sigma_\text{min}$ to $\sigma_\text{max}$, chosen such that $\sigma_\text{min}$ is sufficiently close to zero.\footnote{Setting $\sigma_\text{min}$ to exactly zero would cause problems when we divide by $\sigma$, but we can allow it to get arbitrarily close to zero so that $q(\rvx_{\sigma_\text{min}})$ is arbitrarily close to the data distribution.} We need $s_\theta(\rvx_\sigma, \rvy, \sigma)$ to be a good approximation of the score for all $\sigma$ within this range. We therefore make the loss an integral of \cref{eq:ism-loss} over $\sigma$:
\begin{align} \label{eq:diffusion-loss-all-sigma}
    \mathcal{L}_\text{ISM}(\theta) &= \int_{\sigma_\text{min}}^{\sigma_\text{max}} \lambda(\sigma) \EX_{q(\rvx, \rvx_\sigma, \rvy)} \left[ 
    || \rvs_\theta(\rvx_\sigma, \rvy, \sigma) - \frac{\rvx-\rvx_\sigma}{\sigma^2} ||_2^2 \right] \mathrm{d}\sigma
\end{align}
where $\lambda(\sigma)$ is a weighting function that controls how much network capacity is spent modelling the score function at each noise level. In the following we will describes how $\lambda(\sigma)$ can be set to yield a lower-bound on the data likelihood, or to maximise perceptual quality.

\subsection{Equivalence of score function prediction and data reconstruction} \label{eq:equivalent-parameterisations}
In the previous section we described an objective that trains the neural network to predict the score function $\nabla_{\rvx_\sigma} q(\rvx_\sigma|\rvx,\rvy) = \frac{\rvx-\rvx_\sigma}{\sigma^2}$ given $\rvx_\sigma$ and $\sigma$ in a way that is optimal in terms of mean-squared error, and showed that the same estimate is also an optimal prediction of $\nabla_{\rvx_\sigma} q(\rvx_\sigma|\rvy)$. In this section, we show that this estimate can be used to derive estimates of several other quantities that are also optimal in terms of mean-squared error. First, consider the problem of estimating the clean data $\rvx$. To do so, we can rewrite \cref{eq:diffusion-loss-all-sigma} as
\begin{align} \label{eq:x0-diffusion-loss}
    \mathcal{L}_\text{ISM}(\theta) &= \int_{\sigma_\text{min}}^{\sigma_\text{max}} \frac{\lambda(\sigma)}{\sigma^4} \EX_{q(\rvx_0, \rvx_\sigma, \rvy)} \left[ 
    || (\rvx_\sigma + \sigma^2 \cdot \rvs_\theta(\rvx_\sigma, \rvy, \sigma)) - \rvx ||_2^2 \right] \mathrm{d}\sigma.
\end{align}
This reveals that, while optimising the mean-squared error estimate of the score function in \cref{eq:diffusion-loss-all-sigma}, we are implicitly also optimising a mean-squared error estimate of $\rvx$ which is given by $\rvx_\sigma + \sigma^2 \cdot \rvs_\theta(\rvx_\sigma, \rvy, \sigma)$. We will from now on denote such an estimator $\rvx_\theta(\rvx_\sigma, \rvy, \sigma) := \rvx_\sigma + \sigma^2 \cdot \rvs_\theta(\rvx_\sigma, \rvy, \sigma)$.

Another quantity that is often of interest is $\epsilon$, which can be understood as the unit Gaussian-distributed noise used during reparameterised sampling of $\rvx_\sigma$ given $\rvx$ as $\rvx_\sigma = \rvx + \sigma \cdot \epsilon$. We can therefore write $\epsilon$ as $\frac{\rvx_\sigma-\rvx}{\sigma}$. To obtain the optimal estimate of $\epsilon$, we can rewrite \cref{eq:diffusion-loss-all-sigma} as
\begin{align} \label{eq:epsilon-diffusion-loss}
    \mathcal{L}_\text{ISM}(\theta) &= \int_{\sigma_\text{min}}^{\sigma_\text{max}} \frac{\lambda(\sigma)}{\sigma^2} \EX_{q(\rvx, \rvx_\sigma, \rvy)} \left[ 
    || -\sigma \cdot \rvs_\theta(\rvx_\sigma, \rvy, \sigma) - \frac{\rvx_\sigma-\rvx}{\sigma} ||_2^2 \right] \mathrm{d}\sigma.
\end{align}
It is therefore clear that we can obtain an estimate of $\epsilon$ from a learned score function as $-\sigma \cdot \rvs_\theta(\rvx_\sigma, \rvy, \sigma)$. We will from now on denote such an estimator $\mathbf{\epsilon}_\theta(\rvx_\sigma, \rvy, \sigma) := -\sigma \cdot \rvs_\theta(\rvx_\sigma, \rvy, \sigma)$.

While we have described the perspective of learning an estimator for the score function and then using it to predict these other quantities if required, the problem of training a diffusion model can equally be framed as one of learning an estimator for e.g. $\rvx$ or $\epsilon$ using \cref{eq:x0-diffusion-loss} or \cref{eq:epsilon-diffusion-loss} and then, if required, deriving an estimate of the score function from it. This is equivalent as long as the weighting function is scaled appropriately as in \cref{eq:x0-diffusion-loss,eq:epsilon-diffusion-loss} and the parameterisation of the neural network's output is handled appropriately as we will discuss in 
\cref{sec:diffusion-architecture}. 

\subsection{Weighting the loss to lower-bound data likelihood} \label{sec:diffusion-likelihood}
% In \cref{sec:diffusion-ode} we describe how to compute the likelihoods of data with respect to our diffusion model. One may ask why, then, we derive the $\gL_\text{ISM}$ objective in \cref{eq:diffusion-loss-all-sigma} instead of directly optimising the likelihood of the training data. One reason is that the exact likelihood is expensive to compute since it requires an entire simulation of the ODE trajectory. In this section we show, however, that an appropriate choice of $\lambda(\sigma)$ turns the objective proposed in \cref{eq:diffusion-loss-all-sigma} into a lower bound on the data likelihood. This enables likelihood-based training with a cheap-to-evaluate training objective.

We now see how the score-matching objective proposed previously can be used to construct a lower-bound on the data likelihood. To do so, we consider a technique to sample trajectories from the SDE in \cref{eq:reverse-diffusion} by simulating it from a time corresponding to $\sigma_\text{max}$ to a time corresponding to some $\sigma_\text{min}$ close to zero. To do so, we discretise time into points $N,\ldots,0$. Similarly to the previous section, we will define only the signal-to-noise ratio and not consider the time itself. In particular, we define the points such that the noise standard deviation for point $i$ is
\begin{equation}
    \sigma(i) = \frac{i}{N}(\sigma_\text{max} - \sigma_\text{min}) + \sigma_\text{min}.
\end{equation}
Then, to simulate the reverse SDE, we perform steps $N,\ldots,1$ and at each step $i$ transition from a state with noise standard deviation $\sigma(i)$ to one with noise standard deviation $\rho(i) := \sigma(i-1)$. For brevity in the remainder of this section we will from now on write $\sigma(i)$ as simply $\sigma$ and $\rho(i)$ as $\rho$ when the index is clear from the context. This simulation results in a joint distribution over trajectories of
\begin{align} \label{eq:diffusion-reverse-joint-prob}
    p_\theta(\rvx,\rvx_{\sigma(0)},\ldots,\rvx_{\sigma(N)}) = p(\rvx_{\sigma(N)}) \left( \prod_{i=1}^N p_\theta(\rvx_{\rho(i)} | \rvx_{\sigma(i)}) \right) p_\theta(\rvx|\rvx_{\sigma(0)})
\end{align}
where $p(\rvx_{(N)})$ is the Gaussian approximation of $q(\rvx_{t(N)})$ with no learnable parameters; $p_\theta(\rvx_{\rho(i)} | \rvx_{\sigma(i)})$ is a step parameterised by our score function estimate; and $p_\theta(\rvx|\rvx_{\sigma(0)})$ is a final ``likelihood'' to remove all noise from data which may also make use of the score function. The ``data likelihood'' is then a marginal of this joint distribution, $p_\theta(\rvx)$, evaluated on our training data. In the following we describe how we can use \cref{eq:diffusion-reverse-joint-prob} to construct a lower-bound on $p_\theta(\rvx)$, before expanding on the form of each $p_\theta(\rvx_{\rho(i)}|\rvx_{\sigma(i)})$.

% To construct the lower bound, we consider sampling from the SDE in \cref{eq:reverse-diffusion} by simulating it from time $t_\text{max}$ corresponding to noise-to-signal ratio $\sigma_\text{max}$ to a time $t_\text{min}$ which is close to zero.
% A simple method for simulation is to discretise it into $N$ steps. At each step $i = N,\ldots,1$ we then morph $\rvx_{t(i)}$ into $\rvx_{s(i)}$, where
% \begin{equation}\nonumber
% t(i) := \frac{i}{N} (t_\text{max} - t_\text{min}) + t_\text{min} \quad \text{and} \quad s(i) := t(i-1).
% \end{equation}
% For brevity we will from now on write $t(i)$ as simply $t$ and $s(i)$ as simply $s$ when the index is clear from the context. We can write the joint probability of a trajectory
% \begin{align} \label{eq:diffusion-reverse-joint-prob}
%     p_\theta(\rvx,\rvx_{t(0)},\ldots,\rvx_{t(N)}) = p(\rvx_{t_N}) \left( \prod_{i=1}^N p_\theta(\rvx_{s(i)} | \rvx_{t(i)}) \right) p_\theta(\rvx|\rvx_{t(0)})
% \end{align}
% where $p(\rvx_{(N)})$ is the Gaussian approximation of $q(\rvx_{t(N)})$ with no learnable parameters, while $p_\theta(\rvx_{s(i)} | \rvx_{t(i)})$ is parameterised by our score function estimate. We would like to match a marginal of this, $p_\theta(\rvx_{t_\text{min}})$, to the data distribution, by fitting $\theta$ to maximise $p_\theta(\rvx_0)$ for $\rvx_0$ sampled from the data distribution. We next describe how we use this joint distribution to construct a lower-bound on $p_\theta(\rvx_0)$, before expanding on the form of each $p_\theta(\rvx_{s(i)}|\rvx_{t(i)})$.

We start by noting that the forward process yields an analytically tractable form of $q(\rvx_{\sigma(0)},\ldots,\rvx_{\sigma(N)}|\rvx)$ which we can express as
\begin{equation}
    q(\rvx_{\sigma(0)},\ldots,\rvx_{\sigma(N)}|\rvx) = q(\rvx_{\sigma(N)}|\rvx) \prod_{i=1}^N q(\rvx_{\rho(i)} | \rvx_{\sigma(i)}, \rvx).
\end{equation}We combine it with \cref{eq:diffusion-reverse-joint-prob} to construct the following lower-bound of $p_\theta(\rvx)$:
\begin{align}
    &\log p_\theta(\rvx) \\
    \geq &\log p_\theta(\rvx) - \kl{q(\rvx_{\sigma(0)},\ldots,\rvx_{\sigma(N)}|\rvx)}{p_\theta(\rvx_{\sigma(0)},\ldots,\rvx_{\sigma(N)}|\rvx)} \\
    = &\EX_{q(\rvx_{\sigma(0)},\ldots,\rvx_{\sigma(N)}|\rvx)} \left[ \log \frac{p(\rvx_{\sigma(N)}) \prod_{i=1}^N p_\theta(\rvx_{\rho} | \rvx_{\sigma}) p_\theta(\rvx|\rvx_{\sigma(0)}) }{q(\rvx_{\sigma(N)}|\rvx) \prod_{i=1}^N q(\rvx_{\rho} | \rvx_{\sigma}, \rvx)} \right] \\
    = &\kl{q(\rvx_{\rho(N)}|\rvx)}{p(\rvx_{\sigma(N)})} + \EX_{q(\rvx_{\sigma(0)}|\rvx)} \left[ - \log p_\theta(\rvx|\rvx_{\sigma(0)}) \right] + \mathcal{L}_N(\theta, \rvx) \label{eq:full-diffusion-elbo}
\end{align}
where
\begin{align}
    \mathcal{L}_N(\theta, \rvy) = \sum_{i=1}^N \EX_{q(\rvx_{\sigma}|\rvx)} \left[ \kl{q(\rvx_{\rho} | \rvx_{\sigma}, \rvx)}{p_\theta(\rvx_{\rho} | \rvx_{\sigma})} \right].
\end{align}
Following \citet{kingma2021variational}, we will call the first line of \cref{eq:full-diffusion-elbo} the ``prior loss''; the second line the ``reconstruction loss''; and the third line the ``diffusion loss''. The prior loss is not improved by optimising $\theta$ but can be made arbitrarily small by choosing large $\sigma_\text{max}$. The reconstruction loss can similarly be made arbitrarily small by choosing small $\sigma_\text{min}=\sigma(0)$, even with a simple parameterisation of $p_\theta(\rvx|\rvx_{\sigma(0)})$. Therefore, given appropriate choices of $\sigma_\text{min}$ and $\sigma_\text{max}$, the lower-bound will be dominated by the diffusion loss $\mathcal{L}_t(\rvx)$. For the remainder of this section we will focus on estimating $\mathcal{L}_N(\theta, \rvx)$.

To evaluate $\mathcal{L}_N(\theta, \rvx)$, we first make explicit the form of each of $q(\rvx_{\rho} | \rvx_{\sigma}, \rvx)$ and $p_\theta(\rvx_{\rho} | \rvx_{\sigma})$. From the definition of the forward process, we have that
\begin{align} \label{eq:q-step-pdf}
    q(\rvx_{\rho} | \rvx_{\sigma}, \rvx) &= \frac{q(\rvx_{\rho} | \rvx) q(\rvx_{\sigma} | \rvx_{\rho})}{q(\rvx_{\sigma}|\rvx)} = \frac{\gN(\rvx_{\rho}; \rvx, \sigma_{\rho}^2 \mI) \gN(\rvx_{\sigma}; \rvx_{\rho}, \sigma^2-\rho^2 \mI) }{\gN(\rvx_{\sigma}; \rvx, \sigma_{\sigma}^2 \mI)}
\end{align}
We can exploit the conjugacy of Gaussians to simplify \cref{eq:q-step-pdf} to the Gaussian
\begin{align}
    q(\rvx_\rho|\rvx_\sigma,\rvx) = \gN( \rvx_\rho; \mathbf{\mu}_Q(\rvx_\sigma, \rvx; \rho, \sigma), \sigma_Q^2(\rho, \sigma) \mathbf{I})
\end{align}
where
\begin{align}
    \mathbf{\mu}_Q(\rvx_\sigma, \rvx; \rho, \sigma) &:= \frac{\rho^2}{\sigma^2} \rvx_\sigma + \frac{\sigma^2-\rho^2}{\sigma^2} \rvx, \\
    \text{and}\quad 
    \sigma_Q(s, t) &:= \frac{\rho^2 (\sigma^2-\rho^2)}{\sigma^2}.
\end{align}

Recall that our objective is minimised by minimising the KL divergence between $q(\rvx_\rho|\rvx_\sigma,\rvx)$ and $p_\theta(\rvx_{\rho}|\rvx_{\sigma})$, so intuitively a sensible parameterisation for $p_\theta(\rvx_\rho|\rvx_\sigma)$ would be as close as possible to the form of $q(\rvx_\rho|\rvx_\sigma,\rvx)$. Since $p_\theta(\rvx_{\rho}|\rvx_{\sigma})$ cannot be a function of $\rvx$, we cannot make them exactly equal. We instead define $p_\theta(\rvx_\rho|\rvx_\sigma)$ so that they are close by copying the form of $q(\rvx_\rho|\rvx_\sigma,\rvx)$ but approximating $\rvx$ using our estimate $\hat{\rvx}_\theta(\rvx_\sigma, \rvy, \sigma)$, obtained as described in \cref{eq:equivalent-parameterisations}.\footnote{With non-zero integration step size, this form of $p_\theta(\rvx_{\sigma} | \rvx_{\rho})$ is not exactly the same as that obtained by performing e.g. an Euler integration step on the reverse SDE in \cref{eq:reverse-diffusion-with-nn}, but will become so if we let the integration step size tend to zero\todo{show this in appendix}.}:
\begin{align}
    p_\theta(\rvx_{\rho} | \rvx_{\sigma}) &= \gN( \rvx_{\rho}; \mathbf{\mu}_Q(\rvx_\sigma, \hat{\rvx}_\theta(\rvx_\sigma, \rvy, \sigma); \rho, \sigma), \sigma_Q^2(\rho, \sigma) \mI ).
\end{align}
We can then express the KL divergence as:
\begin{align}
    &\kl{q(\rvx_{\rho} | \rvx_{\sigma}, \rvx)}{p_\theta(\rvx_{\rho} | \rvx_{\sigma})} \\ 
    = &\frac{1}{2 \sigma_Q^2(\rho, \sigma)} || \mathbf{\mu}_Q(\rvx_\sigma, \rvx; \rho, \sigma) - \mathbf{\mu}_Q(\rvx_\sigma, \hat{\rvx}_\theta(\rvx_\sigma, \rvy, \sigma); \rho, \sigma) ||_2^2 \\
    = &\frac{1}{2 \sigma_Q^2(\rho, \sigma)} || \frac{\sigma^2-\rho^2}{\sigma^2} ( \rvx - \hat{\rvx}_\theta(\rvx_\sigma, \rvy, \sigma) ) ||_2^2 \\
    = &\frac{\sigma^2-\rho^2}{2 \sigma^2 \rho^2} || \rvx - \hat{\rvx}_\theta(\rvx_\sigma, \rvy, \sigma) ||_2^2 \\
    = &\frac{\sigma^2 - \rho^2}{2 \rho^2 \sigma^2} || \rvx - \hat{\rvx}_\theta(\rvx_\sigma, \rvy, \sigma) ||_2^2 \\
    = &\frac{1}{2} \left( \frac{1}{\rho^2} - \frac{1}{\sigma^2} \right) || \rvx - \hat{\rvx}_\theta(\rvx_\sigma, \rvy, \sigma) ||_2^2.
\end{align}
Following \citet{kingma2021variational}, we will proceed by introducing the signal-to-noise ratio defined as $\text{SNR}(t) = \frac{1}{\sigma_t^2}$. We can then write the KL divergence as
\begin{align}
\kl{q(\rvx_{s} | \rvx_{t}, \rvx)}{p_\theta(\rvx_{s} | \rvx_{t})} = &\frac{1}{2} \left( SNR(s) - SNR(t) \right) || \rvx - \hat{\rvx}_\theta(\rvx_t, \rvy, \sigma) ||_2^2,
\end{align}
and so
\begin{align}
    \mathcal{L}_\infty(\rvx) = \frac{1}{2} \sum_{i=1}^N \left( SNR(s) - SNR(t) \right) || \rvx - \hat{\rvx}_\theta(\rvx_t, \rvy, \sigma) ||_2^2.
\end{align}
This reveals that the contribution of each denoising step to the log-likelihood is closely related its change to the signal-to-noise ratio. The squared-error term reveals that it is also strongly dependent on the accuracy of the prediction of $\rvx$ given $\rvx_t$ and $\rvy$. A better lower-bound will be obtained by taking more steps, and changing the signal-to-noise ratio by less in each step. To make this lower-bound agnostic to the number of steps, we can consider the limit where infinitely many steps are taken. To do so, we  follow \citet{kingma2021variational} and replace $s$ with $t-\frac{t_\text{diff}}{T}$, where $t_\text{diff} = \frac{t_\text{max}-t_\text{min}}{\tau}$. Then, taking the limit as $\tau \rightarrow 0$ gives 
\begin{align}
    \mathcal{L}_T(\rvx) &= \lim_{T\rightarrow\infty} \frac{1}{2} \sum_{i=1}^T \left[ \left( \frac{SNR(t-\frac{t_\text{diff}}{T}) - SNR(t)}{\tau} \right) || \rvx - \hat{\rvx}_\theta(\rvx_t, \rvy, \sigma) ||_2^2 \right] \\
    &= \frac{1}{2} \int SNR'(t) || \rvx - \hat{\rvx}_\theta(\rvx_t, \rvy, \sigma) ||_2^2 \mathrm{d}t \label{eq:difusion-elbo} \\
    &= \frac{1}{2} \int || \rvx - \hat{\rvx}_\theta(\rvx_\sigma, \rvy, \sigma) ||_2^2 \mathrm{d}\text{SNR}.
\end{align}
We can obtain an unbiased estimate of the integral in \cref{eq:difusion-elbo} using a single Monte Carlo sample of $\sigma$. Optionally, we can also define a proposal distribution $q(\sigma)$ and use importance sampling.


% So we are able to express the lower-bound with a mean-squared error loss between $$ and the learned prediction of $$. We can equivalently write this as a mean-squared error loss between the predicted score function and our target for it. Given that the target is $\rvs := \frac{ - \rvx_t}{\sigma^2}$, we find
% \begin{align}
%     \kl{q(\rvx_{s} | \rvx_{t}, )}{p_\theta(\rvx_{s} | \rvx_{t})} 
%     &= \frac{\hat{\mu}_{0:i-1}^2}{2 \hat{\sigma}_{0,i:i-1}^2} ||  - \hat{\rvx}_\theta(\rvx_t, \rvy, \sigma_{t(i)}) ||_2^2 \\
%     &= \frac{\hat{\mu}_{0:i-1}^2 \sigma^4}{2 \hat{\sigma}_{0,i:i-1}^2} || \frac{}{\sigma_{t(i)}^2} - \frac{\hat{\rvx}_\theta(\rvx_t, \rvy, \sigma_{t(i)})}{\sigma_{t(i)}^2} ||_2^2 \\
%     &= \frac{\hat{\mu}_{0:i-1}^2 \sigma_{t(i)}^4}{2 \hat{\sigma}_{0,i:i-1}^2} || \frac{-\rvx_t}{\sigma_{t(i)}^2} - \rvs_\theta(\rvx_t, \rvy, \sigma_{t(i)}) ||_2^2
% \end{align}
% Recall that our lower-bound $\mathcal{L}_T()$ is a sum over these KL divergences:
% \begin{align}
%     \mathcal{L}_T() &= \sum_{i=2}^T \EX_{q(\rvx_{t(i)}|)} \left[ \kl{q(\rvx_{s(i)} | \rvx_{t(i)}, )}{p_\theta(\rvx_{s(i)} | \rvx_{t(i)})} \right] \\
%     &= \sum_{i=2}^T \frac{\hat{\mu}_{0:i-1}^2 \sigma_{t(i)}^4}{2 \hat{\sigma}_{0,i:i-1}^2} \EX_{q(\rvx_{t(i)}|)} \left[  || \frac{-\rvx_t}{\sigma_{t(i)}^2} - \rvs_\theta(\rvx_t, \rvy, \sigma_{t(i)}) ||_2^2 \right] \\
%     &= \sum_{i=2}^T \frac{\sigma_{t(i)}^4}{2 \hat{\sigma}_{0:i-1}^2} \EX_{q(\rvx_{t(i)}|)} \left[  || \frac{-\rvx_t}{\sigma_{t(i)}^2} - \rvs_\theta(\rvx_t, \rvy, \sigma_{t(i)}) ||_2^2 \right] \\
%     &= \sum_{i=2}^T \frac{1}{2} \sigma_{t(i)}^2 \frac{\sigma_{t(i)}^2}{\sigma_{s(i)}^2} \EX_{q(\rvx_{t(i)}|)} \left[  || \frac{-\rvx_t}{\sigma_{t(i)}^2} - \rvs_\theta(\rvx_t, \rvy, \sigma_{t(i)}) ||_2^2 \right] \\
%     &= \sum_{i=2}^T \frac{1}{2} \sigma_{t(i)}^2 \frac{\sigma_{t(i)}^2}{\sigma_{s(i)}^2} \EX_{q(\rvx_{t(i)}|)} \left[  || \frac{-\rvx_t}{\sigma_{t(i)}^2} - \rvs_\theta(\rvx_t, \rvy, \sigma_{t(i)}) ||_2^2 \right] \\
%     &\approx \sum_{i=2}^T \frac{1}{2} \sigma_{t(i)}^2 (1 + \frac{\frac{\mathrm{d}\sigma_t^2}{\mathrm{d}t} \mathrm{d}t}{\sigma_{s(i)}^2}) \EX_{q(\rvx_{t(i)}|)} \left[  || \frac{-\rvx_t}{\sigma_{t(i)}^2} - \rvs_\theta(\rvx_t, \rvy, \sigma_{t(i)}) ||_2^2 \right] \\
% \end{align}
% In the limit where the number of sampling steps $N$ tends to infinity, it becomes an integral. Given that the internal term depends only on $\sigma_t$ and not on the value of $t$ itself otherwise, ...
% \begin{align}
%     \mathcal{L}_T(\rvx_0) &= 
% \end{align}

% , we will now determine the form of $p_\theta(\rvx_{s(i)}|\rvx_{t(i)})$ so that we can consider the KL divergence between them. Euler integration of the reverse SDE involves taking a deterministic step and then adding Gaussian noise, so $p_\theta(\rvx_{s(i)}|\rvx_{t(i)})$ will be Gaussian. In particular, referring to the reverse SDE in \cref{eq:reverse-diffusion}, the step will be in direction $g(t)^2 s_\theta(\rvx_{t(i)}, \rvy, \sigma_{t(i)})$. Since the step is from $t(i)$ to $s(i)$, the step will produce mean value $\rvx_i - (t(i)-s(i)) g(t)^2 s_\theta(\rvx_{t(i)}, \rvy, \sigma_{t(i)})$.
% %
% % Referring to \cref{eq:reverse-diffusion} again, the variance will be $\int_{s(i)}^{t(i)} g(t(i))^2 \mathrm{d}t = \sigma_{i-1:i}$.
% Since we are interested in minimising the KL divergence to $q(\rvx_{s(i)} | \rvx_{t(i)}, \rvx_0)$, we can simply copy its variance $\hat{\sigma}_{0,i:i-1}^2$. We therefore write $p_\theta(\rvx_{s(i)}|\rvx_{t(i)})$ as
% \begin{equation}
%     p_\theta(\rvx_{s(i)}|\rvx_{t(i)}) = \gN \Big( \rvx_{s(i)}; \rvx_i-(t(i)-s(i)) g(t(i))^2 s_\theta(\rvx_{t(i)}, \rvy, \sigma_{t(i)}); \hat{\sigma}_{0,i:i-1}^2 \mI \Big).
% \end{equation}
% This Euler step clearly will have some integration error when $T$ is finite and therefore $t(i)$ and $s(i)$ are a finite distance apart. In this section we are concerned with ensuring that $\theta$ is good enough to give us a high likelihood with a perfect integrator, and not with minimizing integration errors. We therefore look at the limit of $\mathcal{L}_T(\rvx_0)$ as $T \rightarrow \infty$:
% \begin{align}
%     \mathcal{L}_\infty(\rvx_0) &= \lim_{T\rightarrow\infty} \sum_{i=2}^T \EX_{q(\rvx_{t(i)}|\rvx_0)} \left[ \kl{q(\rvx_{s(i)} | \rvx_{t(i)}, \rvx_0)}{p_\theta(\rvx_{s(i)} | \rvx_{t(i)})} \right] \\
%     &= \int_0^{t_\text{max}} \EX_{q(\rvx_t|\rvx_0)} \left[ \frac{1}{2\sigma_q(t)^2} || \mu_q(\rvx_t, t) - \mu_\theta(\rvx_t, t) ||_2^2 \right] \mathrm{d}t
% \end{align}

% \section{Parameterising the prediction of the score function}
% We now describe a relationship between predicting the score function and predicting various other quantities. Given $\rvx_\sigma$ and an estimate $s_\theta(\rvx_\sigma, \rvy, \sigma) \approx \frac{\rvx_0-\rvx_\sigma}{\sigma^2}$, it is simple to obtain mean-squared error estimates of various other quantities: 
% \begin{itemize}
%     \item An estimate of the clean data $\rvx_0$ can be obtained as ${\hat{\rvx}_\theta(\rvx_\sigma, \rvy, \sigma) :=  \sigma^2 \cdot s_\theta(\rvx_\sigma, \rvy, \sigma) + \rvx_\sigma}$
%     \item The noise added to $\rvx_0$ to create $\rvx_\sigma$, scaled to unit variance, is often denoted $\epsilon := \frac{\rvx_\sigma-\rvx_0}{\sigma}$. This can be estimated as $\hat{\epsilon}_\theta := - \sigma \cdot s_\theta(\rvx_\sigma, \rvy, \sigma)$.
% \end{itemize}

% Similarly, it is possible to map from a prediction of any of these quantities to the estimate of the score function needed for sampling following the reverse SDE and ODE in \cref{eq:reverse-diffusion-with-nn,eq:diffusion-ode-with-nn}. Various papers have proposed parameterising neural networks to predict these other quantities~\cite{ho2020denoising,sohl2015deep} and then implicitly obtaining the score from them; in particular \citet{ho2020denoising} train a neural network to predict $\epsilon$ and we use this approach in \cref{ch:fdm}. More recently, \citet{karras2022elucidating} demonstrated that the choice of which of these to predict is unimportant as long as the network parameterisation and per-timestep weighting of the loss function are explicitly controlled for. Their framework suggests that we: name the neural network output $\rvx_out$; compute $\hat{\rvx}_\theta(\cdots) := c_\text{skip}(\sigma) \cdot \rvx_\sigma + c_\text{out} \cdot \rvx_out$; compute the mean-squared error loss between $\hat{\rvx}$.

% The loss can be equivalently computed in different spaces, for example a a squared error loss to $\rvx_0$ with
% \begin{align}
%     \mathcal{L}_\text{ISM}(\theta, \sigma) &= \frac{1}{\sigma^4} \EX_{q(\rvx_0, \rvx_\sigma, \rvy)} \left[ 
%     || \hat{\rvx}_0(\rvx_\sigma, \rvy, \sigma) - \rvx_0 ||_2^2 \right]
% \end{align}
% where $\hat{\rvx}_0(\rvx_\sigma, \rvy, \sigma) := \sigma^2 \cdot \rvs_\theta(\rvx_\sigma, \rvy, \sigma) + \rvx_\sigma$. 
% It is often also written as a squared error loss to $\epsilon := \frac{\rvx_\sigma - \rvx_0}{\sigma}$, a scaled version of the noise added when sampling $\rvx_\sigma \sim q(\cdot|\rvx_0)$, with
% \begin{align}
%     \mathcal{L}_\text{ISM}(\theta, \sigma) &= \frac{1}{\sigma^2} \EX_{q(\rvx_0, \rvx_\sigma, \rvy)} \left[ 
%     || \hat{\mathbf{\epsilon}}_\theta(\rvx_\sigma, \rvy, \sigma) - \epsilon ||_2^2 \right]
% \end{align}
% where $\hat{\mathbf{\epsilon}}_\theta(\rvx_\sigma, \rvy, \sigma) := \sigma \cdot \rvs_\theta(\rvx_\sigma, \rvy, \sigma)$.

% For sampling to work well, the loss above must be low across a range of noise levels. We can therefore write the full loss with an integral over $\theta$:
% \begin{align} \label{eq:diffusion-loss-all-sigma}
%     \mathcal{L}_\text{ISM}(\theta) &= \int_{\sigma_\text{min}}^{\sigma_\text{max}} \lambda(\sigma) \frac{1}{\sigma^2} \EX_{q(\rvx_0, \rvx_\sigma, \rvy)} \left[ 
%     || \hat{\mathbf{\epsilon}}_\theta(\rvx_\sigma, \rvy, \sigma) - \epsilon ||_2^2 \right] \mathrm{d}\sigma
% \end{align}
% where $\lambda(\sigma)$ is a weighting function that controls how much network capacity is spent modelling the score function at each noise level. We show in \cref{sec:diffusion-likelihood} that, if $\lambda(\sigma)$ is set appropriately, this loss yields a lower-bound on the data likelihood. Before doing so, however, we show how to use the learned artefact $\hat{\mathbf{\epsilon}}_\theta$ to define and sample from a generative model.

\subsection{Weighting the loss for perceptual quality}

\subsection{Parameterizing the neural network} \label{sec:diffusion-architecture}

