\chapter{Review: From Variational Auto-Encoders to Diffusion Models}
\label{sec:diffusion}

Diffusion models, or DMs~\citep{sohl2015deep,ho2020denoising,nichol2021improved,song2020score}, can be understood as hierarchical variational auto-encoders with a very simple encoder: rather than parameterizing a distribution with a neural network, the encoder simply adds Gaussian noise to the data. This ``encoder'' is commonly then referred to as the ``forward'' (or ``noising'') process. The equivalent of the VAE's prior and decoder are then the ``reverse'' (or ``generative'') process, which uses a learned ``denoising'' function in the form of a neural network. Sampling from a DM begins with drawing a sample from a Gaussian distribution that approximates a heavily noised sample from the data distribution. Running the reverse/generative process on this sample then gradually removes noise from it, moving the sample closer to the data manifold. By the end of the generative process, if all is trained well, the samples from this process should match the data distribution. As we will show later, fitting the DM simply involves training a neural network to predict clean data give noisy data using a mean-squared error loss. This leads to more stable training than for a VAE, in which the prior, decoder, and encoder are all learned jointly, leading to a ``moving target'' problem as each component must keep compensating for changes to the others.

We specify ``how much'' noise is added to the data at any point with a timestep $t$ which is defined so that, when $t=0$, our data has no added noise, and more noise is added as $t$ increases. We use $\rvx_t$ to denote a copy of data with noise added according to $t$, so $\rvx_0 := \rvx$ is clean data. Making a comparison to the hierarchical VAE framework, for $t>0$, $\rvx_t$ can be understood as a form of latent variable which contains some information from the data. We no longer use the $\rvz$ or $\rvz_l$ notation since, first, these latent variables now live in the same space as $\rvx$ and, second, this new notation allows us to consider a continuum of latent variables corresponding to any positive real value of $t$.

% \todo[inline]{don't need to discuss this until later}
% The noising and generative processes can be understood in either discrete-time or continuous time. That is, we can imagine noise being added to the data either in a discrete series of steps or as a continuous process. In discrete time the analogy to hierarchical VAEs is precise, but it is simpler to first describe continuous-time diffusion models. In practice, there is little practical distinction between discrete and continuous time diffusion models since we must use a discretisation when sampling from a continuous time diffusion model. 

We define how much noise is present at each timestep through a monotonically increasing schedule $\sigma(t)$ which maps from the timestep to the ``noise-to-signal ratio'' of the data. Formally, given clean data $\rvx_0$, a timestep $t$, and $\sigma := \sigma(t)$, the noisy data is distributed according to
\begin{equation}
    q_\sigma(\rvx_t|\rvx_0) = \gN ( \rvx_t ; \rvx_0, \sigma^2 \mI ).
\end{equation}
Note the notational changes from a VAE's encoder: we no longer parameterise by $\phi$ as there are no longer any encoder parameters; we now explicitly subscript by $\sigma$ which may take on any of a continuous range of values; and our latent variables are now denoted $\rvx_t$. To define a conditional diffusion model, we must also recall the relationship to conditioning information $\rvy$. In conjunction with the data distribution $\pdata(\rvx_0, \rvy)$ we therefore define the marginal distribution
\begin{equation}
    q_\sigma(\rvx_t,\rvy) = \int q_\sigma(\rvx_t|\rvx_0) \pdata(\rvx_0, \rvy) \mathrm{d} \rvx_0
\end{equation}
and its conditional
\begin{equation}
    q_\sigma(\rvx_t|\rvy) = \int q_\sigma(\rvx_t|\rvx_0) \pdata(\rvx_0|\rvy) \mathrm{d} \rvx_0.
\end{equation}

% \todo[inline]{replace $\rvx_\sigma$ here with $\rvz$? and then just redefine later after we introduce the notion of $\rvx_t$}
% To formally introduce the DM framework, we begin by introducing the ``non-learnable encoder'' and associated notation. Recalling that the encoder for a VAE was written $q_\phi(\rvz|\rvx)$, we compare this to a diffusion encoder $q_\sigma(\rvx_\sigma|\rvx)$. There are a few differences. First, we now denote the latent variables $\rvx_\sigma$ instead of $\rvz$; this helps to make clear that the latent variables now live in the same space as the data $\rvx$ and the subscript of $\sigma$ denotes how much noise has been added. In this sense $\sigma$ is comparable to the layer index within a hierarchical VAE. From now on we will also use $\rvx_0 := \rvx$ to denote the data itself with no noise added. Second, we no longer have $\phi$ since our encoder has no learnable parameters. We write $q_\sigma$ to make explicit what noise level we are considering. Given this notation, the encoder simply describes a Gaussian conditional distribution
% \begin{equation}
%     q_\sigma(\rvx_\sigma|\rvx_0) = \gN ( \rvx_\sigma ; \rvx_0, \sigma^2 \mI ).
% \end{equation}
% In conjunction with the data distribution $\pdata(\rvx_0, \rvy)$ this defines the marginal distribution
% \begin{equation}
%     q_\sigma(\rvx_\sigma,\rvy) = \int q_\sigma(\rvx_\sigma|\rvx_0) \pdata(\rvx_0, \rvy) \mathrm{d} \rvx_0
% \end{equation}
% and its conditional
% \begin{equation}
%     q_\sigma(\rvx_\sigma|\rvy) = \int q_\sigma(\rvx_\sigma|\rvx_0) \pdata(\rvx_0|\rvy) \mathrm{d} \rvx_0
% \end{equation}
% which we will build on.

Our training procedure can be understood as characterizing these conditional distributions via a neural network that approximates the score function $\nabla_{\rvx_t} q_\sigma(\rvx_t)$. Our sampling procedure will use the learned approximate score function to morph samples, over a series of $N$ steps, through approximations of $q_{\sigma_0}(\cdot | \rvy)$, $q_{\sigma_1}(\cdot | \rvy)$ and so on until reaching the conditional data distribution $q_{\sigma_N}(\cdot | \rvy) = q_{0}(\cdot | \rvy)$. 

\subsection{Training diffusion models}

\subsection{Sampling from diffusion models}

We can now more formally describe our sampling procedure, following the notation of \citet{karras2022elucidating}. We take as hyperparameters a number of steps $N$ and a series of noise levels $\sigma_0 > \sigma_1 > \cdots > \sigma_N = 0$. If $\sigma_0$ is sufficiently large then, for any $\rvy$, $q_{\sigma_N}(\rvx_{\sigma_N}|\rvy) $ is well approximated by $\gN(\rvx_{\sigma_N} ; \vzero, {\sigma_N}^2\mI)$ and so we start by sampling $\rvx_{\sigma_N} \sim \gN(\vzero, {\sigma_N}^2\mI)$. We then transform $\rvx_{\sigma_N}$ into each $\rvx_i$



\chapter{Old Diffusion Model Stuff}

Diffusion models, or DMs~\citep{sohl2015deep,ho2020denoising,nichol2021improved,song2020score}, are hierarchical variational auto-encoders in which the encoder has a particular structure. Specifically, the encoder takes the form of a Gaussian diffusion process which repeatedly adds noise to the data until any signal in the data is lost. This diffusion process can be described in discrete or continuous time; we will use both but begin with discrete time where the link is most clear.

\section{Discrete-time diffusion}
\subsection{Diffusion process: A Non-learnable Encoder}
We denote the discrete timesteps $0,\ldots,T$, defining them such that $\rvx_0=\rvx$ is data without noise, the first group of latent variables $\rvz_1$ is data with a very small amount of noise added, and so on until the final group of latent variables, $\rvz_T$, is data with so much noise added that it indistinguishable from, or close to indistinguishable from, a sample from a Gaussian distribution. Given that $\rvz_t$ must now live in the same space as the data $\rvx$, we will from now on simplify notation by defining $\rvx_t := \rvz_t$ whenever $t > 0$ and writing $\rvx_t$ instead of $\rvz_t$.

Similarly to our use of $q_\phi(\rvz, \rvx,\rvy)=\pdata(\rvx,\rvy)q_\phi(\rvz_{1:L}|\rvx)$ in \cref{eq:r} to denote the combination of the data distribution and a VAE encoder's distribution, we will write the corresponding distribution for a DM as $q(\rvx_{0:T},\rvy) = \pdata(\rvx_0,\rvy) q(\rvx_{1:T}|\rvx)$. Recall that $\rvx_0 := \rvx$, and note that we have dropped the $\phi$ subscript since the encoder no longer has learnable parameters and now denote the ``depth'' of the encoder with $T$ rather than $L$. This distribution factorises as 
\begin{equation} \label{eq:q-joint}
    q(\rvx_{0:T}, \rvy) = q(\rvx_0, \rvy) \prod_{t=1}^T q(\rvx_t|\rvx_{t-1}).
\end{equation}
where, for each $t \in \{1, \ldots, T\}$, the corresponding ``transition'' distribution is a Gaussian of the form:
\begin{equation} \label{eq:q-step-general}
    q(\rvx_t | \rvx_{t-1}) = \gN(\rvx_t ; c_t  \rvx_{t-1} , \beta_t \rmI ).
\end{equation}
Here, $\beta_t$ and $c_t$ are scalar hyperparameters defined for $t \in \{1, \ldots, T\}$. The scalar $\beta_t$ is the variance of the noise to add at this step. It is typically set to be small, so that only a small amount of noise is added in any given step, but still large enough that, after $T$ steps (with $T$ often set to $1000$), there is much more noise than signal so that $q(\rvx_T)$ is approximately Gaussian. Meanwhile, $c_t$ can be interpreted as the factor by which the ``signal'' is multiplied. There are two common choices for $c_t$. The simplest is to set it to $1$; this leads to what is known as a ``variance-exploding'' diffusion process~\citep{song2020score} in which the variance of the marginal $q(\rvx_t)$ increases as $t$ increases. Alternatively, setting $c_t := \sqrt{1-\beta_t}$ leads to a ``variance-preserving'' diffusion process~\citep{song2020score} because it ensures that, if the marginal $q(\rvx_{t-1})$ has unit variance, $q(\rvx_t)$ will also have unit variance.

For now we will focus on variance-preserving processes, but we will return to variance-exploding processes in the continuous-time setting. We will follow the notation of \citet{nichol2021improved}: the variance of the noise added in each step is $\beta_t$ as in \cref{eq:q-step-general}; they define $\alpha_t = 1-\beta_t$; they use a variance-preserving process so the scaling factor which we called $c_t$ becomes $\sqrt{1-\beta_t}$ or equivalently $\sqrt{\alpha_t}$. The Gaussian transition distribution in this case is therefore
\begin{equation} \label{eq:q-step-iddpm}
    q(\rvx_t | \rvx_{t-1}) = \gN(\rvx_t ; \sqrt{\alpha_t}  \rvx_{t-1} , \beta_t \rmI ).
\end{equation}

Under this definition of $q(\rvx_{0:T}, \rvy)$ all conditional distributions given $\rvx_0$ are Gaussian as well. In particular, it will be helpful during training that, for any $t$, the following is Gaussian: 
\begin{equation} \label{eq:q-step-iddpm}
    q(\rvx_t | \rvx_0) = \gN(\rvx_t ; \sqrt{\bar{\alpha}_t}  \rvx_{t-1} , \bar{\beta}_t \rmI )
\end{equation}
where $\bar{\alpha}_t := \prod_{i=1}^t \alpha_i$ and $\bar{\beta}_t = 1 - \bar{\alpha}_t$.

\subsection{Learnable Prior and Decoder}
Given this simple and non-learnable encoder, the complexity of a diffusion model lies in ``inverting it'' by mapping from noise $\rvx_T$ to data $\rvx_0$. We do so by first sampling $\rvx_{T-1}$ given $\rvx_T$, then $\rvx_{T-2}$ given $\rvx_{T-1}$ and so on until we have sampled $\rvx_0$. Formally, we factorize the ``generative'' distribution as
\begin{equation}
    p_\theta(\rvx_{0:T}) = p(\rvx_T) \prod_{t=1}^T p_\theta(\rvx_{t-1}|\rvx_t).
\end{equation}
In this factorisation, $p(\rvx_T)$ has no dependence on $\theta$ because it is typically set to be a zero-mean Gaussian~\citep{ho2020denoising}. All of the parameters $\theta$ are therefore dedicated to parameterising the transition distributions $p_\theta(\rvx_{i-t}|\rvx_t)$ for each $t$. Note that, even though we get from $\rvx_{i-t}$ to $\rvx_t$ by just adding Gaussian noise, knowing how to ``remove'' this Gaussian noise is complex and requires knowledge of the data distribution.

% \citet{ho2020denoising} proposed to parameterise $p_\theta(\rvx_{t-1}|\rvx_t)$ via a deterministic prediction of $\rvx_0$ from $\rvx_t$ and the known Gaussian distribution $q(\rvx_{t-1}|\rvx_t, \rvx_0)$. Noting that we can use \cref{eq:q-joint} to derive
% \begin{equation}
%     q(\rvx_{t-1}|\rvx_t, \rvx_0) = \gN ( \rvx_{t-1} ; \tilde{\mathbf{\mu}}_t(\rvx_t,\rvx_0), \tilde{\beta}_t \mI )
% \end{equation}
% with $\tilde{\mathbf{\mu}}_t(\rvx_t,\rvx_0) := \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}\rvx_0 + \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}\rvx_t$ and $\tilde{\beta}_t = \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t} \beta_t$, we parameterise $p_\theta(\rvx_{t-1}|\rvx_t)$ as
% \begin{equation}
%     p_\theta(\rvx_{t-1}|\rvx_t) = \gN ( \rvx_{t-1} ; \tilde{\mathbf{\mu}}_t(\rvx_t,\tilde{\rvx}_\theta(\rvx_t, t)), \tilde{\beta}_t \mI )
% \end{equation}
% where $\tilde{\rvx}_\theta(\rvx_t, t)$ is a neural network's prediction of $\rvx_0$ given $\rvx_t$. 

\citet{ho2020denoising} proposed to parameterise $p_\theta(\rvx_{t-1}|\rvx_t)$ as a Gaussian with mean $\tilde{\mathbf{\mu}}_\theta(\rvx_t, t)$ output by a neural network and variance $\beta_t$ matching that of $q(\rvx_t|\rvx_{t-1})$.

The parameters $\theta$ can be trained simply by optimising the ELBO averaged over training data $\rvx \sim \pdata(rvx)$:
\begin{align}
    \gL(\theta, \phi, \rvx_0, \rvy) =& \EX_{q(\rvx_{1:T}|\rvx_0)} \left[ \log \frac{p_\theta(\rvx_{0:T}|\rvy)}{q(\rvx_{1:T}|\rvx_0)} \right].
\end{align}
To derive a simple estimator for this loss we follow \citet{ho2020denoising} by breaking it down as:
\begin{align}
    \gL(\theta, \phi, \rvx_0, \rvy) =& \EX_{q(\rvx_{1:T}|\rvx_0)} \left[ \log \frac{p(\rvx_T) \prod_{t=1}^T p_\theta(\rvx_{t-1}|\rvx_t, \rvy)}{\prod_{t=1}^T q(\rvx_t|\rvx_{t-1})} \right] \\
    =& \EX_{q(\rvx_{1:T}|\rvx_0)} \left[ \log \frac{p(\rvx_T, \rvy) \prod_{t=1}^T p_\theta(\rvx_{t-1}|\rvx_t, \rvy)}{q(\rvx_T|\rvx_0) \prod_{t=2}^T q(\rvx_{t-1}|\rvx_t,\rvx_0)} \right] \\
    =& \EX_{q(\rvx_{1:T}|\rvx_0)} \left[ \log \frac{p(\rvx_T)}{q(\rvx_T|\rvx_0)} + \prod_{t=2}^T \frac{p_\theta(\rvx_{t-1}|\rvx_t, \rvy)}{q(\rvx_{t-1}|\rvx_t,\rvx_0)} + \log p_\theta(\rvx_0|\rvx_1, \rvy) \right] \\
    =& \EX_{q(\rvx_{1:T}|\rvx_0)} \Big[ -\kl{q(\rvx_T|\rvx_0)}{p(\rvx_T)} \nonumber \\
    &\qquad\qquad \left. - \prod_{t=2}^T \kl{q(\rvx_{t-1}|\rvx_t,\rvx_0)}{p_\theta(\rvx_{t-1}|\rvx_t, \rvy)} \right. \nonumber \\
    &\qquad\qquad + \log p_\theta(\rvx_0|\rvx_1) \Big].
\end{align}
When learning $\theta$, the first term, the KL divergence between $q(\rvx_T|\rvx_0)$ and $p_\theta(\rvx_T)$ can be disregarded since it is not dependent on $\theta$. 
The second term can be simplified by noting that, due to the properties of the forward process, $q(\rvx_{t-1}|\rvx_t,\rvx_0)$ is the analytically tractable Gaussian
\begin{equation}
    q(\rvx_{t-1}|\rvx_t, \rvx_0) = \gN ( \rvx_{t-1} ; \tilde{\mathbf{\mu}}_t(\rvx_t,\rvx_0), \tilde{\beta}_t \mI )
\end{equation}
with mean $\tilde{\mathbf{\mu}}_t(\rvx_t,\rvx_0) := \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}\rvx_0 + \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}\rvx_t$ and variance $\tilde{\beta}_t = \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t} \beta_t$. The KL divergence between two Gaussians has an analytic form which we can then exploit. Recalling that $p_\theta(\rvx_{t-1}|\rvx_t, \rvy)$ has learned mean $\tilde{\mathbf{\mu}}_\theta(\rvx_t, t)$ and variance $\beta_t$, and using $d$ to denote the dimensionality of the data, we can write each KL divergence as
\begin{align}
    &\kl{q(\rvx_{t-1}|\rvx_t,\rvx_0)}{p_\theta(\rvx_{t-1}|\rvx_t, \rvy)} \\
    =& \frac{1}{2 \beta_t} ||\tilde{\mathbf{\mu}}_t(\rvx_t,\rvx_0) - \tilde{\mathbf{\mu}}_\theta(\rvx_t, t)||_2^2 + \frac{d}{2} \left( \frac{\tilde{\beta}_t}{\beta_t} + \log\frac{\beta_t}{\tilde{\beta}_t} - 1 \right) \\
    =& \frac{1}{2 \beta_t} ||\tilde{\mathbf{\mu}}_t(\rvx_t,\rvx_0) - \tilde{\mathbf{\mu}}_\theta(\rvx_t, t)||_2^2 + \text{constant}.
\end{align}
where ``constant'' groups together terms with no dependence on $\theta$. The ``likelihood'' term, $\log p_\theta(\rvx_0|\rvx_1)$, is the log-probability of a Gaussian:
\begin{align}
    \log p_\theta(\rvx_0|\rvx_1) &= -\frac{1}{2\beta_1} || \rvx_0 - \tilde{\mathbf{\mu}}_\theta(\rvx_1, 1) ||_2^2 - \frac{d}{2}\left( \log(2\pi) + \log \beta_t \right) \\
    &= -\frac{1}{2\beta_1} || \rvx_0 - \tilde{\mathbf{\mu}}_\theta(\rvx_1, 1) ||_2^2  + \text{constant}.
\end{align}
We can therefore simplify the loss as a whole to:
\begin{align}
    \gL(\theta, \phi, \rvx_0, \rvy) =& \EX_{q(\rvx_{1:T}|\rvx_0)} \left[ \sum_{t=1}^T \frac{1}{2\beta_t} || \tilde{\mathbf{\mu}}_t(\rvx_t,\rvx_0) - \tilde{\mathbf{\mu}}_\theta(\rvx_t, t) ||_2^2 \right] + \text{constant}.
\end{align}
noting that $\tilde{\mathbf{\mu}}_1(\rvx_1,\rvx_0)$ is equal to $\rvx_0$.
This is a simple sum of squared-error losses. We can further rewrite it as an expectation which we can take an unbiased Monte-Carlo estimate of at training time with a single evaluation of the neural network $\mathbf{\mu}_\theta$:
\begin{align} \label{eq:diffusion-mse-loss-mu}
    \gL(\theta, \phi, \rvx_0, \rvy) =& \EX_{q(\rvx_t|\rvx_0,t)u(t)} \left[ \frac{T}{2 \beta_t u(t)} || \tilde{\mathbf{\mu}}_t(\rvx_t,\rvx_0) - \tilde{\mathbf{\mu}}_\theta(\rvx_t, t) ||_2^2 \right] + \text{constant}.
\end{align}
Here, $u(t)$ is a proposal distribution over timesteps, often set to be uniform. Recall that $q(\rvx_t|\rvx_0,t)$ is analytic and Gaussian so sampling $\rvx_t$ on each step for a single $t$ is feasible.

Training a neural network to directly map from $\rvx_t$ and $t$ to $\tilde{\mathbf{\mu}}_t(\rvx_t,\rvx_0)$ has been found to work poorly in the literature~\cite{ho2020denoising}. One reason for this is that $\tilde{\mathbf{\mu}}_t(\rvx_t,\rvx_0)$ is typically very close to $\rvx_t$ and perturbing it only slightly, while keeping $\rvx_t$ fixed, leads to a large fluctuation in the implied value of $\rvx_0$. Neural networks are typically designed to produce outputs with roughly zero mean and unit variance and so may not be able to produce accurate values within this range~\citet{karras2022elucidating}.

Common alternatives involve training the neural network to predict other affine transformations of $\rvx_0$ and $\rvx_t$ such as $\epsilon := f_t^\epsilon(\rvx_t, \rvx_0) = \frac{1}{\sqrt{1-\bar{\alpha}_t}} \rvx_t - \frac{\sqrt{\bar{\alpha}_t}}{\sqrt{1-\bar{\alpha}_t}} \rvx_0$~\citep{ho2020denoising} or the clean data $\rvx_0 = f_t^\rvx(\rvx_t, \rvx_0) := \rvx_0$ itself. We can derive a loss equivalent to \cref{eq:diffusion-mse-loss-mu} with the mean-squared error loss taken between outputs of any of these transformations. To do so generally, we describe each transformation as $f_t(\rvx_t, \rvx_0) := a_t^ \cdot \rvx_t + b_t \cdot \rvx_0$ and, recalling the definition $\tilde{\mathbf{\mu}}_t(\rvx_t,\rvx_0) := \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}\rvx_t + \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}\rvx_0$, define $a_t^\mu := \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}$ and $b_t^\mu := \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}$. Now we ask, given access to $\rvx_t$ and a prediction of the value of some general $f_t(\rvx_t, \rvx_0) := a \cdot \rvx_t + b \cdot \rvx_0$, how do we infer the implied value of $\tilde{\mathbf{\mu}}_t(\rvx_t,\rvx_0)$ required in \cref{eq:diffusion-mse-loss-mu}?
\begin{align}
    \tilde{\mathbf{\mu}}_t(\rvx_t,\rvx_0) &= a_t^\mu \cdot \rvx_t + b_t^\mu \cdot \rvx_0 \\
    &= a_t^\mu \cdot \rvx_t + b_t^\mu \left( \frac{f_t(\rvx_t, \rvx_0) - a \cdot \rvx_t}{b} \right) \\
    &= \left( a_t^\mu - a_t \frac{b_t^\mu}{b_t} \right) \rvx_t + \frac{b_t^\mu}{b_t} f_t(\rvx_t, \rvx_0).
\end{align}
Substituting this into \cref{eq:diffusion-mse-loss-mu} yields
\begin{align}
    \gL(\theta, \phi, \rvx_0, \rvy) =& \EX_{q(\rvx_t|\rvx_0,t)u(t)} \left[ \frac{T}{2 \beta_t u(t)} || \left( a_t^\mu - a_t \frac{b_t^\mu}{b_t} \right) \rvx_t + \frac{b_t^\mu}{b_t} f_t(\rvx_t, \rvx_0) - \tilde{\mathbf{\mu}}_\theta(\rvx_t, t) ||_2^2 \right] + \text{constant}.
\end{align}
If we reparameterize $\tilde{\mathbf{\mu}}_\theta(\rvx_t, t)$ via a learned prediction $\tilde{\mathbf{f}}_\theta(\rvx_t, t)$, so that $\tilde{\mathbf{\mu}}_\theta(\rvx_t, t) = \left( a_t^\mu - a_t \frac{b_t^\mu}{b_t} \right) \rvx_t + \frac{b_t^\mu}{b_t} \tilde{\mathbf{x}}_\theta(\rvx_t, t)$ then this simplifies to
\begin{align}
    \gL(\theta, \phi, \rvx_0, \rvy) =& \EX_{q(\rvx_t|\rvx_0,t)u(t)} \left[ \frac{T}{2 \beta_t u(t)} || \left( a_t^\mu - a_t \frac{b_t^\mu}{b_t} \right) \rvx_t + \frac{b_t^\mu}{b_t} f_t(\rvx_t, \rvx_0) - \left( a_t^\mu - a_t \frac{b_t^\mu}{b_t} \right) \rvx_t - \frac{b_t^\mu}{b_t} \tilde{\mathbf{f}}_\theta(\rvx_t, t) ||_2^2 \right] + \text{constant}. \\
    &= \EX_{q(\rvx_t|\rvx_0,t)u(t)} \left[ \frac{T}{2 \beta_t u(t)} || \frac{b_t^\mu}{b_t} f_t(\rvx_t, \rvx_0) - \frac{b_t^\mu}{b_t} \tilde{\mathbf{f}}_\theta(\rvx_t, t) ||_2^2 \right] + \text{constant}. \\
    &= \EX_{q(\rvx_t|\rvx_0,t)u(t)} \left[ \frac{T {b_t^\mu}^2 }{2 \beta_t u(t) b_t^2 } || f_t(\rvx_t, \rvx_0) - \tilde{\mathbf{f}}_\theta(\rvx_t, t) ||_2^2 \right] + \text{constant}. \\
    &= \EX_{q(\rvx_t|\rvx_0,t)u(t)} \left[ \frac{T \bar{\alpha}_{t-1} \beta_t }{2 u(t) b_t^2 (1-\bar{\alpha}_t)^2 } || f_t(\rvx_t, \rvx_0) - \tilde{\mathbf{f}}_\theta(\rvx_t, t) ||_2^2 \right] + \text{constant}.
\end{align}
For the case of $\epsilon$ prediction, when we use $f_t^\epsilon(\rvx_t, \rvx_0) = \frac{1}{\sqrt{1-\bar{\alpha}_t}} \rvx_t - \frac{\sqrt{\bar{\alpha}_t}}{\sqrt{1-\bar{\alpha}_t}} \rvx_0$, this yields the loss
\begin{equation}
    \gL(\theta, \phi, \rvx_0, \rvy) = \EX_{q(\rvx_t|\rvx_0,t)u(t)} \left[ \frac{T \bar{\alpha}_{t-1} \beta_t }{2 u(t) (1-\bar{\alpha}_t) } || f_t^\epsilon(\rvx_t, \rvx_0) - \tilde{\mathbf{\epsilon}}_\theta(\rvx_t, t) ||_2^2 \right] + \text{constant}.
\end{equation}


\section{Continuous time diffusion}

\newpage
\newpage

Diffusion models, or DMs~\citep{sohl2015deep,ho2020denoising,nichol2021improved,song2020score}, are another class of generative model. We will describe the conditional extension~\citep{tashiro2021csdi}, in which the modeled $\rvx$ is conditioned on observations $\rvy$. DMs simulate a diffusion process which transforms $\rvx$ to noise, and generate data by learning the probabilistic inverse of the diffusion process. The diffusion process happens over timesteps $0,\ldots,T$ such that $\rvx_0=\rvx$ is data without noise, $\rvx_1$ has a very small amount of noise added, and so on until $\rvx_T$ is almost independent of $\rvx_0$ and approximates a random sample from a unit Gaussian. In the diffusion process we consider, the distribution over $\rvx_t$ depends only on $\rvx_{t-1}$:
\begin{equation} \label{eq:q-step}
    q(\rvx_t | \rvx_{t-1}) = \gN(\rvx_t ; \sqrt{\alpha_t}  \rvx_{t-1} , (1-\alpha_t) \rmI ).
\end{equation}
Hyperparameters $\alpha_1,\ldots,\alpha_T$ are chosen to all be close to but slightly less than $1$ so that the amount of noise added at each step is small.
The combination of this diffusion process and a data distribution $q(\rvx_0,\rvy)$ (recalling that $\rvx_0=\rvx$) defines the joint distribution
\begin{equation} \label{eq:q-joint}
    q(\rvx_{0:T}, \rvy) = q(\rvx_0, \rvy) \prod_{t=1}^T q(\rvx_t|\rvx_{t-1}).
\end{equation}
Recall that $q(\rvx_0, \rvy)$ is simply new notation for the data distribution $\pdata(\rvx,\rvy)$ and the encoder distribution, this equation is analogous to the product of the data distribution and the encoder's distribution
$\prod_{l=1}^L \pmodel(\rvz|\rvz_{<l})$ in a hierarchical VAE from \cref{sec:hierarchical-vae}. This leads to one perspective on diffusion models as simply a hierarchical VAE in which the encoder has a simple and non-learnable structure.

DMs work by ``inverting'' the diffusion process: given values of $\rvx_t$ and $\rvy$ a neural network is used to parameterize $p_\theta(\rvx_{t-1}|\rvx_t, \rvy)$, an approximation of $q(\rvx_{t-1}|\rvx_t,\rvy)$. This neural network lets us draw samples of $\rvx_0$ by first sampling $\rvx_T$ from a unit Gaussian (recall that the diffusion process was chosen so that $q(\rvx_T)$ is well approximated by a unit Gaussian), and then iteratively sampling $\rvx_{t-1} \sim p_\theta(\cdot|\rvx_t,\rvy)$ for $t=T,T-1,\ldots,1$. The joint distribution of sampled $\rvx_{0:T}$ given $\rvy$ is
\begin{equation} \label{eq:p-joint}
    p_\theta(\rvx_{0:T}|\rvy) = p(\rvx_{T}) \prod_{t=1}^T p_\theta(\rvx_{t-1}|\rvx_t, \rvy)
\end{equation}
where $p(\rvx_{T})$ is a unit Gaussian that does not depend on $\theta$. Training the conditional DM therefore involves fitting $p_\theta(\rvx_{t-1}|\rvx_t,\rvy)$ to approximate $q(\rvx_{t-1}|\rvx_t, \rvy)$ for all choices of $t$, $\rvx_t$, and $\rvy$.

Several observations have been made in recent years which simplify the learning of $p_\theta(\rvx_{t-1}|\rvx_t,\rvy)$. \citet{sohl2015deep} showed that when $\alpha_t$ is close to 1, $p_\theta(\rvx_{t-1}|\rvx_t)$ is approximately Gaussian~\cite{sohl2015deep}. Furthermore, \citet{ho2020denoising} showed that this Gaussian's variance can be modeled well with a non-learned function of $t$, and that a good estimate of the Gaussian's mean can be obtained from a ``denoising model'' as follows. Given data $\rvx_0$ and unit Gaussian noise $\epsilon$, the denoising model (in the form of a neural network) is fed ``noisy'' data $\rvx_t := \sqrt{\tilde{\alpha}_t} \rvx_0 + \sqrt{1 - \tilde{\alpha}_t} \epsilon$ and trained to recover $\epsilon$ via a mean squared error loss. The parameters $\tilde{\alpha}_t := \prod_{i=1}^t \alpha_i$ are chosen to ensure that the marginal distribution of $\rvx_t$ given $\rvx_0$ is $q(\rvx_t|\rvx_0)$ as derived from \cref{eq:q-step}. Given a weighting function $\lambda(t)$, the denoising loss is
\begin{equation} \label{eq:dm-loss}
    \mathcal{L}(\theta) = \E_{q(\rvx_0, \rvy, \epsilon)} \left[ \sum_{t=1}^T \lambda(t) \lVert \epsilon - \epsilon_\theta(\rvx_t, \rvy, t) \rVert_2^2 \right] \quad \text{with} \quad \rvx_t = \sqrt{\tilde{\alpha}_t} \rvx_0 + \sqrt{1 - \tilde{\alpha}_t} \epsilon.
\end{equation}
The mean of $p_\theta(\rvx_{t-1}|\rvx_t,\rvy)$ is obtained from the denoising model's output $\epsilon_\theta(\rvx_t, \rvy, t)$ as ${\frac{1}{\alpha_t}\rvx_t - \frac{1-\alpha_t}{\sqrt{1-\tilde{\alpha}_t}}\epsilon_\theta(\rvx_t, \rvy, t)}$.
If the weighting function $\lambda(t)$ is chosen appropriately, optimising \cref{eq:dm-loss} is equivalent to optimising a lower-bound on the data likelihood under $p_\theta$. In practice, simply setting $\lambda(t) := 1$ for all $t$ can produce more visually compelling results in the image domain~\citep{ho2020denoising}.

% In our proposed method, as in \citet{tashiro2021csdi}, the shapes of $\rvx_0$ and $\rvy$ sampled from $q(\cdot)$ vary. This is because we want to train a model which can flexibly adapt to e.g. varying numbers of observed frames. To map \cref{eq:dm-loss} to this scenario, note that both $\rvx_0$ and $\rvy$ implicitly contain information about which frames in the video they represent (via the index vectors $\mathcal{X}$ and $\mathcal{Y}$ introduced in the previous section). This information is used inside the neural network $\epsilon_\theta(\rvx_t,\rvy, t)$ so that interactions between frames can be conditioned on the distance between them (as described in the following section) and also to ensure that the sampled noise vector $\epsilon$ has the same shape as $\rvx_0$.

% \subsection{Continuous-time diffusion}

