\chapter{Conclusions and outlook}  \label{ch:conclusion}

We have demonstrated that deep generative models, and especially diffusion models, can achieve good performance even when trained over a very broad task distribution. We have also demonstrated use-cases of models trained in this manner in tasks including image inpainting, long video generation, and visual planning. 

In particular, our flexible diffusion model for long video generation in \cref{ch:fdm} generates near-photorealistic with long-range coherence while respecting and efficiently using fixed, finite computational resources. It outperforms prior work on long-duration video modelling as measured by quantitative and semantically meaningful metrics. By doing so, it demonstrates that flexible generative models have applications not just in enabling new conditional generation tasks, but also in enabling any kind of realistic generation of data that is sufficiently complex. 

In \cref{ch:tddm}, we highlighted the pitfalls of performing conditional generative modelling on varying dimensional datasets when treating state values and dimensions completely separately. We proposed a trans-dimensional model that uses jump diffusion to generate both state values and dimensions jointly during the generative process, thus making it amenable to the use of arbitrarily-conditional generation at test-time.  We believe that the same approach can further enable diffusion models to be applied in a wider variety of domains where previous restrictive fixed dimension assumptions have been unsuitable.

In \cref{ch:cigcvae} we presented IPA, a method to adapt an unconditional VAE into a conditional, and potentially flexibly conditional, VAE. Image completions generated with IPA were, at the time of its publication, close to the state-of-the-art in terms of visual fidelity in addition to improving on all baselines in terms of their coverage of the posterior as measured by LPIPS-GT. This high-fidelity coverage of the posterior makes IPA ideal for use in Bayesian optimal experimental design, as we then demonstrated. Our theoretical results suggest that, for the applications presented, using the weights of an unconditional VAE is approximately as good as training a conditional VAE from scratch. This proves that, at least in our image setting, even an unconditional VAE's encoder performs the operations necessary to function as the encoder of a flexible generative model. The fast training time when finetuning the prior into a conditional prior further suggest that the representations needed by these models are very similar between unconditional, conditional, and flexibly conditional generative models. This hints at flexible generation being broadly feasible within the variational auto-encoder framework.

\section*{Outlook}

One exciting avenue, we believe, is simply to exploit the capabilities of the models we have presented. It is likely that better performance for FDM could be achieved by designing better sampling schemes. More automated techniques to optimise these sampling schemes, and even to make them adapt when given a set of observations, would likely have further benefits. The trans-dimensional generative models we presented can in principle be applied to any domain with varying-dimensional data. Ones in which objects are explicitly represented, or tracked, would be prime targets with relevance to the vision domain~\citep{luo2021multiple,niedoba2024diffusion}. On the molecular data on which we demonstrated it, there are also many more applications to explore, such as generating larger molecules and generating molecules conditioned on what other molecules they interact with, a task of great interest in drug discovery~\citep{hoogeboom2022equivariant}. All the techniques presented could also have applications in domains with more structured data, like modelling the values obtained from simulators~\citep{weilbach2023graphically}. In these cases, data could be sufficiently complex to require the FDM-style techniques suggested in \cref{ch:fdm} in addition to being varying-dimensional and needing to be handled using the techniques proposed in \cref{ch:tddm}.

One promising direction for future research is simply to scale up the compute resources and amount of data with which we have trained the models whose results we presented. Perceptual quality and quantitative metrics have both been demonstrated to scale smoothly with compute for images~\citep{peebles2022scalable} and Sora~\citep{brooks2024video} and Stable Diffusion 3~\citep{esser2024scaling} both show what is possible with sufficient compute in the video and image domains. Doing so is likely to improve the realism of our generated images and video, allow for e.g. larger resolutions, and improve the modelling of long-range dependencies in our video data such as conditioning on the far future with FDM.

Consider the datasets on which we trained FDM. In each there was a policy for generating the sequences of actions that causally led to the frame-to-frame changes in camera pose. In MineRL the video was generated by agents that were trained to explore novel Minecraft worlds to find a goal block approximately 64 meters away~\citep{saxena2021clockwork}. The CARLA data was produced by a camera attached to an agent driven by a low level proportional–integral–derivative controller following waypoints laid down by a high level planner that was given new, random location goals to
drive to intermittently. In both cases our video model had no access to either the policy or the specific actions taken by these agents and, so, in a formal sense, our models integrate or marginalise over actions drawn from the stochastic policy used to generate the videos in the first place. We posit that our model is likely to also work well if we explicitly model actions and rewards, transforming it video generative model into a vision-based world model in the reinforcement learning sense~\citep{kaiser2019model}. Such a possibility is in addition to the already-demonstrated ability of our FDM model to be conditioned on future frames, which can in principle be up to 100s in the future on CARLA Town01). Doing so can be interpreted as running a “visual” controller which proposes a path between a current state and a specified goal.

With regard to further expanding such ``control'' or ``planning'' capabilities, we also point out that the models in this thesis were trained over various data modalities, primarily images, video and molecular data, but never over multiple modalities jointly. For instance, consider a model capable of generating speech and motion data while conditioning on video, input audio, proprioception, etc. It would have the interface needed to operate as an agent in the real world and in principle be capable of performing any tasks a human can perform. Our techniques for using flexible diffusion to generate complex data with bounded compute may be extremely helpful when scaling to model this type of data. Additionally, training on multi-modal data could be made simpler with our flexible diffusion objective in \cref{ch:fdm} that contains a task distribution that could be made to sometimes drop entire modalities. This would enable training with a mixture of single- and multi-modal data.

Finally, much of the work presented in this thesis also uses older approaches to sampling from diffusion models that the current state-of-the-art. Combining e.g. our video model with fast samplers like the Heun integrator~\citep{karras2022elucidating} is likely to greatly improve sampling speed at little cost. Incorporating newer approaches like consistency models~\citep{song2023consistency} and rectified flow~\citep{esser2024scaling} could improve speed even further.


\section*{Issues with thesis as currently written?}

\begin{itemize}
    \item TDDM does not do flexible diffusion as we defined it
    \item Am I overclaiming about what is suggested by the VAE stuff working?
    \item inconsistency in notation between $g(t)$ and $g_t$ ? and other things with $t$ as argument vs subscript
    \item put world modelling example into intro?
    \item make Fig 5.4 nicely formatted
    \item Touch on discrete time training in background - train of Diracs in integral. more natural when writing expectation, in which case we should treat u(.) as a discrete distribution. Also time can be discrete, as in TDDM (?)
    \item consider moving some of the thesis conclusion back into individual chapters
    \item remove some of the TDDM proofs and just refer to the TDDM paper
    \item make it more explicit that TDDM video model is also an any-order autoregressive model
    \item flexible diffusion models can condition on any subset of ``components'' not ``dimensions''
    \item Use wording "Reprinted with permission of …" and add acknoledgement in preface
\end{itemize}

