\chapter{Conclusions and outlook}  \label{ch:conclusion}

The combination of our contributions shows that state-of-the-art generative models can be made flexible, in the sense of being made adaptable to a wide variety of tasks at test-time, with little loss in sample quality. 
%
We found in \cref{ch:fdm} that this is the case even for very complex and high-dimensional data and, in fact, that flexible generative modelling can improve on the sample quality of standard approaches in such scenarios. Our flexible diffusion model generated near-photorealistic video with long-range coherence, outperforming prior work on long-duration video modelling.
%
With the jump diffusion model introduced in \cref{ch:tddm}, we have shown that flexible generative modelling is also possible on varying-dimensional data where we need to model the relationship between the number of dimensions and the conditioning information. We demonstrated this on both molecular data, where it enabled flexible generation given a substructure with potential applications in drug discovery~\citep{hoogeboom2022equivariant}, and video data, where it enabled visual planning even when we do not know how much time it should take to reach the goal frame.
%
Finally, in \cref{ch:cigcvae} we showed that flexible generative modelling is possible with the VAE framework, in addition to the DM framework, and that flexible VAEs can be used to obtain image inpainting systems with well-calibrated sample diversity.


\section*{Outlook}

One next step to build on this dissertation would be to simply incorporate new methods to speed up sampling. In particular, the DM-based generative models in \cref{ch:fdm,ch:tddm} both suffer from relatively slow sampling, similarly to prior DMs in other domains~\citep{ho2020denoising,song2020score}. For example, the video model presented in \cref{ch:fdm} took approximately 16 minutes to generate a 300 frame video on a GPU.  Combining our DMs with fast samplers like the Heun integrator~\citep{karras2022elucidating} is likely to greatly improve sampling speed at little cost. Incorporating newer approaches like consistency models~\citep{song2023consistency} and rectified flow~\citep{esser2024scaling} could improve speed even further. 

Another clear and promising direction for future research is to scale up the compute resources and amount of data with which we have trained our flexible generative models. Perceptual quality and quantitative metrics have both been demonstrated to scale smoothly with compute for images~\citep{peebles2022scalable}; we refer to \citet{esser2024scaling,brooks2024video} for examples of generative models trained with large compute in the image and video domains.

All the techniques presented could also have applications in domains with more structured data, like modelling the values obtained from simulators~\citep{weilbach2023graphically}. In these cases, data could be sufficiently complex to require the techniques suggested in \cref{ch:fdm} in addition to being varying-dimensional and needing to be handled using the techniques proposed in \cref{ch:tddm}. Integrating flexible generative models into tools like simulation-based inference software~\citep{gloeckler2024all} could lead to widespread use for such domains.

Consider the datasets on which we trained the video model in \cref{ch:fdm}. In each there was a policy for generating the sequences of actions that causally led to the frame-to-frame changes in camera pose. In MineRL the video was generated by agents that were trained to explore novel Minecraft worlds to find a goal block approximately 64 meters away~\citep{saxena2021clockwork}. The CARLA data was produced by a camera attached to an agent driven by a low level proportional–integral–derivative controller following waypoints laid down by a high level planner that was given new, random location goals to
drive to intermittently. In both cases our video model had no access to either the policy or the specific actions taken by these agents and, so, in a formal sense, our models integrate or marginalise over actions drawn from the stochastic policy used to generate the videos in the first place. We posit that our model is likely to also work well if we explicitly model actions and rewards, transforming it video generative model into a vision-based world model in the reinforcement learning sense~\citep{kaiser2019model}. Such a possibility is in addition to the already-demonstrated ability of our model to be conditioned on future frames, which can in principle be up to 100s of frames in the future on CARLA Town01). Combining these capabilities would make it possible to condition on a current and goal frame, and explicitly produce actions to navigate between them.

The above would be a joint vision-action model, but we could also integrate more modalities. For instance, consider a model capable of generating speech and motion data while conditioning on video, input audio, proprioception, etc. It would have the interface needed to operate as an agent in the real world and in principle be capable of performing any tasks a human can perform. Our techniques from \cref{ch:fdm} for generating complex data with bounded compute may be extremely helpful when scaling to model this type of data. Additionally, training on multi-modal data could be made more practical with our flexible diffusion objective whose task distribution could be made to sometimes drop entire modalities. This would enable training with a mixture of single- and multi-modal data, or on any data source where modalities are sometimes corrupted.


\section*{Issues with thesis as currently written?}

\begin{itemize}
    \item inconsistency in notation between $g(t)$ and $g_t$ ? and other things with $t$ as argument vs subscript. like $\rvb_t$ the drift term.
    \item put world modelling example into intro?
    \item make Fig 5.4 nicely formatted
    \item Touch on discrete time training in background - train of Diracs in integral. more natural when writing expectation, in which case we should treat u(.) as a discrete distribution. Also time can be discrete, as in FDM
    \item Remove subscript zeros, i.e. $\rvx_0$, from TDDM section
    \item Update table of notation
\end{itemize}

