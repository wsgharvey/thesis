\chapter{Conclusions}  \label{ch:conclusion}

We have demonstrated the power of deep generative models, and especially diffusion models, to achieve good performance even when trained over a very broad task distribution. Training over such a task distribution allows for ``flexible conditioning'' at test-time. Delving further in the direction of adding flexibility, we showed that, in the case of video diffusion models, frames can be ``dropped out'' and letting us learn a plausible model of the world with smaller computational budgets than would otherwise be necessary. While we have focused on models in the vision domain primarily due to the easily accessible datasets in this domain and well-studied benchmarks, related work suggests that such flexible conditioning is likely to work similarly well in other domains~\citep{weilbach2022graphically}. Applying such models in more domains is an exciting area for future research. Another exciting avenue, we believe, is simply to exploit the capabilities of the models we have presented. For example, our video model could be leveraged as a world model, with the additional capability of being able to condition on desired future states as well as on the current world state.