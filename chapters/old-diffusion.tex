\chapter{Review: Diffusion Models}
\label{sec:diffusion}

Diffusion models, or DMs~\citep{sohl2015deep,ho2020denoising,nichol2021improved,song2020score}, can be understood as hierarchical variational auto-encoders with a very simple encoder: rather than parameterizing a distribution with a neural network, the encoder simply adds Gaussian noise to the data. This ``encoder'' is commonly then referred to as the ``forward'' (or ``noising'') process. The equivalent of the VAE's prior and decoder are then the ``reverse'' (or ``generative'') process, which uses a learned ``denoising'' function in the form of a neural network. Sampling from a DM begins with drawing a sample from a Gaussian distribution that approximates a heavily noised sample from the data distribution. Running the reverse/generative process on this sample then gradually removes noise from it, moving the sample closer to the data manifold. By the end of the generative process, if all is trained well, the samples from this process should match the data distribution. As we will show later, fitting the DM simply involves training a neural network to predict clean data give noisy data using a mean-squared error loss. This leads to more stable training than for a VAE, in which the prior, decoder, and encoder are all learned jointly, leading to a ``moving target'' problem as each component must keep compensating for changes to the others.

We specify ``how much'' noise is added to the data at any point with a timestep $t$ which is defined so that, when $t=0$, our data has no added noise, and more noise is added as $t$ increases. We use $\rvx_t$ to denote a copy of data with noise added according to $t$, so $\rvx_0 := \rvx$ is clean data. Making a comparison to the hierarchical VAE framework, for $t>0$, $\rvx_t$ can be understood as a form of latent variable which contains some information from the data. We no longer use the $\rvz$ or $\rvz_l$ notation since, first, these latent variables now live in the same space as $\rvx$ and, second, this new notation allows us to consider a continuum of latent variables corresponding to any positive real value of $t$.

% \todo[inline]{don't need to discuss this until later}
% The noising and generative processes can be understood in either discrete-time or continuous time. That is, we can imagine noise being added to the data either in a discrete series of steps or as a continuous process. In discrete time the analogy to hierarchical VAEs is precise, but it is simpler to first describe continuous-time diffusion models. In practice, there is little practical distinction between discrete and continuous time diffusion models since we must use a discretisation when sampling from a continuous time diffusion model. 

We define how much noise is present at each timestep through a monotonically increasing schedule $\sigma(t)$ which maps from the timestep to the ``noise-to-signal ratio'' of the data. Formally, given clean data $\rvx_0$, a timestep $t$, and $\sigma := \sigma(t)$, the noisy data is distributed according to
\begin{equation} \label{eq:diffusion-xt-given-x0}
    q_\sigma(\rvx_t|\rvx_0) = \gN ( \rvx_t ; \rvx_0, \sigma^2 \mI ).
\end{equation}
Note the notational changes from a VAE's encoder: we no longer parameterise by $\phi$ as there are no longer any encoder parameters; we now explicitly subscript by $\sigma$ which may take on any of a continuous range of values; and our latent variables are now denoted $\rvx_t$. To define a conditional diffusion model, we must also recall the relationship to conditioning information $\rvy$. In conjunction with the data distribution $\pdata(\rvx_0, \rvy)$ we therefore define the marginal distribution
\begin{equation}
    q_\sigma(\rvx_t,\rvy) = \int q_\sigma(\rvx_t|\rvx_0) \pdata(\rvx_0, \rvy) \mathrm{d} \rvx_0
\end{equation}
and its conditional
\begin{equation}
    q_\sigma(\rvx_t|\rvy) = \int q_\sigma(\rvx_t|\rvx_0) \pdata(\rvx_0|\rvy) \mathrm{d} \rvx_0.
\end{equation}

Our training procedure can be understood as characterising these conditional distributions via a neural network that approximates the score function $\nabla_{\rvx_t} \log q_\sigma(\rvx_t)$. Our sampling procedure will use the learned approximate score function to morph samples, over a series of $N$ steps, through approximations of $q_{\sigma_0}(\cdot | \rvy)$, $q_{\sigma_1}(\cdot | \rvy)$ and so on until reaching the conditional data distribution $q_{\sigma_N}(\cdot | \rvy) = q_{0}(\cdot | \rvy)$. 

% For brevity, we will overload notation slightly by defining $\rvx_t := \rvx_{\sigma(t)}$ and $q_t(\cdot) := q_{\sigma(t)}(\cdot)$. We will from now on use either $\rvx_t$ and $q_t(\cdot)$ or $\rvx_\sigma$ and $q_\sigma(\cdot)$ according to convenience. We will always define $\sigma$ such that $\sigma(0) = 0$ so that $\rvx_0$ $q_0(\cdot)$ refer to clean data in either case.


\section{Score matching, or how to train your diffusion model}

The training objective of a diffusion model can be understood as independent of the schedule $\sigma$, and so in this section we will avoid referring to $t$ and instead overload notation by defining $\rvx_\sigma$ as equal to $\rvx_t = \rvx_{\sigma^{-1}(\sigma)}$, and similarly write $q_\sigma$ instead of $q_t$. We will always define $\sigma$ such that $\sigma(0) = 0$ so that $\rvx_0$ and $q_0(\cdot)$ refer to clean data in either case.

We will derive an objective for training a neural network to approximate the score function $\nabla_{\rvx_\sigma} \log q_\sigma(\rvx_\sigma |\rvy)$~\citep{vincent2011connection,song2019generative}. The neural network has parameters $\theta$ and takes $\rvx$, $\rvy$, and $\sigma$ as input. We call its output $\rvs_\theta(\rvx, \rvy, \sigma)$. For any given $\sigma$, corresponding to a specific distribution which we wish to fit the score function of. 

% Consider the joint distribution $q_{\sigma}(\rvx_\sigma, \rvx_0, \rvy)$ in which we explicitly consider the relationship between noised data $\rvx_\sigma$ and clean data $\rvx_0$. Its conditional $q_{\sigma}(\rvx_\sigma|\rvx_0,\rvy)$ is Gaussian with mean $\rvx_0$ and variance $\sigma^2$, following \cref{eq:diffusion-xt-given-x0}. Its score function can therefore be analytically computed as $\frac{\rvx_0 - \rvx_\sigma}{\sigma^2}$. Following \citet{vincent2011connection}, we now show that fitting $\rvs_\theta(\rvx, \rvy, \sigma)$ to this quantity with a mean-squared error loss is equivalent to fitting it to score of the distribution of interest, $q_{\sigma}(\rvx_\sigma | \rvy)$.

An ideal objective to do would be the mean-squared error between the neural network's outputs and the desired conditional score function $\nabla_{\rvx_\sigma} \log q_{\sigma}(\rvx_\sigma|\rvy)$. We call this the score-matching objective $\gL_\text{SM}$, but in general it is intractable because we do not have access to $\nabla_{\rvx_\sigma} \log q_{\sigma}(\rvx_\sigma|\rvy)$. We can, however, obtain an unbiased estimate of it; we begin by breaking it down as
\begin{align}
    \mathcal{L}_\text{SM}(\theta, \sigma) &= \EX_{q_{\sigma}(\rvx_\sigma, \rvy)} \left[ || \nabla_{\rvx_\sigma} \log q_{\sigma}(\rvx_\sigma|\rvy) - \rvs_\theta(\rvx_\sigma, \rvy, \sigma) ||_2^2 \right] \\
    &= \EX_{q_{\sigma}(\rvx_\sigma, \rvy)} \big[ 
    || \rvs_\theta(\rvx_\sigma, \rvy, \sigma) ||_2^2
    \nonumber\\ &\qquad\qquad\qquad
    - 2 \left\langle \nabla_{\rvx_\sigma} \log q_{\sigma}(\rvx_\sigma|\rvy),
    \rvs_\theta(\rvx_\sigma, \rvy, \sigma) \right\rangle \big]
    \nonumber\\ &\qquad\qquad\qquad
    + C_1
\end{align}
where $C_1$ does not depend on $\theta$. We show in \cref{sec:proof-that-diffusion-does-score-matching} that this second term, which we call $S(\theta, \sigma)$, can be rewritten in terms of the tractable score of the Gaussian $q_{\sigma}(\rvx_\sigma|\rvy)$ instead of the intractable score of $q_{\sigma}(\rvx_\sigma|\rvy)$:
\begin{align}
\allowdisplaybreaks
S(\theta, \sigma) &= \EX_{q_{\sigma}(\rvx_\sigma, \rvy)} \left[ - 2 \left\langle \nabla_{\rvx_\sigma} \log q_{\sigma}(\rvx_\sigma|\rvy), \rvs_\theta(\rvx_\sigma, \rvy, \sigma) \right\rangle \right] \\
    &= \EX_{q_{\sigma}(\rvx_0, \rvx_\sigma, \rvy)} \left[ -2 \left\langle \nabla_{\rvx_\sigma} \log q_{\sigma}(\rvx_\sigma|\rvx_0), \rvs_\theta(\rvx_\sigma, \rvy, \sigma) \right\rangle  \right].
\end{align}
Substituting this identity back into our desired objective, and letting $C_1$ and $C_2$ be two scalars that do not depend on $\theta$, we get
\begin{align}
    \mathcal{L}_\text{SM}(\theta, \sigma) &= \EX_{q_{\sigma}(\rvx_0, \rvx_\sigma, \rvy)} \big[ 
    || \rvs_\theta(\rvx_\sigma, \rvy, \sigma) ||_2^2
    \nonumber\\ &\qquad\qquad\qquad
    - 2 \left\langle \nabla_{\rvx_\sigma} \log q_{\sigma}(\rvx_\sigma|\rvx_0), \rvs_\theta(\rvx_\sigma, \rvy, \sigma) \right\rangle \big] + C_1 \\
    &= \EX_{q_{\sigma}(\rvx_0, \rvx_\sigma, \rvy)} \big[ 
    || \rvs_\theta(\rvx_\sigma, \rvy, \sigma) ||_2^2
    \nonumber\\ &\qquad\qquad\qquad
    - 2 \left\langle \nabla_{\rvx_\sigma} \log q_{\sigma}(\rvx_\sigma|\rvx_0), \rvs_\theta(\rvx_\sigma, \rvy, \sigma) \right\rangle
    \nonumber\\ &\qquad\qquad\qquad
    + || \nabla_{\rvx_\sigma} \log q_{\sigma}(\rvx_\sigma|\rvx_0) ||_2^2 \big] + C_2 \\
    &= \EX_{q_{\sigma}(\rvx_0, \rvx_\sigma, \rvy)} \big[ 
    || \rvs_\theta(\rvx_\sigma, \rvy, \sigma) - \nabla_{\rvx_\sigma} \log q_{\sigma}(\rvx_\sigma|\rvx_0) ||_2^2 \big] + C_2.
\end{align}
Introducing the analytic score of the Gaussian $\rvx_0$, and defining our implicit score-matching loss as $\mathcal{L}_\text{ISM}(\theta, \sigma) := \mathcal{L}_\text{SM}(\theta, \sigma) - C_2$, we get
\begin{align}
    \mathcal{L}_\text{ISM}(\theta, \sigma) &= \EX_{q_{\sigma}(\rvx_0, \rvx_\sigma, \rvy)} \left[ 
    || \rvs_\theta(\rvx_\sigma, \rvy, \sigma) - \frac{\rvx_0-\rvx_\sigma}{\sigma^2} ||_2^2 \right]
\end{align}
We can therefore train a diffusion model with the simple objective of a mean-squared error loss between the neural network's output and $\frac{\rvx_0-\rvx_\sigma}{\sigma^2}$.

The loss can be equivalently computed in different spaces, for example a a squared error loss to $\rvx_0$ with
\begin{align}
    \mathcal{L}_\text{ISM}(\theta, \sigma) &= \frac{1}{\sigma^4} \EX_{q_{\sigma}(\rvx_0, \rvx_\sigma, \rvy)} \left[ 
    || \hat{\rvx}_0(\rvx_\sigma, \rvy, \sigma) - \rvx_0 ||_2^2 \right]
\end{align}
where $\hat{\rvx}_0(\rvx_\sigma, \rvy, \sigma) := \sigma^2 \cdot \rvs_\theta(\rvx_\sigma, \rvy, \sigma) + \rvx_\sigma$. 
It is often also written as a squared error loss to $\epsilon := \frac{\rvx_\sigma - \rvx_0}{\sigma}$, a scaled version of the noise added when sampling $\rvx_\sigma \sim q_\sigma(\cdot|\rvx_0)$, with
\begin{align}
    \mathcal{L}_\text{ISM}(\theta, \sigma) &= \frac{1}{\sigma^2} \EX_{q_{\sigma}(\rvx_0, \rvx_\sigma, \rvy)} \left[ 
    || \hat{\mathbf{\epsilon}}_\theta(\rvx_\sigma, \rvy, \sigma) - \epsilon ||_2^2 \right]
\end{align}
where $\hat{\mathbf{\epsilon}}_\theta(\rvx_\sigma, \rvy, \sigma) := \sigma \cdot \rvs_\theta(\rvx_\sigma, \rvy, \sigma)$.

For sampling to work well, the loss above must be low across a range of noise levels. We can therefore write the full loss with an integral over $\theta$:
\begin{align} \label{eq:diffusion-loss-all-sigma}
    \mathcal{L}_\text{ISM}(\theta) &= \int_{\sigma_\text{min}}^{\sigma_\text{max}} \lambda(\sigma) \frac{1}{\sigma^2} \EX_{q_{\sigma}(\rvx_0, \rvx_\sigma, \rvy)} \left[ 
    || \hat{\mathbf{\epsilon}}_\theta(\rvx_\sigma, \rvy, \sigma) - \epsilon ||_2^2 \right] \mathrm{d}\sigma
\end{align}
where $\lambda(\sigma)$ is a weighting function that controls how much network capacity is spent modelling the score function at each noise level. We show in \cref{sec:diffusion-likelihood} that, if $\lambda(\sigma)$ is set appropriately, this loss yields a lower-bound on the data likelihood. Before doing so, however, we show how to use the learned artefact $\hat{\mathbf{\epsilon}}_\theta$ to define and sample from a generative model.

% We show in ..., following \citet{kingma2021variational}, that setting $\lambda(\sigma) := \sigma^2$ makes \cref{eq:diffusion-loss-all-sigma} a lower-bound on the data log likelihood.

\section{Variance-exploding diffusion as a stochastic differential equation}
Approximating the score function of these $q_\sigma(\cdot|\rvy)$ enables us to, for example, we can now use approximate Langevin dynamics to approximately sample from each $q_\sigma(\cdot|\rvy)$. Our target, though, is to sample from $q_0(\cdot|\rvy)$. State-of-the-art methods for doing so start by sampling $\rvx_{\sigma_\text{max}}$ from an approximation of $q_{\sigma_\text{max}}(\cdot|\rvy)$ and then gradually nudging these samples through a series of distributions $q_{\sigma}(\cdot|\rvy)$ for decreasing values of $\sigma$. By the time we reach $\sigma = 0$, the samples will be distributed according to the posterior under the data distribution, $q_0(\rvx_0|\rvy)$. Knowing how to ``nudge'' samples from being distributed according to $q_{\sigma}(\cdot|\rvy)$ to being distributed according to $q_{\sigma\mathrm{d}\sigma}(\cdot|\rvy)$ requires some form of ``links'' between these two distributions. One way in which we can ``link'' our distributions is by viewing them all as marginals of a stochastic process. In particular, consider the stochastic process which starts from the data distribution $q_0(\rvx_0|\rvy)$ at time $t = 0$ and in which noise is added independently to each dimension as $t$ increases according to the stochastic differential equation
\begin{equation}
    \mathrm{d}\rvx = g(t) \mathrm{d}\rvw
\end{equation}
where $\rvw$ is the standard Wiener process, also known as Brownian motion. By timestep $t \geq 0$, the variance of the added noise is
\begin{equation}
    \sigma_t^2 = \int_0^{t'} g(t')^2 \mathrm{d}t',
\end{equation}
and so each $\rvx_t$ will be distributed according to $q_{\sigma(t)}(\rvx_t)$.  Importantly, this process implies a relationship between $\rvx_0$, $\rvx_s$, and $\rvx_t$ for any $0 \leq s \leq t$. In particular, the joint distribution of $\rvx_0$, $\rvx_s$, $\rvx_t$ can now be 

\section{Diffusion models as variational auto-encoders}

\subsection{A mechanism for sampling}
So far we have defined a family of conditional distributions $q_\sigma(\rvx_t|\rvy)$ for all $\sigma \geq 0$, and presented a method to learn an approximation of each score $\nabla_\rvx \log q_\sigma(\rvx|\rvy)$. We now look at how to use this score function to obtain good samples from our target distribution, $q_0(\rvx_0|\rvy)$. We will investigate two methods for doing so, but first present a rough outline that they both follow. Using $\sigma_\text{data}$ as a scalar estimate of the standard deviation of the data distribution. We start by sampling $\rvx_{\sigma_\text{max}}$ from $\gN(0; (\sigma_\text{data}^2+\sigma_\text{max}^2)\mI)$, a Gaussian approximation of $q_{\sigma_\text{max}}(\rvx_{\sigma_\text{max}} | \rvy)$. This approximation becomes better as $\sigma_\text{max}$ increases so it is typically set to a large value (e.g. 80~\cite{karras2022elucidating}). We then step through a series of $N$ distributions $p_\sigma(\rvx|\rvy)$ with $\sigma$ equal to each of $\sigma_0 := \sigma_\text{max} > \sigma_1 > \cdots > \sigma_N = 0$. To step between each pair of distributions, deriving e.g. $\rvx_{\sigma_{i-1}}$ from $\rvx_{\sigma_{i}}$, we must ``nudge'' each $\rvx$ in a direction that approximately morphs samples from $q_{\sigma_{i-1}}(\cdot|\rvy)$ towards being distributed according to $q_{\sigma_{i}}(\cdot|\rvy)$. The form of this ``nudge'' is what differentiates the algorithms we will look at. By the end of this sequence, we will have nudged the samples all the way to $q_0(\cdot|\rvy)$, yielding approximate samples from the target distribution.

% We now present three methods.

% \subsection{A Markovian noising process}
We can view $q_\sigma( \rvx_\sigma | \rvy)$ for each $\sigma \in \{\sigma_0,\ldots, \sigma_N\}$ as a marginal of a larger joint distribution $q_{\sigma_0,\sigma_1,\ldots,\sigma_N}(\rvx_{\sigma_0},\rvx_{\sigma_1},\ldots,\rvx_{\sigma_N})$. If we assume that this joint distribution has the Markovian form
\begin{equation} \label{eq:ddpm-joint-q}
    q_{\sigma_0,\sigma_1,\ldots,\sigma_N}(\rvx_{\sigma_0},\rvx_{\sigma_1},\ldots,\rvx_{\sigma_N}|\rvy) = q_{\sigma_N}(\rvx_0|\rvy) \prod_{i=1}^N q_{\sigma_{i}:\sigma_{i-1}}(\rvx_{\sigma_i-1}|\rvx_{\sigma_{i}})
\end{equation}
then there is only one allowable form for each conditional distributions which yields the required marginals. Specifically we will have
\begin{equation}
    q_{\sigma_{i-1}:\sigma_i}(\rvx_i|\rvx_{i-1}) = \gN( \rvx_{i-1} ; \left(\sigma_i^2 - \sigma_{i-1}^2 \right) \mI ).
\end{equation}

A natural way to set up the generative process, then, would be such that the joint distribution of generations at all steps $\rvx_{\sigma_1},\ldots,\rvx_{\sigma_N}$ matches the joint distribution in \cref{eq:ddpm-joint-q}. Naming such a joint distribution $p_\theta(\rvx_{\sigma_0},\rvx_{\sigma_1},\ldots,\rvx_{\sigma_N})$, the factorisation of $q$ suggests that we can similarly factorise it as
\begin{equation} \label{eq:ddpm-joint-p}
    p_\theta(\rvx_{\sigma_0},\rvx_{\sigma_1},\ldots,\rvx_{\sigma_N}|\rvy) = p(\rvx_{\sigma_0}) \prod_{i=1}^N p_\theta(\rvx_{\sigma_{i}}|\rvx_{\sigma_{i-1}}, \rvy)
\end{equation}
where $p(\rvx_{\sigma_0})$ is set to $\gN(0; (\sigma_\text{data}^2+\sigma_0^2)\mI) \approx q_{\sigma_0}(\rvx_{\sigma_0})$ as discussed earlier. Before considering how we parameterise $p_\theta(\rvx_{\sigma_{i}}|\rvx_{\sigma_{i-1}})$, first note that we can obtain a lower-bound on the log-likelihood with
\begin{align}
    \log p_\theta(\rvx_0|\rvy) &= \log p_\theta(\rvx_{\sigma_N}|\rvy) \\
    &\geq \EX_{q(\rvx_{\sigma_{0:N-1}}|\rvx_{\sigma_N})} \left[ \log \frac{p_\theta (\rvx_{\sigma_{0:N}}|\rvy)}{q(\rvx_{\sigma_{0:N-1}}|\rvx_{\sigma_N})} \right]
\end{align}
where we use $\rvx_{\sigma_{0:N-1}}$ as shorthand for $\rvx_{\sigma_{0}},\ldots,\rvx_{\sigma_{N-1}}$. This can be broken down into two distinct terms:
\begin{align} \label{eq:diffusion-elbo-2}
    \log p_\theta(\rvx_0|\rvy) &\geq - \kl{q(\rvx_{\sigma_0}|\rvy)}{p(\rvx_{\sigma_0})} - \sum_{i=1}^N \kl{q_{\sigma_{i-1}:\sigma_{i}}(\rvx_{\sigma_i}|\rvx_{\sigma_{i-1}}, \rvx_{\sigma_N})}{p_\theta(\rvx_{\sigma_{i}}|\rvx_{\sigma_{i-1}}, \rvy)}
\end{align}
It is now clear that, to maximise this evidence lower-bound, we should select each $p_\theta(\rvx_{\sigma_{i}}|\rvx_{\sigma_{i-1}}, \rvy)$ so as to minimise its KL divergence to $q_{\sigma_{i-1}:\sigma_{i}}(\rvx_{\sigma_i}|\rvx_{\sigma_{i-1}}, \rvx_{\sigma_N})$. Note that, due to the properties of our noising process defined in \cref{eq:ddpm-joint-q}, $q_{\sigma_{i-1}:\sigma_{i}}(\rvx_{\sigma_i}|\rvx_{\sigma_{i-1}}, \rvx_{\sigma_N})$ is a Gaussian 
\begin{equation}
    q(\rvx_{t-1}|\rvx_t, \rvx_0) = \gN ( \rvx_{t-1} ; \tilde{\mathbf{\mu}}_t(\rvx_t,\rvx_0), \tilde{\beta}_t \mI )
\end{equation}
with $\tilde{\mathbf{\mu}}_t(\rvx_t,\rvx_0) := \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}\rvx_0 + \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}\rvx_t$ and $\tilde{\beta}_t = \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t} \beta_t$. Since we need $p_\theta(\rvx_{t-1}|\rvx_t)$ to be close to this distribution to maximise the lower-bound, we use a very similar distribution:
\begin{equation}
    p_\theta(\rvx_{t-1}|\rvx_t) = \gN ( \rvx_{t-1} ; \tilde{\mathbf{\mu}}_t(\rvx_t,\hat{\rvx}_\theta(\rvx_t, t)), \tilde{\beta}_t \mI ).
\end{equation}
Note the only difference between $p_\theta(\rvx_{t-1}|\rvx_t)$ and $q(\rvx_{t-1}|\rvx_t, \rvx_0)$ is that we replaced $\rvx_0$ with a learned estimate of $\rvx_0$ given $\rvx_t$. Our sampling procedure can then be as follows: ... .

\subsection{Computing the evidence lower-bound}
Given the suggested form for $p_\theta(\rvx_{t-1}|\rvx_t)$, we can simplify the lower-bound in \cref{eq:diffusion-elbo-2} using the analytic form of the KL divergence between two Gaussians. In particular, we have that
\begin{align}
    &\kl{q(\rvx_{t-1}|\rvx_t,\rvx_0)}{p_\theta(\rvx_{t-1}|\rvx_t, \rvy)} \\
    =& \frac{1}{2 \beta_t} ||\tilde{\mathbf{\mu}}_t(\rvx_t,\rvx_0) - \tilde{\mathbf{\mu}}_\theta(\rvx_t, t)||_2^2 + \frac{d}{2} \left( \frac{\tilde{\beta}_t}{\beta_t} + \log\frac{\beta_t}{\tilde{\beta}_t} - 1 \right) \\
    =& \frac{1}{2 \beta_t} ||\tilde{\mathbf{\mu}}_t(\rvx_t,\rvx_0) - \tilde{\mathbf{\mu}}_\theta(\rvx_t, t)||_2^2 + \text{constant}.
\end{align}
Derive ELBO in terms of predictions of $\rvx_0$. Then in terms of score and connect back to loss weighting for score prediction.

Then take the continuous time limit, and point out that it's an SDE.

Then introduce corresponding ODE sampler.

% \begin{align} \label{eq:diffusion-elbo-2}
%     \log p_\theta(\rvx_0|\rvy) &\geq - \kl{q(\rvx_{\sigma_0}|\rvy)}{p(\rvx_{\sigma_0})} - \sum_{i=1}^N \kl{q_{\sigma_{i-1}:\sigma_{i}}(\rvx_{\sigma_i}|\rvx_{\sigma_{i-1}}, \rvx_{\sigma_N})}{p_\theta(\rvx_{\sigma_{i}}|\rvx_{\sigma_{i-1}}, \rvy)} \\
%     &\geq - \kl{q(\rvx_{\sigma_0}|\rvy)}{p(\rvx_{\sigma_0})} - \sum_{i=1}^N \kl{q_{\sigma_{i-1}:\sigma_{i}}(\rvx_{\sigma_i}|\rvx_{\sigma_{i-1}}, \rvx_{\sigma_N})}{p_\theta(\rvx_{\sigma_{i}}|\rvx_{\sigma_{i-1}}, \rvy)}
% \end{align}

% with mean and covariance
% $$ \mu \quad\text{and}\quad \sigma $$.

% \begin{equation}
% p_\theta(\rvx_{\sigma_{i}}|\rvx_{\sigma_{i-1}}) = \frac{q_{\sigma_{i-1}:\sigma_{i}}(\rvx_i|\rvx_{i-1})q_{\sigma_{i-1}}(\rvx_{\sigma_{i-1}})}{q_{\sigma_{i-1}}(\rvx_{\sigma_{i-1}})}
% \end{equation}
% for each $i \in \{1,\ldots,N\}$.

TODO: infinite limit, so infinite depth VAEs


\section{Treating diffusion models as differential equations}
\subsection{Efficient samplers}
\subsection{Exact likelihoods}


% \subsection{Ordinary differential equations}


% \subsection{Stochastic differential equations}


% using a Gaussian approximation of $q_{\sigma_\text{max}}(\rvx_t, \rvy)$.

% now start by approximating $\rvx_{\sigma_\text{max}}$ for some large $\sigma_\text{max}$. noise le

% We can now more formally describe our sampling procedure, following the notation of \citet{karras2022elucidating}. We take as hyperparameters a number of steps $N$ and a series of noise levels $\sigma_0 > \sigma_1 > \cdots > \sigma_N = 0$. If $\sigma_0$ is sufficiently large then, for any $\rvy$, $q_{\sigma_N}(\rvx_{\sigma_N}|\rvy) $ is well approximated by $\gN(\rvx_{\sigma_N} ; \vzero, {\sigma_N}^2\mI)$ and so we start by sampling $\rvx_{\sigma_N} \sim \gN(\vzero, {\sigma_N}^2\mI)$. We then transform $\rvx_{\sigma_N}$ into each $\rvx_i$


\section{Evluating and optimising data likelihood} \label{sec:diffusion-likelihood}


\chapter{Some proof} \label{sec:proof-that-diffusion-does-score-matching}

\begin{align}
\allowdisplaybreaks
S(\theta, t) &= \EX_{q_{\sigma(t)}(\rvx_t, \rvy)} \left[ - 2 \left\langle \nabla_{\rvx_t} \log q_{\sigma(t)}(\rvx_t|\rvy)^\top, \rvs_\theta(\rvx_t, \rvy, \sigma) \right\rangle \right] \\
    &= \EX_{\pdata(\rvy)}\left[  - 2 \int q_{\sigma(t)}(\rvx_t | \rvy) \left\langle \nabla_{\rvx_t} \log q_{\sigma(t)}(\rvx_t | \rvy), \rvs_\theta(\rvx_t, \rvy, \sigma) \right\rangle \mathrm{d} \rvx_t \right] \\
    &= \EX_{\pdata(\rvy)}\left[ - 2 \int q_{\sigma(t)}(\rvx_t | \rvy) \left\langle \frac{\nabla_{\rvx_t} q_{\sigma(t)}(\rvx_t|\rvy)}{q_{\sigma(t)}(\rvx_t | \rvy)}, \rvs_\theta(\rvx_t, \rvy, \sigma) \right\rangle \mathrm{d} \rvx_t \right] \\
    &= \EX_{\pdata(\rvy)}\left[  - 2 \int \left\langle \nabla_{\rvx_t} q_{\sigma(t)}(\rvx_t | \rvy), \rvs_\theta(\rvx_t, \rvy, \sigma) \right\rangle \mathrm{d} \rvx_t \right] \\
    &= \EX_{\pdata(\rvy)}\left[  - 2 \int \left\langle \nabla_{\rvx_t} \int \pdata(\rvx_0|\rvy) q_{\sigma(t)}(\rvx_t|\rvx_0) \mathrm{d}\rvx_0, \rvs_\theta(\rvx_t, \rvy, \sigma) \right\rangle \mathrm{d} \rvx_t \right] \\
    &= \EX_{\pdata(\rvy)}\left[  - 2 \int \left\langle \int \pdata(\rvx_0|\rvy) \nabla_{\rvx_t} q_{\sigma(t)}(\rvx_t|\rvx_0) \mathrm{d}\rvx_0, \rvs_\theta(\rvx_t, \rvy, \sigma) \right\rangle \mathrm{d} \rvx_t \right] \\
    &= \EX_{\pdata(\rvy)}\left[  - 2 \int \left\langle \int \pdata(\rvx_0|\rvy) q_{\sigma(t)}(\rvx_t|\rvx_0) \nabla_{\rvx_t} \log q_{\sigma(t)}(\rvx_t|\rvx_0) \mathrm{d}\rvx_0, \rvs_\theta(\rvx_t, \rvy, \sigma) \right\rangle \mathrm{d} \rvx_t \right] \\
    &= \EX_{\pdata(\rvy)}\left[  - 2 \int q_{\sigma(t)}(\rvx_0, \rvx_t|\rvy) \left\langle \nabla_{\rvx_t} \log q_{\sigma(t)}(\rvx_t|\rvx_0), \rvs_\theta(\rvx_t, \rvy, \sigma) \right\rangle  \mathrm{d}\rvx_0 \mathrm{d} \rvx_t \right] \\
    &= \EX_{q_{\sigma(t)}(\rvx_0, \rvx_t, \rvy)} \left[ -2 \left\langle \nabla_{\rvx_t} \log q_{\sigma(t)}(\rvx_t|\rvx_0), \rvs_\theta(\rvx_t, \rvy, \sigma) \right\rangle  \right]
\end{align}


\chapter{Old Diffusion Model Stuff}

Diffusion models, or DMs~\citep{sohl2015deep,ho2020denoising,nichol2021improved,song2020score}, are hierarchical variational auto-encoders in which the encoder has a particular structure. Specifically, the encoder takes the form of a Gaussian diffusion process which repeatedly adds noise to the data until any signal in the data is lost. This diffusion process can be described in discrete or continuous time; we will use both but begin with discrete time where the link is most clear.

\section{Discrete-time diffusion}
\subsection{Diffusion process: A Non-learnable Encoder}
We denote the discrete timesteps $0,\ldots,T$, defining them such that $\rvx_0=\rvx$ is data without noise, the first group of latent variables $\rvz_1$ is data with a very small amount of noise added, and so on until the final group of latent variables, $\rvz_T$, is data with so much noise added that it indistinguishable from, or close to indistinguishable from, a sample from a Gaussian distribution. Given that $\rvz_t$ must now live in the same space as the data $\rvx$, we will from now on simplify notation by defining $\rvx_t := \rvz_t$ whenever $t > 0$ and writing $\rvx_t$ instead of $\rvz_t$.

Similarly to our use of $q_\phi(\rvz, \rvx,\rvy)=\pdata(\rvx,\rvy)q_\phi(\rvz_{1:L}|\rvx)$ in \cref{eq:r} to denote the combination of the data distribution and a VAE encoder's distribution, we will write the corresponding distribution for a DM as $q(\rvx_{0:T},\rvy) = \pdata(\rvx_0,\rvy) q(\rvx_{1:T}|\rvx)$. Recall that $\rvx_0 := \rvx$, and note that we have dropped the $\phi$ subscript since the encoder no longer has learnable parameters and now denote the ``depth'' of the encoder with $T$ rather than $L$. This distribution factorises as 
\begin{equation} \label{eq:q-joint}
    q(\rvx_{0:T}, \rvy) = q(\rvx_0, \rvy) \prod_{t=1}^T q(\rvx_t|\rvx_{t-1}).
\end{equation}
where, for each $t \in \{1, \ldots, T\}$, the corresponding ``transition'' distribution is a Gaussian of the form:
\begin{equation} \label{eq:q-step-general}
    q(\rvx_t | \rvx_{t-1}) = \gN(\rvx_t ; c_t  \rvx_{t-1} , \beta_t \rmI ).
\end{equation}
Here, $\beta_t$ and $c_t$ are scalar hyperparameters defined for $t \in \{1, \ldots, T\}$. The scalar $\beta_t$ is the variance of the noise to add at this step. It is typically set to be small, so that only a small amount of noise is added in any given step, but still large enough that, after $T$ steps (with $T$ often set to $1000$), there is much more noise than signal so that $q(\rvx_T)$ is approximately Gaussian. Meanwhile, $c_t$ can be interpreted as the factor by which the ``signal'' is multiplied. There are two common choices for $c_t$. The simplest is to set it to $1$; this leads to what is known as a ``variance-exploding'' diffusion process~\citep{song2020score} in which the variance of the marginal $q(\rvx_t)$ increases as $t$ increases. Alternatively, setting $c_t := \sqrt{1-\beta_t}$ leads to a ``variance-preserving'' diffusion process~\citep{song2020score} because it ensures that, if the marginal $q(\rvx_{t-1})$ has unit variance, $q(\rvx_t)$ will also have unit variance.

For now we will focus on variance-preserving processes, but we will return to variance-exploding processes in the continuous-time setting. We will follow the notation of \citet{nichol2021improved}: the variance of the noise added in each step is $\beta_t$ as in \cref{eq:q-step-general}; they define $\alpha_t = 1-\beta_t$; they use a variance-preserving process so the scaling factor which we called $c_t$ becomes $\sqrt{1-\beta_t}$ or equivalently $\sqrt{\alpha_t}$. The Gaussian transition distribution in this case is therefore
\begin{equation} \label{eq:q-step-iddpm}
    q(\rvx_t | \rvx_{t-1}) = \gN(\rvx_t ; \sqrt{\alpha_t}  \rvx_{t-1} , \beta_t \rmI ).
\end{equation}

Under this definition of $q(\rvx_{0:T}, \rvy)$ all conditional distributions given $\rvx_0$ are Gaussian as well. In particular, it will be helpful during training that, for any $t$, the following is Gaussian: 
\begin{equation} \label{eq:q-step-iddpm}
    q(\rvx_t | \rvx_0) = \gN(\rvx_t ; \sqrt{\bar{\alpha}_t}  \rvx_{t-1} , \bar{\beta}_t \rmI )
\end{equation}
where $\bar{\alpha}_t := \prod_{i=1}^t \alpha_i$ and $\bar{\beta}_t = 1 - \bar{\alpha}_t$.

\subsection{Learnable Prior and Decoder}
Given this simple and non-learnable encoder, the complexity of a diffusion model lies in ``inverting it'' by mapping from noise $\rvx_T$ to data $\rvx_0$. We do so by first sampling $\rvx_{T-1}$ given $\rvx_T$, then $\rvx_{T-2}$ given $\rvx_{T-1}$ and so on until we have sampled $\rvx_0$. Formally, we factorize the ``generative'' distribution as
\begin{equation}
    p_\theta(\rvx_{0:T}) = p(\rvx_T) \prod_{t=1}^T p_\theta(\rvx_{t-1}|\rvx_t).
\end{equation}
In this factorisation, $p(\rvx_T)$ has no dependence on $\theta$ because it is typically set to be a zero-mean Gaussian~\citep{ho2020denoising}. All of the parameters $\theta$ are therefore dedicated to parameterising the transition distributions $p_\theta(\rvx_{i-t}|\rvx_t)$ for each $t$. Note that, even though we get from $\rvx_{i-t}$ to $\rvx_t$ by just adding Gaussian noise, knowing how to ``remove'' this Gaussian noise is complex and requires knowledge of the data distribution.

% \citet{ho2020denoising} proposed to parameterise $p_\theta(\rvx_{t-1}|\rvx_t)$ via a deterministic prediction of $\rvx_0$ from $\rvx_t$ and the known Gaussian distribution $q(\rvx_{t-1}|\rvx_t, \rvx_0)$. Noting that we can use \cref{eq:q-joint} to derive
% \begin{equation}
%     q(\rvx_{t-1}|\rvx_t, \rvx_0) = \gN ( \rvx_{t-1} ; \tilde{\mathbf{\mu}}_t(\rvx_t,\rvx_0), \tilde{\beta}_t \mI )
% \end{equation}
% with $\tilde{\mathbf{\mu}}_t(\rvx_t,\rvx_0) := \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}\rvx_0 + \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}\rvx_t$ and $\tilde{\beta}_t = \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t} \beta_t$, we parameterise $p_\theta(\rvx_{t-1}|\rvx_t)$ as
% \begin{equation}
%     p_\theta(\rvx_{t-1}|\rvx_t) = \gN ( \rvx_{t-1} ; \tilde{\mathbf{\mu}}_t(\rvx_t,\tilde{\rvx}_\theta(\rvx_t, t)), \tilde{\beta}_t \mI )
% \end{equation}
% where $\tilde{\rvx}_\theta(\rvx_t, t)$ is a neural network's prediction of $\rvx_0$ given $\rvx_t$. 

\citet{ho2020denoising} proposed to parameterise $p_\theta(\rvx_{t-1}|\rvx_t)$ as a Gaussian with mean $\tilde{\mathbf{\mu}}_\theta(\rvx_t, t)$ output by a neural network and variance $\beta_t$ matching that of $q(\rvx_t|\rvx_{t-1})$.

The parameters $\theta$ can be trained simply by optimising the ELBO averaged over training data $\rvx \sim \pdata(rvx)$:
\begin{align}
    \gL(\theta, \phi, \rvx_0, \rvy) =& \EX_{q(\rvx_{1:T}|\rvx_0)} \left[ \log \frac{p_\theta(\rvx_{0:T}|\rvy)}{q(\rvx_{1:T}|\rvx_0)} \right].
\end{align}
To derive a simple estimator for this loss we follow \citet{ho2020denoising} by breaking it down as:
\begin{align}
    \gL(\theta, \phi, \rvx_0, \rvy) =& \EX_{q(\rvx_{1:T}|\rvx_0)} \left[ \log \frac{p(\rvx_T) \prod_{t=1}^T p_\theta(\rvx_{t-1}|\rvx_t, \rvy)}{\prod_{t=1}^T q(\rvx_t|\rvx_{t-1})} \right] \\
    =& \EX_{q(\rvx_{1:T}|\rvx_0)} \left[ \log \frac{p(\rvx_T, \rvy) \prod_{t=1}^T p_\theta(\rvx_{t-1}|\rvx_t, \rvy)}{q(\rvx_T|\rvx_0) \prod_{t=2}^T q(\rvx_{t-1}|\rvx_t,\rvx_0)} \right] \\
    =& \EX_{q(\rvx_{1:T}|\rvx_0)} \left[ \log \frac{p(\rvx_T)}{q(\rvx_T|\rvx_0)} + \prod_{t=2}^T \frac{p_\theta(\rvx_{t-1}|\rvx_t, \rvy)}{q(\rvx_{t-1}|\rvx_t,\rvx_0)} + \log p_\theta(\rvx_0|\rvx_1, \rvy) \right] \\
    =& \EX_{q(\rvx_{1:T}|\rvx_0)} \Big[ -\kl{q(\rvx_T|\rvx_0)}{p(\rvx_T)} \nonumber \\
    &\qquad\qquad \left. - \prod_{t=2}^T \kl{q(\rvx_{t-1}|\rvx_t,\rvx_0)}{p_\theta(\rvx_{t-1}|\rvx_t, \rvy)} \right. \nonumber \\
    &\qquad\qquad + \log p_\theta(\rvx_0|\rvx_1) \Big].
\end{align}
When learning $\theta$, the first term, the KL divergence between $q(\rvx_T|\rvx_0)$ and $p_\theta(\rvx_T)$ can be disregarded since it is not dependent on $\theta$. 
The second term can be simplified by noting that, due to the properties of the forward process, $q(\rvx_{t-1}|\rvx_t,\rvx_0)$ is the analytically tractable Gaussian
\begin{equation}
    q(\rvx_{t-1}|\rvx_t, \rvx_0) = \gN ( \rvx_{t-1} ; \tilde{\mathbf{\mu}}_t(\rvx_t,\rvx_0), \tilde{\beta}_t \mI )
\end{equation}
with mean $\tilde{\mathbf{\mu}}_t(\rvx_t,\rvx_0) := \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}\rvx_0 + \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}\rvx_t$ and variance $\tilde{\beta}_t = \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t} \beta_t$. The KL divergence between two Gaussians has an analytic form which we can then exploit. Recalling that $p_\theta(\rvx_{t-1}|\rvx_t, \rvy)$ has learned mean $\tilde{\mathbf{\mu}}_\theta(\rvx_t, t)$ and variance $\beta_t$, and using $d$ to denote the dimensionality of the data, we can write each KL divergence as
\begin{align}
    &\kl{q(\rvx_{t-1}|\rvx_t,\rvx_0)}{p_\theta(\rvx_{t-1}|\rvx_t, \rvy)} \\
    =& \frac{1}{2 \beta_t} ||\tilde{\mathbf{\mu}}_t(\rvx_t,\rvx_0) - \tilde{\mathbf{\mu}}_\theta(\rvx_t, t)||_2^2 + \frac{d}{2} \left( \frac{\tilde{\beta}_t}{\beta_t} + \log\frac{\beta_t}{\tilde{\beta}_t} - 1 \right) \\
    =& \frac{1}{2 \beta_t} ||\tilde{\mathbf{\mu}}_t(\rvx_t,\rvx_0) - \tilde{\mathbf{\mu}}_\theta(\rvx_t, t)||_2^2 + \text{constant}.
\end{align}
where ``constant'' groups together terms with no dependence on $\theta$. The ``likelihood'' term, $\log p_\theta(\rvx_0|\rvx_1)$, is the log-probability of a Gaussian:
\begin{align}
    \log p_\theta(\rvx_0|\rvx_1) &= -\frac{1}{2\beta_1} || \rvx_0 - \tilde{\mathbf{\mu}}_\theta(\rvx_1, 1) ||_2^2 - \frac{d}{2}\left( \log(2\pi) + \log \beta_t \right) \\
    &= -\frac{1}{2\beta_1} || \rvx_0 - \tilde{\mathbf{\mu}}_\theta(\rvx_1, 1) ||_2^2  + \text{constant}.
\end{align}
We can therefore simplify the loss as a whole to:
\begin{align}
    \gL(\theta, \phi, \rvx_0, \rvy) =& \EX_{q(\rvx_{1:T}|\rvx_0)} \left[ \sum_{t=1}^T \frac{1}{2\beta_t} || \tilde{\mathbf{\mu}}_t(\rvx_t,\rvx_0) - \tilde{\mathbf{\mu}}_\theta(\rvx_t, t) ||_2^2 \right] + \text{constant}.
\end{align}
noting that $\tilde{\mathbf{\mu}}_1(\rvx_1,\rvx_0)$ is equal to $\rvx_0$.
This is a simple sum of squared-error losses. We can further rewrite it as an expectation which we can take an unbiased Monte-Carlo estimate of at training time with a single evaluation of the neural network $\mathbf{\mu}_\theta$:
\begin{align} \label{eq:diffusion-mse-loss-mu}
    \gL(\theta, \phi, \rvx_0, \rvy) =& \EX_{q(\rvx_t|\rvx_0,t)u(t)} \left[ \frac{T}{2 \beta_t u(t)} || \tilde{\mathbf{\mu}}_t(\rvx_t,\rvx_0) - \tilde{\mathbf{\mu}}_\theta(\rvx_t, t) ||_2^2 \right] + \text{constant}.
\end{align}
Here, $u(t)$ is a proposal distribution over timesteps, often set to be uniform. Recall that $q(\rvx_t|\rvx_0,t)$ is analytic and Gaussian so sampling $\rvx_t$ on each step for a single $t$ is feasible.

Training a neural network to directly map from $\rvx_t$ and $t$ to $\tilde{\mathbf{\mu}}_t(\rvx_t,\rvx_0)$ has been found to work poorly in the literature~\cite{ho2020denoising}. One reason for this is that $\tilde{\mathbf{\mu}}_t(\rvx_t,\rvx_0)$ is typically very close to $\rvx_t$ and perturbing it only slightly, while keeping $\rvx_t$ fixed, leads to a large fluctuation in the implied value of $\rvx_0$. Neural networks are typically designed to produce outputs with roughly zero mean and unit variance and so may not be able to produce accurate values within this range~\citet{karras2022elucidating}.

Common alternatives involve training the neural network to predict other affine transformations of $\rvx_0$ and $\rvx_t$ such as $\epsilon := f_t^\epsilon(\rvx_t, \rvx_0) = \frac{1}{\sqrt{1-\bar{\alpha}_t}} \rvx_t - \frac{\sqrt{\bar{\alpha}_t}}{\sqrt{1-\bar{\alpha}_t}} \rvx_0$~\citep{ho2020denoising} or the clean data $\rvx_0 = f_t^\rvx(\rvx_t, \rvx_0) := \rvx_0$ itself. We can derive a loss equivalent to \cref{eq:diffusion-mse-loss-mu} with the mean-squared error loss taken between outputs of any of these transformations. To do so generally, we describe each transformation as $f_t(\rvx_t, \rvx_0) := a_t^ \cdot \rvx_t + b_t \cdot \rvx_0$ and, recalling the definition $\tilde{\mathbf{\mu}}_t(\rvx_t,\rvx_0) := \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}\rvx_t + \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}\rvx_0$, define $a_t^\mu := \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}$ and $b_t^\mu := \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}$. Now we ask, given access to $\rvx_t$ and a prediction of the value of some general $f_t(\rvx_t, \rvx_0) := a \cdot \rvx_t + b \cdot \rvx_0$, how do we infer the implied value of $\tilde{\mathbf{\mu}}_t(\rvx_t,\rvx_0)$ required in \cref{eq:diffusion-mse-loss-mu}?
\begin{align}
    \tilde{\mathbf{\mu}}_t(\rvx_t,\rvx_0) &= a_t^\mu \cdot \rvx_t + b_t^\mu \cdot \rvx_0 \\
    &= a_t^\mu \cdot \rvx_t + b_t^\mu \left( \frac{f_t(\rvx_t, \rvx_0) - a \cdot \rvx_t}{b} \right) \\
    &= \left( a_t^\mu - a_t \frac{b_t^\mu}{b_t} \right) \rvx_t + \frac{b_t^\mu}{b_t} f_t(\rvx_t, \rvx_0).
\end{align}
Substituting this into \cref{eq:diffusion-mse-loss-mu} yields
\begin{align}
    \gL(\theta, \phi, \rvx_0, \rvy) =& \EX_{q(\rvx_t|\rvx_0,t)u(t)} \left[ \frac{T}{2 \beta_t u(t)} || \left( a_t^\mu - a_t \frac{b_t^\mu}{b_t} \right) \rvx_t + \frac{b_t^\mu}{b_t} f_t(\rvx_t, \rvx_0) - \tilde{\mathbf{\mu}}_\theta(\rvx_t, t) ||_2^2 \right] + \text{constant}.
\end{align}
If we reparameterize $\tilde{\mathbf{\mu}}_\theta(\rvx_t, t)$ via a learned prediction $\tilde{\mathbf{f}}_\theta(\rvx_t, t)$, so that $\tilde{\mathbf{\mu}}_\theta(\rvx_t, t) = \left( a_t^\mu - a_t \frac{b_t^\mu}{b_t} \right) \rvx_t + \frac{b_t^\mu}{b_t} \tilde{\mathbf{x}}_\theta(\rvx_t, t)$ then this simplifies to
\begin{align}
    \gL(\theta, \phi, \rvx_0, \rvy) =& \EX_{q(\rvx_t|\rvx_0,t)u(t)} \left[ \frac{T}{2 \beta_t u(t)} || \left( a_t^\mu - a_t \frac{b_t^\mu}{b_t} \right) \rvx_t + \frac{b_t^\mu}{b_t} f_t(\rvx_t, \rvx_0) - \left( a_t^\mu - a_t \frac{b_t^\mu}{b_t} \right) \rvx_t - \frac{b_t^\mu}{b_t} \tilde{\mathbf{f}}_\theta(\rvx_t, t) ||_2^2 \right] + \text{constant}. \\
    &= \EX_{q(\rvx_t|\rvx_0,t)u(t)} \left[ \frac{T}{2 \beta_t u(t)} || \frac{b_t^\mu}{b_t} f_t(\rvx_t, \rvx_0) - \frac{b_t^\mu}{b_t} \tilde{\mathbf{f}}_\theta(\rvx_t, t) ||_2^2 \right] + \text{constant}. \\
    &= \EX_{q(\rvx_t|\rvx_0,t)u(t)} \left[ \frac{T {b_t^\mu}^2 }{2 \beta_t u(t) b_t^2 } || f_t(\rvx_t, \rvx_0) - \tilde{\mathbf{f}}_\theta(\rvx_t, t) ||_2^2 \right] + \text{constant}. \\
    &= \EX_{q(\rvx_t|\rvx_0,t)u(t)} \left[ \frac{T \bar{\alpha}_{t-1} \beta_t }{2 u(t) b_t^2 (1-\bar{\alpha}_t)^2 } || f_t(\rvx_t, \rvx_0) - \tilde{\mathbf{f}}_\theta(\rvx_t, t) ||_2^2 \right] + \text{constant}.
\end{align}
For the case of $\epsilon$ prediction, when we use $f_t^\epsilon(\rvx_t, \rvx_0) = \frac{1}{\sqrt{1-\bar{\alpha}_t}} \rvx_t - \frac{\sqrt{\bar{\alpha}_t}}{\sqrt{1-\bar{\alpha}_t}} \rvx_0$, this yields the loss
\begin{equation}
    \gL(\theta, \phi, \rvx_0, \rvy) = \EX_{q(\rvx_t|\rvx_0,t)u(t)} \left[ \frac{T \bar{\alpha}_{t-1} \beta_t }{2 u(t) (1-\bar{\alpha}_t) } || f_t^\epsilon(\rvx_t, \rvx_0) - \tilde{\mathbf{\epsilon}}_\theta(\rvx_t, t) ||_2^2 \right] + \text{constant}.
\end{equation}


\section{Continuous time diffusion}

\newpage
\newpage

Diffusion models, or DMs~\citep{sohl2015deep,ho2020denoising,nichol2021improved,song2020score}, are another class of generative model. We will describe the conditional extension~\citep{tashiro2021csdi}, in which the modeled $\rvx$ is conditioned on observations $\rvy$. DMs simulate a diffusion process which transforms $\rvx$ to noise, and generate data by learning the probabilistic inverse of the diffusion process. The diffusion process happens over timesteps $0,\ldots,T$ such that $\rvx_0=\rvx$ is data without noise, $\rvx_1$ has a very small amount of noise added, and so on until $\rvx_T$ is almost independent of $\rvx_0$ and approximates a random sample from a unit Gaussian. In the diffusion process we consider, the distribution over $\rvx_t$ depends only on $\rvx_{t-1}$:
\begin{equation} \label{eq:q-step}
    q(\rvx_t | \rvx_{t-1}) = \gN(\rvx_t ; \sqrt{\alpha_t}  \rvx_{t-1} , (1-\alpha_t) \rmI ).
\end{equation}
Hyperparameters $\alpha_1,\ldots,\alpha_T$ are chosen to all be close to but slightly less than $1$ so that the amount of noise added at each step is small.
The combination of this diffusion process and a data distribution $q(\rvx_0,\rvy)$ (recalling that $\rvx_0=\rvx$) defines the joint distribution
\begin{equation} \label{eq:q-joint}
    q(\rvx_{0:T}, \rvy) = q(\rvx_0, \rvy) \prod_{t=1}^T q(\rvx_t|\rvx_{t-1}).
\end{equation}
Recall that $q(\rvx_0, \rvy)$ is simply new notation for the data distribution $\pdata(\rvx,\rvy)$ and the encoder distribution, this equation is analogous to the product of the data distribution and the encoder's distribution
$\prod_{l=1}^L \pmodel(\rvz|\rvz_{<l})$ in a hierarchical VAE from \cref{sec:hierarchical-vae}. This leads to one perspective on diffusion models as simply a hierarchical VAE in which the encoder has a simple and non-learnable structure.

DMs work by ``inverting'' the diffusion process: given values of $\rvx_t$ and $\rvy$ a neural network is used to parameterize $p_\theta(\rvx_{t-1}|\rvx_t, \rvy)$, an approximation of $q(\rvx_{t-1}|\rvx_t,\rvy)$. This neural network lets us draw samples of $\rvx_0$ by first sampling $\rvx_T$ from a unit Gaussian (recall that the diffusion process was chosen so that $q(\rvx_T)$ is well approximated by a unit Gaussian), and then iteratively sampling $\rvx_{t-1} \sim p_\theta(\cdot|\rvx_t,\rvy)$ for $t=T,T-1,\ldots,1$. The joint distribution of sampled $\rvx_{0:T}$ given $\rvy$ is
\begin{equation} \label{eq:p-joint}
    p_\theta(\rvx_{0:T}|\rvy) = p(\rvx_{T}) \prod_{t=1}^T p_\theta(\rvx_{t-1}|\rvx_t, \rvy)
\end{equation}
where $p(\rvx_{T})$ is a unit Gaussian that does not depend on $\theta$. Training the conditional DM therefore involves fitting $p_\theta(\rvx_{t-1}|\rvx_t,\rvy)$ to approximate $q(\rvx_{t-1}|\rvx_t, \rvy)$ for all choices of $t$, $\rvx_t$, and $\rvy$.

Several observations have been made in recent years which simplify the learning of $p_\theta(\rvx_{t-1}|\rvx_t,\rvy)$. \citet{sohl2015deep} showed that when $\alpha_t$ is close to 1, $p_\theta(\rvx_{t-1}|\rvx_t)$ is approximately Gaussian~\cite{sohl2015deep}. Furthermore, \citet{ho2020denoising} showed that this Gaussian's variance can be modeled well with a non-learned function of $t$, and that a good estimate of the Gaussian's mean can be obtained from a ``denoising model'' as follows. Given data $\rvx_0$ and unit Gaussian noise $\epsilon$, the denoising model (in the form of a neural network) is fed ``noisy'' data $\rvx_t := \sqrt{\tilde{\alpha}_t} \rvx_0 + \sqrt{1 - \tilde{\alpha}_t} \epsilon$ and trained to recover $\epsilon$ via a mean squared error loss. The parameters $\tilde{\alpha}_t := \prod_{i=1}^t \alpha_i$ are chosen to ensure that the marginal distribution of $\rvx_t$ given $\rvx_0$ is $q(\rvx_t|\rvx_0)$ as derived from \cref{eq:q-step}. Given a weighting function $\lambda(t)$, the denoising loss is
\begin{equation} \label{eq:dm-loss}
    \mathcal{L}(\theta) = \E_{q(\rvx_0, \rvy, \epsilon)} \left[ \sum_{t=1}^T \lambda(t) \lVert \epsilon - \epsilon_\theta(\rvx_t, \rvy, t) \rVert_2^2 \right] \quad \text{with} \quad \rvx_t = \sqrt{\tilde{\alpha}_t} \rvx_0 + \sqrt{1 - \tilde{\alpha}_t} \epsilon.
\end{equation}
The mean of $p_\theta(\rvx_{t-1}|\rvx_t,\rvy)$ is obtained from the denoising model's output $\epsilon_\theta(\rvx_t, \rvy, t)$ as ${\frac{1}{\alpha_t}\rvx_t - \frac{1-\alpha_t}{\sqrt{1-\tilde{\alpha}_t}}\epsilon_\theta(\rvx_t, \rvy, t)}$.
If the weighting function $\lambda(t)$ is chosen appropriately, optimising \cref{eq:dm-loss} is equivalent to optimising a lower-bound on the data likelihood under $p_\theta$. In practice, simply setting $\lambda(t) := 1$ for all $t$ can produce more visually compelling results in the image domain~\citep{ho2020denoising}.

% In our proposed method, as in \citet{tashiro2021csdi}, the shapes of $\rvx_0$ and $\rvy$ sampled from $q(\cdot)$ vary. This is because we want to train a model which can flexibly adapt to e.g. varying numbers of observed frames. To map \cref{eq:dm-loss} to this scenario, note that both $\rvx_0$ and $\rvy$ implicitly contain information about which frames in the video they represent (via the index vectors $\mathcal{X}$ and $\mathcal{Y}$ introduced in the previous section). This information is used inside the neural network $\epsilon_\theta(\rvx_t,\rvy, t)$ so that interactions between frames can be conditioned on the distance between them (as described in the following section) and also to ensure that the sampled noise vector $\epsilon$ has the same shape as $\rvx_0$.

% \subsection{Continuous-time diffusion}

