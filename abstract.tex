%% The following is a directive for TeXShop to indicate the main file
%%!TEX root = diss.tex

\chapter{Abstract}

Recent advances in the field of deep generative modelling are leading to increasingly faithful models of real-world data including images and videos. Of particular practical interest are conditional generative models, which parameterise conditional probability distributions given data features. Flexibly-conditional generative models are more flexible than conventional conditional models in the sense that they allow \textit{any} data features to be conditioned on. This makes them applicable to tasks like image inpainting where we want the same model that can, e.g., inpaint the top half of an image, to also be capable of, e.g., inpainting the bottom half. Flexible conditioning has previously been demonstrated for data types including fixed-size images and short videos, but our thesis is that it can be enabled in a much broader variety of settings. The first setting we will consider is long-video generation, which is normally problematic because the data is high-dimensional and compute constraints can prevent our model from conditioning on all possible frames. The second is where the data dimensionality (e.g. number of frames in a video) is stochastic and can depend on what we condition on. We present techniques to enable flexible conditioning in both of these settings. We further show that the resulting models can sometimes improve on baselines in terms of sample quality even for conventional generation tasks. Another barrier to flexibly-conditional modelling has been the computational cost of training any high-quality generative models on moderate- or high-resolution visual data. We therefore end by presenting the first technique to mitigate this cost for the training of flexibly-conditional variational auto-encoders by incorporating pretrained unconditional model weights.


% In the past, arbitrarily-conditional models have had some limitations. For example, most arbitrarily-conditional generative models are not applicable if the data is too complex or high-dimensional. Similarly, they would not apply if the data has varying dimensionality. Our work can be viewed as making arbitrarily-conditional models more flexible by making them applicable to more complex and higher-dimensional data types such as long videos. It can alternatively be viewed as making 

% Recent improvements to deep generative models are leading to increasingly faithful models of real-world data including images and videos. Of particular practical interest are conditional generative models, which parameterise a conditional probability distribution. This means that they can take an input, such as the first frame of a video, and be used to sample possible data, such as the rest of the video frames, conditioned on the input. They have many use-cases but are expensive to train and must be re-trained for each new task of interest. This is an issue if, for example, we had the aforementioned model that conditions on a first frame but were then told that we needed to sample videos conditioned on the \textit{final} frame. We posit that many modern generative models can be altered to avoid this limitation. To show this, we propose methods for training them that are compatible with many different tasks at test-time without retraining. We call such models \textit{flexible} generative models. We can use the same flexible model to sample a video conditioned on any of the first frame, the last frame, both of these, or any other set of frames. We show that flexible generative models can be designed to have performance similar to, or sometimes better than, conventional conditional generative models. We will demonstrate that, in particular, diffusion models are often naturally amenable to being used for flexible conditioning in this way before presenting innovations to enable flexible conditioning in cases where it is not straightforward: when operating under memory constraints with complex data; and when the data has varying and unknown dimensionality that may depend on what we condition on. We conclude by showing that this amenability to flexible conditioning is not unique to diffusion models via a demonstration of a flexible variational auto-encoder. On tasks including image inpainting and video generation, the models we present demonstrate qualitatively and quantitatively improved generation quality versus baselines as well as enabling controllability through flexible conditioning.
 