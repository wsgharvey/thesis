%% The following is a directive for TeXShop to indicate the main file
%%!TEX root = diss.tex

\chapter{Abstract}


% \paragraph{Option 1}
% A fascinating and overarching challenge for humankind is to learn deep generative models of video and other things. Thousands have tried and failed to do so. But no more. The journey is over. This thesis solves it.

% \paragraph{Option 2}
% We are scared. Seriously scared. Generative AI is coming to eat us. To appease it, we assist it. We build models for it so that it will like us. If we are lucky, it will find them useful for applications like video generation and planning.

% \paragraph{Option 3}


A deep generative model (DGM) uses a neural network to parameterize a probability distribution. In this thesis, the distributions of interest are over images and video. Deep generative models are often trained to work well for a single generation task. For the video domain these may be e.g. unconditional video generation; video generation conditioned on the first few frames (\ie video continuation); or video generation conditioned on every $n$th frame (\ie temporal super-resolution). We posit that modern DGMs can achieve spectacular performance without being restricted to a single task and aim to develop generative models that can be \textit{flexibly conditioned}. That is, we want to train a single neural network which will allow us to, at test-time, select any of these tasks, or think of a new task, and approximate the appropriate conditional distribution to solve it.

We take a first step in this direction within the hierarchical variational auto-encoder (HVAE) framework. Training an HVAE from scratch is costly and so, rather than training a flexibly conditional HVAE scratch, we present a method to adapt pretrained unconditional HVAEs for conditional generation. Our method of adaptation is fast and cheap, yet produces an artifact that outperforms all previous inpainting methods in terms of image realism and diversity. The possibility of fast adaptation hints at the similarity and transferability between the representations required for different kinds of conditional generation.

% We take a first step in this direction within the hierarchical variational auto-encoder (VAE) framework. We show that, given a VAE trained for unconditional image generation, we can cheaply adapt it to produce an image-conditional VAE suitable for tasks like image inpainting. The training cost of this adaptation, while non-zero, is very low and the resulting model improves over all previous inpainting methods in terms of image realism and diversity. \todo{Put this somewhere else. We do so \textit{without} even finetuning the parameters of its decoder. This somehow makes it related to a diffusion model.}

We then move to the diffusion model (DM) framework, which allows for faster training than with HVAEs. We demonstrate the suitability of DMs to video generation, including when tasks are flexibly chosen at test-time. A complication for generation of sufficiently long videos, however, is that, given finite memory constraints, it is infeasible to process all frames together. To get around this, we train a DM which is flexible in a second manner: it can ``drop out'' an arbitrary set of video frames so that they are neither generated nor conditioned on. The full task for our generative model is then to generate one arbitrary subset of video frames, conditioned on any other arbitrary (but disjoint) subset of video frames, with remaining video frames dropped from consideration. By repeatedly using this model to sample different subsets of video frames, we can eventually sample as many frames as desired for a video. Through this technique, we can choose which order to generate frames in, dependent on the test-time task. For example, if we are conditioning only on the first few frames, we may generate frames in order from the start of the video to the end of the video. If we also condition on the last frame, it may instead be helpful to begin by generating ``waypoint'' frames and then fill in the gaps between them. Our proposed flexible model lets us compare such different options without retraining and, by doing so, generate videos with better long-term coherence than our baselines.

We finally make one further extension to the DM framework to enhance their applicability to conditional generation. So far we can flexibly sample videos conditioned on frames at any known indices, but cannot make use of observed frames at unknown indices. For example, consider that we are given two frames and want to ``fill in'' the frames between them but do not know how far apart the two frames are. We suggest to do so by making the number of frames itself a random variable which is sampled by a DM. This means that, at test-time, we can condition on frame values at unknown indices, and their values will naturally affect the number of frames in the sampled data.

% Finally, we note that the order in which things are generated can have significant importance.


% \paragraph{From thesis proposal}
% A deep generative model (DGM) uses a neural network to parameterize a probability distribution. This thesis focuses on distributions over high-dimensional objects like images and video. In a \textit{conditional} DGM, the distribution from which samples are drawn depends on information ``conditioned on'' at test-time. Traditionally a conditional DGM is trained to condition on different values of the same feature. We further define \textit{flexibly conditional} DGMs, where we vary \textit{which} features are conditioned on, as well as the values of those features. We posit that modern deep generative models work well even when we use a very broad distribution over which features to condition on.

% We demonstrate this first by presenting a method for cheaply converting a pretrained unconditional DGM in the form of a hierarchical variational auto-encoder into a flexibly conditional DGM which can produce an image conditioned on any given subset of image pixels. This makes it suited to image inpainting, and it improves over previous methods in terms of image realism and diversity.

% We next consider flexibly conditional DGMs of long videos. For sufficiently long videos, it is infeasible to model all frames jointly given finite memory constraints. We therefore make one further methodological contribution: we train the model to work even when some video frames are dropped from consideration. Our model therefore serves as a generative model of an arbitrary subset of video frames, conditioned on another arbitrary (but disjoint) subset of video frames, with any remaining video frames dropped from consideration. This model can be applied repeatedly over different subsets of video frames if we desire to sample values for all frames. We show that, when doing so, the order in which frames are generated has a large and dataset-dependent effect on the coherence of the generated video. Building a flexibly conditional DGM as we describe allows the same model to be used with different generation orders and so lets us explore different options and eventually generate videos with greater long-term coherence than our baselines. This could have applications in tools for video creation/editing as well as for planning from visual data in domains like autonomous driving.



% Consider placing version information if you circulate multiple drafts
%\vfill
%\begin{center}
%\begin{sf}
%\fbox{Revision: \today}
%\end{sf}
%\end{center}
