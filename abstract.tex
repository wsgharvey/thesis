%% The following is a directive for TeXShop to indicate the main file
%%!TEX root = diss.tex

\chapter{Abstract}

Recent advances in the field of deep generative modelling are leading to increasingly faithful models of real-world data including images and videos. Of particular practical interest are conditional generative models, which parameterise conditional probability distributions given data features.
%
Arbitrarily-conditional generative models are more flexible than conventional conditional models in the sense that they allow \textit{any} data features to be conditioned on. This makes them applicable tasks like image inpainting where we want the same model that can, e.g., inpaint the top half of an image, to also be capable of e.g. inpainting the bottom half.
%
In this dissertation we demonstrate generalisations of arbitrarily-conditional models that are applicable where conventional arbitrarily-conditional models break down. Two main focuses are: \textbf{(1)} domains like long-video generation where the data is high-dimensional and the model is run under memory constraints that prevent it from conditioning on all possible frames; and \textbf{(2)} settings where the data dimensionality (e.g. number of frames in a video) is stochastic and can depend on what we condition on. 
%
% We present techniques to enable analogues of arbitrarily-conditional modelling in both of these settings. 
%
The models we present in these settings enable previously-unseen conditional generation tasks including ``planning'' by generating 1000-frame videos given the first few frames and the final frame. In addition, they sometimes improve on baselines in terms of sample quality even for more conventional tasks.
%
In the past, another barrier to arbitrarily-conditional modelling has been the computational cost of training any high-quality generative models on moderate- or high-resolution visual data. We therefore end by presenting the first technique to mitigate this cost for the training of arbitrarily-conditional variational auto-encoders by making use of pretrained unconditional model weights.


% In the past, arbitrarily-conditional models have had some limitations. For example, most arbitrarily-conditional generative models are not applicable if the data is too complex or high-dimensional. Similarly, they would not apply if the data has varying dimensionality. Our work can be viewed as making arbitrarily-conditional models more flexible by making them applicable to more complex and higher-dimensional data types such as long videos. It can alternatively be viewed as making 

% Recent improvements to deep generative models are leading to increasingly faithful models of real-world data including images and videos. Of particular practical interest are conditional generative models, which parameterise a conditional probability distribution. This means that they can take an input, such as the first frame of a video, and be used to sample possible data, such as the rest of the video frames, conditioned on the input. They have many use-cases but are expensive to train and must be re-trained for each new task of interest. This is an issue if, for example, we had the aforementioned model that conditions on a first frame but were then told that we needed to sample videos conditioned on the \textit{final} frame. We posit that many modern generative models can be altered to avoid this limitation. To show this, we propose methods for training them that are compatible with many different tasks at test-time without retraining. We call such models \textit{flexible} generative models. We can use the same flexible model to sample a video conditioned on any of the first frame, the last frame, both of these, or any other set of frames. We show that flexible generative models can be designed to have performance similar to, or sometimes better than, conventional conditional generative models. We will demonstrate that, in particular, diffusion models are often naturally amenable to being used for flexible conditioning in this way before presenting innovations to enable flexible conditioning in cases where it is not straightforward: when operating under memory constraints with complex data; and when the data has varying and unknown dimensionality that may depend on what we condition on. We conclude by showing that this amenability to flexible conditioning is not unique to diffusion models via a demonstration of a flexible variational auto-encoder. On tasks including image inpainting and video generation, the models we present demonstrate qualitatively and quantitatively improved generation quality versus baselines as well as enabling controllability through flexible conditioning.
 