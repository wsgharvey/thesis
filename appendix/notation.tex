\chapter{Notation}

\Cref{tab:diffusion-notation-appendix,tab:notation-appendix-conditional-diffusion,tab:notation-appendix-fdm,tab:notation-appendix-tddm,tab:notation-appendix-cigcvae} provide a concise reference for the notation used throughout this dissertation. Note that we exclude some symbols that are referenced in only a single section. More thorough explanations are provided where symbols are introduced in the text.

\begin{table*}
  \caption{Symbols defined in \cref{ch:diffusion}. 
  % Together with \cref{tab:diffusion-notation-appendix-more}, this covers all symbols referred to in more than one part of \cref{ch:diffusion}.
  }
  \label{tab:diffusion-notation-appendix}
  \centering
  \footnotesize
  \begin{tabular}{rp{9cm}}
    \toprule
    Symbol    & Definition   \\
    \midrule
    $\rvx$                                  & Data which we are learning to generate. \\ %wish to learn a generative model of. \\
    % $\rvy$                                  & Data on which the generative model should be conditioned. \\
    $\pdata(\rvx)$                          & Data distribution. \\
    $p_\theta(\rvx)$                        & A distribution over data parameterised by a deep generative model with parameters $\theta$. \\
    $t$                                     & Time in relation to the diffusion SDE and ODE.  \\
    $\rvx_t$                                & ``Noisy'' data at time $t \geq 0$ in the diffusion process.  \\
    $\rvw$, $\bar{\rvw}$                    & Standard and reverse-time Wiener processes. \\
    $g(t)$                                  & Scaling factor for Wiener process noise added in the forward SDE at time $t$. Defined to be positive for all $t > 0$. \\
    $\rvb(\rvx_t, t)$                       & Drift term in our forward diffusion SDE. Zero for variance-exploding diffusion processes (all processes introduced before \cref{sec:more-general-diffusion-processes}) and $-\frac{1}{2}g(t)^2 \rvx_t$ for variance-preserving processes. \\
    $\alpha(t)$                             & Factor by which signal $\rvx$ has been scaled by time $t$ during a diffusion process. One for variance-exploding processes; $\text{exp}(-\int_0^t g(s)^2 \mathrm{d}s)$ for variance-preserving processes. \\
    $\sigma(t)^2$                           & Variance of noise added by time $t$. Given by $\sigma(t)^2 = \int g(t)^2 \mathrm{d}t$ for variance-exploding processes; $1-\alpha(t)^2$ for variance-preserving processes. \\
    % $\sigma$                                & Equivalent to $\sigma(t)$. When it simplifies results, we will define the process explicitly in terms of $\sigma$ instead of using $t$. \\
    $\rvx_\sigma$                           & ``Noisy'' data, equivalent to $\rvx_t$ for $t$ satisfying $\sigma = \sigma(t)$. \\
    $\text{SNR}(\sigma)$                    & The signal-to-noise ratio. Given by $1/\sigma(t)^2$ in a variance-exploding process or $\alpha(t)^2 / \sigma(t)^2$ in a variance-preserving process. \\
    $q(\rvx,\rvx_{t_1},\ldots,\rvx_{t_n})$   & Joint distribution defined by the data distribution and forward SDE for any $t_1,\ldots,t_n \geq 0$. We will also use $q$ to denote any conditional or marginal of such a distribution.  \\
    $q(\rvx,\rvx_{\sigma_1},\ldots,\rvx_{\sigma_n})$   & Equivalent to $q(\rvx,\rvx_{t_1},\ldots,\rvx_{t_n})$ if each $\sigma_i = \sigma(t_i)$. This distribution is agnostic to $g(t)$. \\
%     % $\Sigma$ ?
%     % p_\theta from DDPM ?
%     % Losses?
%     \bottomrule
%   \end{tabular}
% \end{table*}
% \begin{table*}
%   \caption{Symbols defined in \cref{ch:diffusion} building on \cref{tab:diffusion-notation-appendix}. }
%   \label{tab:diffusion-notation-appendix-more}
%   \centering
%   \begin{tabular}{rp{8.5cm}}
%     \toprule
%     Symbol    & Definition   \\
%     \midrule
    $\epsilon$                               & Defined as $\frac{\rvx_t-\alpha(t)\rvx}{\sigma(t)}$. \\
    $\preds_\theta(\rvx_t, t)$  & Diffusion model's prediction of the score $\nabla_{\rvx_t} \log q(\rvx_t)$. \\
    $\predx_\theta(\rvx_t, t)$  & Diffusion model's prediction of clean data $\rvx$ given $\rvx_t$. \\
    $\prede_\theta(\rvx_t, t)$                      & Diffusion model's prediction of $\epsilon$. \\
    $\lambda^\rvs, \lambda^\rvx, \lambda^\epsilon$  & Per-timestep weighting factors for diffusion training objective when computed with mean-squared error loss to score function, $\rvx$, or $\epsilon$ respectively. \\
    $\beta(t)$                               & Hyperparameter controlling amount of stochasticity injected in the reverse SDE described in \cref{sec:other-sdes}. \\
    $u(t)$ or $u(\sigma)$                    & Distribution from which $t$ (or $\sigma$) is sampled during training. \\ 
    $\Sigma[0],\ldots,\Sigma[N]$             & An increasing sequence of standard deviations describing where the diffusion SDE is discretised when discussing training or sampling in discrete-time. \\
    $p_\theta(\rvx_{t-1} | \rvx_t)$          & The transition distribution used in DDPM sampling, defined in \cref{eq:ddpm-transition}.  \\
    $\gL_\text{ISM}(\theta, \sigma)$         &  Implicit score-matching objective defined in \cref{eq:ism-loss} for a given $\sigma$.  \\
    $\gL_\text{DM}(\theta)$                  & Diffusion objective; weighted integral of $\gL_\text{ISM}(\theta, \sigma)$ over $\sigma$. \\
    % $\gL()$  &  \\
    \bottomrule
  \end{tabular}
\end{table*}


\begin{table*}
  \caption{Symbols defined in \cref{ch:conditional-diffusion,ch:flexible-diffusion}.}
  \label{tab:notation-appendix-conditional-diffusion}
  \centering
  \footnotesize
  \begin{tabular}{rp{9cm}}
    \toprule
    Symbol    & Definition   \\
    \midrule
    $\rvy$                                  & Data which we wish to condition on. \\
    $\pdata(\rvx, \rvy)$                    & Data which we wish to condition on. \\
    $q(\rvx,\rvy,\rvx_{t_1},\ldots)$        & Integration of $q(\rvx,\rvx_{t_1},\ldots)$ from above with conditioning information $\rvy$. Defined as $q(\rvx,\rvx_{t_1},\ldots)\pdata(\rvy|\rvx)$.   \\
    $q(\rvx,\rvy,\rvx_{\sigma_1},\ldots)$   & Integration of $q(\rvx,\rvx_{\sigma_1},\ldots)$ from above with conditioning information $\rvy$. Defined as $q(\rvx,\rvx_{\sigma_1},\ldots)\pdata(\rvy|\rvx)$.   \\
    $\preds(\rvx_t, \rvy, t)$                     & Conditional version of $\preds(\rvx_t, t)$. \\
    $\predx(\rvx_t, \rvy, t)$                     & Conditional version of $\predx(\rvx_t, t)$. \\
    $\prede(\rvx_t, \rvy, t)$                     & Conditional version of $\prede(\rvx_t, t)$. \\
    $\rva$  & Data which we wish to condition 2SDM on (when $\rvy$ is overloaded to mean an embedding vector). \\
    $p_\theta(\rvx|\rvy,\rva)$                    & Distribution parameterised by 2SDM's conditional image model. \\
    $p_\phi(\rvy|\rva)$                           & Distribution over $\rvy$ parameterised by 2SDM's auxiliary model. \\
    $\alpha(t)$ or $\alpha$                       & ''Strength'' with which we condition on $\rvy$ in reconstruction guidance, classifier guidance, or classifier-free guidance.  \\
    $\gY$                                         &   Set of indices of $n$ data components on which we wish to condition, $\{y_1, \ldots, y_n\}$. When used in conjunction with $\rvy$ we have that $\rvy = [\rvx[y_1], \ldots, \rvx[y_n]] = \rvx[\gY]$. \\
    $\gL_\text{CDM}(\theta)$                      & The conditional diffusion loss. \\
    $\gL_\text{FCDM}(\theta)$                      & The flexibly-conditional diffusion loss. \\
    \bottomrule
  \end{tabular}
\end{table*}


\begin{table*}
  \caption{Symbols defined in \cref{ch:fdm}.}
  \label{tab:notation-appendix-fdm}
  \centering
  \footnotesize
  \begin{tabular}{rp{9cm}}
    \toprule
    Symbol    & Definition   \\
    \midrule
    $\rvv$                                  & Full video, typically with more frames than can be diffused over jointly by a diffusion model under memory constraints. \\
    $\gX$                                  & Indices of frames we wish to generate. \\
    $\gY$                                  & Indices of frames we wish to condition on. \\
    $\rvx$                                 & Values for frames indexed by $\gX$. May be extracted from $\rvv$ as $\rvv[\gX]$. \\
    $\rvy$                                 & Values for frames indexed by $\gY$. May be extracted from $\rvv$ as $\rvv[\gY]$. \\
    $K$                                    & Upper bound on number of frames that the diffusion model can generate/condition on simultaneously. \\
    $N$                                    & Number of frames in a video. \\
    $S$                                    & Number of stages in a sampling scheme. \\
    $[ (\gX_s, \gY_s)_{s=1}^S ]$           & A sampling scheme consisting of a sequence of indices of frames to generate and condition on. \\
    $\gL_\text{FDM}(\theta)$                      & The flexible diffusion model loss, which trains the model to cope with varying $\gX$ and varying $\gY$. \\
    \bottomrule
  \end{tabular}
\end{table*}


\begin{table*}
  \caption{Symbols defined in \cref{ch:tddm}.}
  \label{tab:notation-appendix-tddm}
  \centering
  \footnotesize
  \begin{tabular}{rp{9cm}}
    \toprule
    Symbol    & Definition   \\
    \midrule
    $d$                                     & Dimensionality of one component (e.g. number of dimensions per video frame). \\
    $N$                                     & Maximum number of components in any data point. \\
    $\rvx$                                  & Data values as in previous chapters. \\
    $n$                                     & Number of components in a data point. \\
    $\mX$                                   & Defined as $(n, \rvx)$, a representation of both the data values and dimensionality. \\
    $\pdata(\mX)$                                   & The (potentially trans-dimensional) data distribution.  \\
    $\rvx_t$                                & Data values at time $t$. May now be lower-dimensional than $\rvx$. \\
    $n_t$                                   & Number of components in $\rvx_t$. \\
    $\mX_t$                                 & Defined as $(n_t, \rvx_t)$. \\
    $q_{0,t_1,\ldots,t_n}(\mX,\mX_{t_1},\ldots)$   & Joint distribution over both data values and dimensionality at various times, defined by combination of data distribution and forward process.  \\
    $q_{0,t_1,\ldots,t_n}(\rvx,n,\rvx_{t_1},n_{t_1},\ldots)$    & Joint distribution over both data values and dimensionality at various times, defined by combination of data distribution and forward process.  \\
    $K_t(\mV | \mX_t)$  & Transition kernel is stochastically applied during the a jump diffusion process. \\
    $ \lambda(\mX_t, t) $                    &  Rate at which jumps occur during a jump diffusion process.  \\
    $\ftk_t( \mV |\mX_t)$                    & Transition kernel for the forward process. \\
    $\forwardrate(\mX_t, t)$                 &  Rate at which jumps occur during the forward process.  \\
    $\delidxdist(i | n_t)$                 & Distribution over the index of the component to delete in a jump in the forward process. Used to parameterise $\ftk_t(\mV|\mX_t)$. \\
    $\text{del}(\mX_t, i)$                 & Operator which deletes component $i$ in $\mX_t$. \\
    $\pref(\mX_T)$                 & Reference distribution $\mathbb{I} \{ n_T = 1\} \mathcal{N}(\rvx_T; 0, I_{d}) \approx q_T(\mX_T)$. \\
    $\btk_t( \mV |\mX_t)$                    & Transition kernel for the reverse process. \\
    $\backwardrate(\mX_t, t)$                & Rate at which jumps occur during the reverse process. \\
    $\text{ins}(\mX_t, \yadd, i)$                             & Insertion operator which adds component $\yadd$ to $\mX_t$ in index $i$.   \\
    $\autonet_t(\yadd, i | \mX_t)$           & Distribution over component value and index to add at a jump in the reverse process.    \\
    $\backwarddrift(\rvx_t, t)$              & Drift term for the reverse process. \\
    $\backwarddiffcoeff(t)$                  & Scaling factor for Wiener process noise added in the reverse process. \\
    $^*$             & Denotes the ``optimal'' form of a term in the reverse process which is required to ensure that the reverse process is the time-reversal of the forward process. \\
    $\backwarddrift_\theta(\mX_t, t)$ & Neural network's parameterisation of reverse process drift. Parameterised via $\predx_\theta$ or $\preds_\theta$ as in earlier sections. \\
    $\backwardrate_\theta(\mX_t, t) $ & Neural network's parameterisation of reverse process rate. \\
    $\autonet_t^\theta(\yadd, i | \mX_t)$ & Neural network's parameterisation of distribution over value and index to add in reverse process. \\
    $\gL_\text{TDDM}(\theta)$                      & The trans-dimensional diffusion model loss. \\
     % &  \\
     % &  \\
    \bottomrule
  \end{tabular}
\end{table*}


\begin{table*}
  % \caption{Definitions for symbols used in \cref{ch:cigcvae}.}
  % \label{tab:cigcvae-notation}
  % \centering
  % \begin{tabular}{rp{10cm}}
  \caption{Symbols defined in \cref{ch:cigcvae}.}
  \label{tab:notation-appendix-cigcvae}
  \centering
  \footnotesize
  \begin{tabular}{rp{9cm}}
    \toprule
    Symbol    & Definition   \\
    \midrule
    $\rvz$                                   & VAE's latent variables. \\
    $\rvx$                                & Data we learn a generative model of. \\
    $\rvy$                            & Data on which our generative model is conditioned. \\
    $\tildeI{}$                           & The part of $\rvx$ which the conditional VAE models, defined as all pixels not in $\rvy$ for image completion, or simply $\tildeI := \rvx$ for other conditional generation tasks. \\
    $\theta$                              & Parameters for VAE's prior and decoder. \\
    $\phi$                                & Parameters for VAE's encoder. \\
    % $\pdata (\rvx, \rvy)$            & Distribution from which we assume training/test instances are i.i.d.~samples. \\
    $p_\theta(\rvz)$                        & Learned prior of VAE. \\
    $p_\theta(\rvx|\rvz)$                   & Distribution output by VAE's decoder given $\rvz$. \\
    $q_\phi(\rvz|\rvx)$                           & Distribution output by VAE's encoder given $\rvx$. \\
    $p_\theta(\rvz, \rvx, \rvy)$        & Joint distribution defined as $p_\theta(\rvz)p_\theta(\rvx|\rvz)\pdata (\rvy|\rvx)$. \\
    $q_\phi(\rvz,\rvx, \rvy)$                 & Joint distribution defined as $\pdata (\rvx, \rvy)q_\phi(\rvz|\rvx)$. \\
    $\psi$                            & Partial encoder parameters. \\
    $p_\psi(\rvz|\rvy)$                 & Distribution output by partial encoder. \\
    $p_{\theta,\psi}(\rvx|\rvy)$             & Distribution over from which images are sampled given a conditioning input. Defined as $\int p_\theta(\rvx|\rvz)p_\psi(\rvz|\rvy) \mathrm{d}\rvz$. \\
    $\unp{}(\rvz, \rvx, \rvy)$           & Distribution with parameters $\optimal{\theta}$ and $\optimal{\phi}$ that are optimal as defined in \cref{proof:joint-training}. Defined equivalently as $p_{\optimal{\theta}}(\rvz, \rvx, \rvy)$ or $q_{\optimal{\phi}}(\rvz,\rvx,\rvy)$.  \\
    \midrule
    \multicolumn{2}{l}{Symbols for Bayesian optimal experimental design:} \\
    $\coord{}_t$                                 & The location scanned at time $t$. \\
    $v$                                   & Variable we wish to infer. \\
    $\rvy_{\coord{}_1,\ldots,\coord{}_t}$             & Image in which all pixel values are missing except for those observed by scans at locations $\coord{}_1,\ldots,\coord{}_t$. \\
    $f_{\coord{}_1,\ldots,\coord{}_t}$                    & Function mapping from an image $\rvx$ to $\rvy_{\coord{}_1,\ldots,\coord{}_t}$. \\
    $g(v|\rvy)$                       & Classification distribution for $v$ given partially observed image $\rvy$. \\
    $\gO_\text{VAE}(\theta)$                      & The VAE objective. \\   
    $\gO_\text{CVAE}(\theta)$                      & The conditional VAE objective. \\   
    \bottomrule
  \end{tabular}
  \vspace{-1em}
\end{table*}
